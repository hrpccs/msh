diff --git a/bolt/include/bolt/Core/BinaryBasicBlock.h b/bolt/include/bolt/Core/BinaryBasicBlock.h
index 8c044597b5fd..1c68749e28af 100644
--- a/bolt/include/bolt/Core/BinaryBasicBlock.h
+++ b/bolt/include/bolt/Core/BinaryBasicBlock.h
@@ -939,7 +939,12 @@ public:
   /// Check if the block has a jump table instruction.
   bool hasJumpTable() const { return getJumpTable() != nullptr; }
 
+  bool isPseudoPreheader() const { return IsPseudoPreheader; }
+  void setIsPseudoPreheader(bool Flag) { IsPseudoPreheader = Flag; } 
+
 private:
+  bool IsPseudoPreheader = false;
+  
   void adjustNumPseudos(const MCInst &Inst, int Sign);
 
   template <typename Itr> void adjustNumPseudos(Itr Begin, Itr End, int Sign) {
diff --git a/bolt/include/bolt/Core/BinaryContext.h b/bolt/include/bolt/Core/BinaryContext.h
index 7e2d4d6e045c..c1fedfdfb850 100644
--- a/bolt/include/bolt/Core/BinaryContext.h
+++ b/bolt/include/bolt/Core/BinaryContext.h
@@ -58,6 +58,18 @@ using namespace object;
 
 namespace bolt {
 
+struct LatProfEntry {
+uint64_t begin;
+uint64_t end;
+uint64_t lat;
+};
+
+struct PredProfEntry {
+uint64_t target;
+uint64_t src;
+uint64_t counts;
+};
+
 class BinaryFunction;
 class ExecutableFileMemoryManager;
 
@@ -155,6 +167,17 @@ class BinaryContext {
   /// Unique build ID if available for the binary.
   std::optional<std::string> FileBuildID;
 
+  /// Cache miss PC list
+  struct CMPCEntry {
+    uint64_t addr;
+    float l2m_prob;
+    float l3m_prob;
+    uint64_t rel_oh_th;
+  };
+
+
+  std::unique_ptr<std::vector<CMPCEntry>> CMPCList;
+   
   /// Set of all sections.
   struct CompareSections {
     bool operator()(const BinarySection *A, const BinarySection *B) const {
@@ -233,6 +256,9 @@ class BinaryContext {
   void deregisterSectionName(const BinarySection &Section);
 
 public:
+  std::vector<LatProfEntry> LatProfList;
+  std::vector<PredProfEntry> PredProfList;    
+  
   static Expected<std::unique_ptr<BinaryContext>>
   createBinaryContext(const ObjectFile *File, bool IsPIC,
                       std::unique_ptr<DWARFContext> DwCtx);
@@ -320,6 +346,13 @@ public:
   StringRef getFilename() const { return Filename; }
   void setFilename(StringRef Name) { Filename = std::string(Name); }
 
+  /// Assign PC list of deliquent loads
+  Error setCMPC(StringRef Filename);
+  bool checkCMPCList(uint64_t Address) const;
+
+  Error setScavProf(StringRef LatProf, StringRef PredProf);
+   
+  
   std::optional<StringRef> getFileBuildID() const {
     if (FileBuildID)
       return StringRef(*FileBuildID);
diff --git a/bolt/include/bolt/Core/BinaryFunction.h b/bolt/include/bolt/Core/BinaryFunction.h
index a96243c579eb..fea2889455a3 100644
--- a/bolt/include/bolt/Core/BinaryFunction.h
+++ b/bolt/include/bolt/Core/BinaryFunction.h
@@ -213,6 +213,9 @@ public:
   using const_cfi_iterator = CFIInstrMapType::const_iterator;
 
 private:
+  // [SAM]
+  BitVector FilteredRegs;
+  
   /// Current state of the function.
   State CurrentState{State::Empty};
 
@@ -911,6 +914,14 @@ public:
     return nullptr;
   }
 
+  // [SAM] public interface for PnY
+  void setFilteredRegs(BitVector &Regs) { FilteredRegs = Regs; }
+  BitVector &getFilteredRegs() { return FilteredRegs; }
+  BinaryFunction *PseudoCopy = nullptr;
+  
+  bool HasPnYPoint = false;
+  bool IsPseudoCopy = false;
+
   /// Return instruction at a given offset in the function. Valid before
   /// CFG is constructed or while instruction offsets are available in CFG.
   MCInst *getInstructionAtOffset(uint64_t Offset);
@@ -2280,6 +2291,12 @@ public:
   bool isAArch64Veneer() const;
 
   virtual ~BinaryFunction();
+
+private:
+  bool ret_float = false;
+public:
+  bool getRetFloat() { return ret_float; }
+  void setRetFloat() { ret_float = true; }
 };
 
 inline raw_ostream &operator<<(raw_ostream &OS,
diff --git a/bolt/include/bolt/Core/BinaryLoop.h b/bolt/include/bolt/Core/BinaryLoop.h
index 72dce77df8c1..0400633efd66 100644
--- a/bolt/include/bolt/Core/BinaryLoop.h
+++ b/bolt/include/bolt/Core/BinaryLoop.h
@@ -23,9 +23,20 @@ namespace bolt {
 class BinaryBasicBlock;
 
 class BinaryLoop : public LoopBase<BinaryBasicBlock, BinaryLoop> {
+private:
+  BitVector RegsToEvacuate;
+  BitVector RegsToAsymPush;
+  BitVector RegsToAsymPop;
+
 public:
-  BinaryLoop() : LoopBase<BinaryBasicBlock, BinaryLoop>() {}
+  BinaryLoop() : LoopBase<BinaryBasicBlock, BinaryLoop>() {
+    RegsToEvacuate.clear();
+    RegsToAsymPush.clear();
+    RegsToAsymPop.clear();
+    RegsAlreadyUsed.clear();
+  }
 
+  bool containStackUpdate{false};
   // The total count of all the back edges of this loop.
   uint64_t TotalBackEdgeCount{0};
 
@@ -35,6 +46,43 @@ public:
   // The times the loop is exited.
   uint64_t ExitCount{0};
 
+  BitVector RegsAlreadyUsed;
+  unsigned InductionReg{0};
+  int InductionRegInitVal{0};
+  void addRegsAlreadyUsed(const BitVector &Regs) { 
+    if (RegsAlreadyUsed.empty())
+      RegsAlreadyUsed.resize(Regs.size());
+    RegsAlreadyUsed |= Regs; 
+  }
+
+  void addRegsToEvacuate(const BitVector &Regs) { 
+    if (RegsToEvacuate.empty())
+      RegsToEvacuate.resize(Regs.size());
+    RegsToEvacuate |= Regs; 
+  }
+  
+
+  void clearRegsToEvacuate() { RegsToEvacuate.clear(); }
+  const BitVector &getRegsToEvacuate() const { return RegsToEvacuate; }
+  
+  //interface for RegsToAsymPush and RegsToAsymPop
+  void addRegsToAsymPush(const BitVector &Regs) { 
+    if (RegsToAsymPush.empty())
+      RegsToAsymPush.resize(Regs.size());
+    RegsToAsymPush |= Regs; 
+  }
+  void clearRegsToAsymPush() { RegsToAsymPush.clear(); }
+  const BitVector &getRegsToAsymPush() const { return RegsToAsymPush; }
+
+  void addRegsToAsymPop(const BitVector &Regs) { 
+    if (RegsToAsymPop.empty())
+      RegsToAsymPop.resize(Regs.size());
+    RegsToAsymPop |= Regs; 
+  }
+  void clearRegsToAsymPop() { RegsToAsymPop.clear(); }
+  const BitVector &getRegsToAsymPop() const { return RegsToAsymPop; }
+
+  
   // Most of the public interface is provided by LoopBase.
 
 protected:
diff --git a/bolt/include/bolt/Core/FunctionLayout.h b/bolt/include/bolt/Core/FunctionLayout.h
index 904da3a4a93a..3b73b9239b2e 100644
--- a/bolt/include/bolt/Core/FunctionLayout.h
+++ b/bolt/include/bolt/Core/FunctionLayout.h
@@ -128,6 +128,8 @@ public:
   friend class FunctionLayout;
 };
 
+// [SAM] This is the way optimization passes manipulate the instruction.
+
 /// The function layout represents the fragments we split a function into and
 /// the order of basic blocks within each fragment.
 ///
diff --git a/bolt/include/bolt/Core/MCPlusBuilder.h b/bolt/include/bolt/Core/MCPlusBuilder.h
index e6eb1429d3a1..989d2e897e94 100644
--- a/bolt/include/bolt/Core/MCPlusBuilder.h
+++ b/bolt/include/bolt/Core/MCPlusBuilder.h
@@ -14,6 +14,7 @@
 #ifndef BOLT_CORE_MCPLUSBUILDER_H
 #define BOLT_CORE_MCPLUSBUILDER_H
 
+// #include "bolt/Core/BinaryContext.h"
 #include "bolt/Core/MCPlus.h"
 #include "bolt/Core/Relocation.h"
 #include "llvm/ADT/ArrayRef.h"
@@ -49,1948 +50,2097 @@ class BinaryFunction;
 
 /// Different types of indirect branches encountered during disassembly.
 enum class IndirectBranchType : char {
-  UNKNOWN = 0,             /// Unable to determine type.
-  POSSIBLE_TAIL_CALL,      /// Possibly a tail call.
-  POSSIBLE_JUMP_TABLE,     /// Possibly a switch/jump table.
-  POSSIBLE_PIC_JUMP_TABLE, /// Possibly a jump table for PIC.
-  POSSIBLE_GOTO,           /// Possibly a gcc's computed goto.
-  POSSIBLE_FIXED_BRANCH,   /// Possibly an indirect branch to a fixed location.
+    UNKNOWN = 0,               /// Unable to determine type.
+    POSSIBLE_TAIL_CALL,        /// Possibly a tail call.
+    POSSIBLE_JUMP_TABLE,       /// Possibly a switch/jump table.
+    POSSIBLE_PIC_JUMP_TABLE,   /// Possibly a jump table for PIC.
+    POSSIBLE_GOTO,             /// Possibly a gcc's computed goto.
+    POSSIBLE_FIXED_BRANCH,     /// Possibly an indirect branch to a fixed
+                               /// location.
 };
 
 class MCPlusBuilder {
-public:
-  using AllocatorIdTy = uint16_t;
-
-private:
-  /// A struct that represents a single annotation allocator
-  struct AnnotationAllocator {
-    SpecificBumpPtrAllocator<MCInst> MCInstAllocator;
-    BumpPtrAllocator ValueAllocator;
-    std::unordered_set<MCPlus::MCAnnotation *> AnnotationPool;
-  };
-
-  /// A set of annotation allocators
-  std::unordered_map<AllocatorIdTy, AnnotationAllocator> AnnotationAllocators;
-
-  /// A variable that is used to generate unique ids for annotation allocators
-  AllocatorIdTy MaxAllocatorId = 0;
-
-  /// We encode Index and Value into a 64-bit immediate operand value.
-  static int64_t encodeAnnotationImm(uint8_t Index, int64_t Value) {
-    if (LLVM_UNLIKELY(Value != extractAnnotationValue(Value)))
-      report_fatal_error("annotation value out of range");
-
-    Value &= 0xff'ffff'ffff'ffff;
-    Value |= (int64_t)Index << 56;
-
-    return Value;
-  }
-
-  /// Extract annotation index from immediate operand value.
-  static uint8_t extractAnnotationIndex(int64_t ImmValue) {
-    return ImmValue >> 56;
-  }
-
-  /// Extract annotation value from immediate operand value.
-  static int64_t extractAnnotationValue(int64_t ImmValue) {
-    return SignExtend64<56>(ImmValue & 0xff'ffff'ffff'ffffULL);
-  }
-
-  MCInst *getAnnotationInst(const MCInst &Inst) const {
-    if (Inst.getNumOperands() == 0)
-      return nullptr;
-
-    const MCOperand &LastOp = Inst.getOperand(Inst.getNumOperands() - 1);
-    if (!LastOp.isInst())
-      return nullptr;
-
-    MCInst *AnnotationInst = const_cast<MCInst *>(LastOp.getInst());
-    assert(AnnotationInst->getOpcode() == TargetOpcode::ANNOTATION_LABEL);
-
-    return AnnotationInst;
-  }
-
-  void removeAnnotationInst(MCInst &Inst) const {
-    assert(getAnnotationInst(Inst) && "Expected annotation instruction.");
-    Inst.erase(std::prev(Inst.end()));
-    assert(!getAnnotationInst(Inst) &&
-           "More than one annotation instruction detected.");
-  }
-
-  void setAnnotationOpValue(MCInst &Inst, unsigned Index, int64_t Value,
-                            AllocatorIdTy AllocatorId = 0) {
-    MCInst *AnnotationInst = getAnnotationInst(Inst);
-    if (!AnnotationInst) {
-      AnnotationAllocator &Allocator = getAnnotationAllocator(AllocatorId);
-      AnnotationInst = new (Allocator.MCInstAllocator.Allocate()) MCInst();
-      AnnotationInst->setOpcode(TargetOpcode::ANNOTATION_LABEL);
-      Inst.addOperand(MCOperand::createInst(AnnotationInst));
-    }
-
-    const int64_t AnnotationValue = encodeAnnotationImm(Index, Value);
-    for (int I = AnnotationInst->getNumOperands() - 1; I >= 0; --I) {
-      int64_t ImmValue = AnnotationInst->getOperand(I).getImm();
-      if (extractAnnotationIndex(ImmValue) == Index) {
-        AnnotationInst->getOperand(I).setImm(AnnotationValue);
-        return;
-      }
+  public:
+    using AllocatorIdTy = uint16_t;
+
+  private:
+    /// A struct that represents a single annotation allocator
+    struct AnnotationAllocator {
+        SpecificBumpPtrAllocator<MCInst>           MCInstAllocator;
+        BumpPtrAllocator                           ValueAllocator;
+        std::unordered_set<MCPlus::MCAnnotation *> AnnotationPool;
+    };
+
+    /// A set of annotation allocators
+    std::unordered_map<AllocatorIdTy, AnnotationAllocator> AnnotationAllocators;
+
+    /// A variable that is used to generate unique ids for annotation allocators
+    AllocatorIdTy MaxAllocatorId = 0;
+
+    /// We encode Index and Value into a 64-bit immediate operand value.
+    static int64_t encodeAnnotationImm(uint8_t Index, int64_t Value) {
+        if (LLVM_UNLIKELY(Value != extractAnnotationValue(Value)))
+            report_fatal_error("annotation value out of range");
+
+        Value &= 0xff'ffff'ffff'ffff;
+        Value |= (int64_t) Index << 56;
+
+        return Value;
+    }
+
+    /// Extract annotation index from immediate operand value.
+    static uint8_t extractAnnotationIndex(int64_t ImmValue) {
+        return ImmValue >> 56;
+    }
+
+    /// Extract annotation value from immediate operand value.
+    static int64_t extractAnnotationValue(int64_t ImmValue) {
+        return SignExtend64<56>(ImmValue & 0xff'ffff'ffff'ffffULL);
+    }
+
+    MCInst *getAnnotationInst(const MCInst &Inst) const {
+        if (Inst.getNumOperands() == 0)
+            return nullptr;
+
+        const MCOperand &LastOp = Inst.getOperand(Inst.getNumOperands() - 1);
+        if (!LastOp.isInst())
+            return nullptr;
+
+        MCInst *AnnotationInst = const_cast<MCInst *>(LastOp.getInst());
+        assert(AnnotationInst->getOpcode() == TargetOpcode::ANNOTATION_LABEL);
+
+        return AnnotationInst;
+    }
+
+    void removeAnnotationInst(MCInst &Inst) const {
+        assert(getAnnotationInst(Inst) && "Expected annotation instruction.");
+        Inst.erase(std::prev(Inst.end()));
+        assert(!getAnnotationInst(Inst) &&
+               "More than one annotation instruction detected.");
     }
 
-    AnnotationInst->addOperand(MCOperand::createImm(AnnotationValue));
-  }
+    void setAnnotationOpValue(MCInst &Inst, unsigned Index, int64_t Value,
+                              AllocatorIdTy AllocatorId = 0) {
+        MCInst *AnnotationInst = getAnnotationInst(Inst);
+        if (!AnnotationInst) {
+            AnnotationAllocator &Allocator =
+                getAnnotationAllocator(AllocatorId);
+            AnnotationInst =
+                new (Allocator.MCInstAllocator.Allocate()) MCInst();
+            AnnotationInst->setOpcode(TargetOpcode::ANNOTATION_LABEL);
+            Inst.addOperand(MCOperand::createInst(AnnotationInst));
+        }
 
-  std::optional<int64_t> getAnnotationOpValue(const MCInst &Inst,
-                                              unsigned Index) const {
-    const MCInst *AnnotationInst = getAnnotationInst(Inst);
-    if (!AnnotationInst)
-      return std::nullopt;
+        const int64_t AnnotationValue = encodeAnnotationImm(Index, Value);
+        for (int I = AnnotationInst->getNumOperands() - 1; I >= 0; --I) {
+            int64_t ImmValue = AnnotationInst->getOperand(I).getImm();
+            if (extractAnnotationIndex(ImmValue) == Index) {
+                AnnotationInst->getOperand(I).setImm(AnnotationValue);
+                return;
+            }
+        }
 
-    for (int I = AnnotationInst->getNumOperands() - 1; I >= 0; --I) {
-      int64_t ImmValue = AnnotationInst->getOperand(I).getImm();
-      if (extractAnnotationIndex(ImmValue) == Index) {
-        return extractAnnotationValue(ImmValue);
-      }
+        AnnotationInst->addOperand(MCOperand::createImm(AnnotationValue));
     }
 
-    return std::nullopt;
-  }
+    std::optional<int64_t> getAnnotationOpValue(const MCInst &Inst,
+                                                unsigned      Index) const {
+        const MCInst *AnnotationInst = getAnnotationInst(Inst);
+        if (!AnnotationInst)
+            return std::nullopt;
+
+        for (int I = AnnotationInst->getNumOperands() - 1; I >= 0; --I) {
+            int64_t ImmValue = AnnotationInst->getOperand(I).getImm();
+            if (extractAnnotationIndex(ImmValue) == Index) {
+                return extractAnnotationValue(ImmValue);
+            }
+        }
+
+        return std::nullopt;
+    }
 
-protected:
-  const MCInstrAnalysis *Analysis;
-  const MCInstrInfo *Info;
-  const MCRegisterInfo *RegInfo;
+  protected:
+    const MCInstrAnalysis *Analysis;
+    const MCInstrInfo     *Info;
+    const MCRegisterInfo  *RegInfo;
 
-  /// Map annotation name into an annotation index.
-  StringMap<uint64_t> AnnotationNameIndexMap;
+    /// Map annotation name into an annotation index.
+    StringMap<uint64_t> AnnotationNameIndexMap;
 
-  /// Names of non-standard annotations.
-  SmallVector<std::string, 8> AnnotationNames;
+    /// Names of non-standard annotations.
+    SmallVector<std::string, 8> AnnotationNames;
 
-  /// Allocate the TailCall annotation value. Clients of the target-specific
-  /// MCPlusBuilder classes must use convert/lower/create* interfaces instead.
-  void setTailCall(MCInst &Inst);
+    /// Allocate the TailCall annotation value. Clients of the target-specific
+    /// MCPlusBuilder classes must use convert/lower/create* interfaces instead.
+    void setTailCall(MCInst &Inst);
 
-  /// Transfer annotations from \p SrcInst to \p DstInst.
-  void moveAnnotations(MCInst &&SrcInst, MCInst &DstInst) const {
-    assert(!getAnnotationInst(DstInst) &&
-           "Destination instruction should not have annotations.");
-    const MCInst *AnnotationInst = getAnnotationInst(SrcInst);
-    if (!AnnotationInst)
-      return;
+    /// Transfer annotations from \p SrcInst to \p DstInst.
+    void moveAnnotations(MCInst &&SrcInst, MCInst &DstInst) const {
+        assert(!getAnnotationInst(DstInst) &&
+               "Destination instruction should not have annotations.");
+        const MCInst *AnnotationInst = getAnnotationInst(SrcInst);
+        if (!AnnotationInst)
+            return;
 
-    DstInst.addOperand(MCOperand::createInst(AnnotationInst));
-    removeAnnotationInst(SrcInst);
-  }
+        DstInst.addOperand(MCOperand::createInst(AnnotationInst));
+        removeAnnotationInst(SrcInst);
+    }
 
-public:
-  class InstructionIterator {
   public:
-    using iterator_category = std::bidirectional_iterator_tag;
-    using value_type = MCInst;
-    using difference_type = std::ptrdiff_t;
-    using pointer = value_type *;
-    using reference = value_type &;
-
-    class Impl {
-    public:
-      virtual Impl *Copy() const = 0;
-      virtual void Next() = 0;
-      virtual void Prev() = 0;
-      virtual MCInst &Deref() = 0;
-      virtual bool Compare(const Impl &Other) const = 0;
-      virtual ~Impl() {}
-    };
+    class InstructionIterator {
+      public:
+        using iterator_category = std::bidirectional_iterator_tag;
+        using value_type        = MCInst;
+        using difference_type   = std::ptrdiff_t;
+        using pointer           = value_type *;
+        using reference         = value_type &;
+
+        class Impl {
+          public:
+            virtual Impl   *Copy() const                     = 0;
+            virtual void    Next()                           = 0;
+            virtual void    Prev()                           = 0;
+            virtual MCInst &Deref()                          = 0;
+            virtual bool    Compare(const Impl &Other) const = 0;
+            virtual ~Impl() {}
+        };
+
+        template <typename T> class SeqImpl : public Impl {
+          public:
+            virtual Impl   *Copy() const override { return new SeqImpl(Itr); }
+            virtual void    Next() override { ++Itr; }
+            virtual void    Prev() override { --Itr; }
+            virtual MCInst &Deref() override {
+                return const_cast<MCInst &>(*Itr);
+            }
+            virtual bool Compare(const Impl &Other) const override {
+                // assumes that Other is same underlying type
+                return Itr == static_cast<const SeqImpl<T> &>(Other).Itr;
+            }
+            explicit SeqImpl(T &&Itr) : Itr(std::move(Itr)) {}
+            explicit SeqImpl(const T &Itr) : Itr(Itr) {}
+
+          private:
+            T Itr;
+        };
+
+        template <typename T> class MapImpl : public Impl {
+          public:
+            virtual Impl   *Copy() const override { return new MapImpl(Itr); }
+            virtual void    Next() override { ++Itr; }
+            virtual void    Prev() override { --Itr; }
+            virtual MCInst &Deref() override {
+                return const_cast<MCInst &>(Itr->second);
+            }
+            virtual bool Compare(const Impl &Other) const override {
+                // assumes that Other is same underlying type
+                return Itr == static_cast<const MapImpl<T> &>(Other).Itr;
+            }
+            explicit MapImpl(T &&Itr) : Itr(std::move(Itr)) {}
+            explicit MapImpl(const T &Itr) : Itr(Itr) {}
+
+          private:
+            T Itr;
+        };
+
+        InstructionIterator &operator++() {
+            Itr->Next();
+            return *this;
+        }
+        InstructionIterator &operator--() {
+            Itr->Prev();
+            return *this;
+        }
+        InstructionIterator operator++(int) {
+            std::unique_ptr<Impl> Tmp(Itr->Copy());
+            Itr->Next();
+            return InstructionIterator(std::move(Tmp));
+        }
+        InstructionIterator operator--(int) {
+            std::unique_ptr<Impl> Tmp(Itr->Copy());
+            Itr->Prev();
+            return InstructionIterator(std::move(Tmp));
+        }
+        bool operator==(const InstructionIterator &Other) const {
+            return Itr->Compare(*Other.Itr);
+        }
+        bool operator!=(const InstructionIterator &Other) const {
+            return !Itr->Compare(*Other.Itr);
+        }
+        MCInst &operator*() { return Itr->Deref(); }
+        MCInst *operator->() { return &Itr->Deref(); }
 
-    template <typename T> class SeqImpl : public Impl {
-    public:
-      virtual Impl *Copy() const override { return new SeqImpl(Itr); }
-      virtual void Next() override { ++Itr; }
-      virtual void Prev() override { --Itr; }
-      virtual MCInst &Deref() override { return const_cast<MCInst &>(*Itr); }
-      virtual bool Compare(const Impl &Other) const override {
-        // assumes that Other is same underlying type
-        return Itr == static_cast<const SeqImpl<T> &>(Other).Itr;
-      }
-      explicit SeqImpl(T &&Itr) : Itr(std::move(Itr)) {}
-      explicit SeqImpl(const T &Itr) : Itr(Itr) {}
-
-    private:
-      T Itr;
-    };
+        InstructionIterator &operator=(InstructionIterator &&Other) {
+            Itr = std::move(Other.Itr);
+            return *this;
+        }
+        InstructionIterator &operator=(const InstructionIterator &Other) {
+            if (this != &Other)
+                Itr.reset(Other.Itr->Copy());
+            return *this;
+        }
+        InstructionIterator() {}
+        InstructionIterator(const InstructionIterator &Other)
+            : Itr(Other.Itr->Copy()) {}
+        InstructionIterator(InstructionIterator &&Other)
+            : Itr(std::move(Other.Itr)) {}
+        explicit InstructionIterator(std::unique_ptr<Impl> Itr)
+            : Itr(std::move(Itr)) {}
+
+        InstructionIterator(InstructionListType::iterator Itr)
+            : Itr(new SeqImpl<InstructionListType::iterator>(Itr)) {}
+
+        template <typename T>
+        InstructionIterator(T *Itr) : Itr(new SeqImpl<T *>(Itr)) {}
+
+        InstructionIterator(ArrayRef<MCInst>::iterator Itr)
+            : Itr(new SeqImpl<ArrayRef<MCInst>::iterator>(Itr)) {}
 
-    template <typename T> class MapImpl : public Impl {
-    public:
-      virtual Impl *Copy() const override { return new MapImpl(Itr); }
-      virtual void Next() override { ++Itr; }
-      virtual void Prev() override { --Itr; }
-      virtual MCInst &Deref() override {
-        return const_cast<MCInst &>(Itr->second);
-      }
-      virtual bool Compare(const Impl &Other) const override {
-        // assumes that Other is same underlying type
-        return Itr == static_cast<const MapImpl<T> &>(Other).Itr;
-      }
-      explicit MapImpl(T &&Itr) : Itr(std::move(Itr)) {}
-      explicit MapImpl(const T &Itr) : Itr(Itr) {}
-
-    private:
-      T Itr;
+        InstructionIterator(MutableArrayRef<MCInst>::iterator Itr)
+            : Itr(new SeqImpl<MutableArrayRef<MCInst>::iterator>(Itr)) {}
+
+        // TODO: it would be nice to templatize this on the key type.
+        InstructionIterator(std::map<uint32_t, MCInst>::iterator Itr)
+            : Itr(new MapImpl<std::map<uint32_t, MCInst>::iterator>(Itr)) {}
+
+      private:
+        std::unique_ptr<Impl> Itr;
     };
 
-    InstructionIterator &operator++() {
-      Itr->Next();
-      return *this;
+  public:
+    MCPlusBuilder(const MCInstrAnalysis *Analysis, const MCInstrInfo *Info,
+                  const MCRegisterInfo *RegInfo)
+        : Analysis(Analysis), Info(Info), RegInfo(RegInfo) {
+        // Initialize the default annotation allocator with id 0
+        AnnotationAllocators.emplace(0, AnnotationAllocator());
+        MaxAllocatorId++;
+        // Build alias map
+        initAliases();
+        initSizeMap();
+    }
+
+    /// Create and return target-specific MC symbolizer for the \p Function.
+    virtual std::unique_ptr<MCSymbolizer>
+    createTargetSymbolizer(BinaryFunction &Function) const {
+        return nullptr;
+    }
+
+    /// Initialize a new annotation allocator and return its id
+    AllocatorIdTy initializeNewAnnotationAllocator() {
+        AnnotationAllocators.emplace(MaxAllocatorId, AnnotationAllocator());
+        return MaxAllocatorId++;
+    }
+
+    /// Return the annotation allocator of a given id
+    AnnotationAllocator &getAnnotationAllocator(AllocatorIdTy AllocatorId) {
+        assert(AnnotationAllocators.count(AllocatorId) &&
+               "allocator not initialized");
+        return AnnotationAllocators.find(AllocatorId)->second;
+    }
+
+    // Check if an annotation allocator with the given id exists
+    bool checkAllocatorExists(AllocatorIdTy AllocatorId) {
+        return AnnotationAllocators.count(AllocatorId);
+    }
+
+    /// Free the values allocator within the annotation allocator
+    void freeValuesAllocator(AllocatorIdTy AllocatorId) {
+        AnnotationAllocator &Allocator = getAnnotationAllocator(AllocatorId);
+        for (MCPlus::MCAnnotation *Annotation : Allocator.AnnotationPool)
+            Annotation->~MCAnnotation();
+
+        Allocator.AnnotationPool.clear();
+        Allocator.ValueAllocator.Reset();
+    }
+
+    virtual ~MCPlusBuilder() { freeAnnotations(); }
+
+    /// Free all memory allocated for annotations
+    void freeAnnotations() {
+        for (auto &Element : AnnotationAllocators) {
+            AnnotationAllocator &Allocator = Element.second;
+            for (MCPlus::MCAnnotation *Annotation : Allocator.AnnotationPool)
+                Annotation->~MCAnnotation();
+
+            Allocator.AnnotationPool.clear();
+            Allocator.ValueAllocator.Reset();
+            Allocator.MCInstAllocator.DestroyAll();
+        }
+    }
+
+    using CompFuncTy = std::function<bool(const MCSymbol *, const MCSymbol *)>;
+
+    bool equals(const MCInst &A, const MCInst &B, CompFuncTy Comp) const;
+
+    bool equals(const MCOperand &A, const MCOperand &B, CompFuncTy Comp) const;
+
+    bool equals(const MCExpr &A, const MCExpr &B, CompFuncTy Comp) const;
+
+    virtual bool equals(const MCTargetExpr &A, const MCTargetExpr &B,
+                        CompFuncTy Comp) const;
+
+    virtual bool isBranch(const MCInst &Inst) const {
+        return Analysis->isBranch(Inst);
+    }
+
+    virtual bool isConditionalBranch(const MCInst &Inst) const {
+        return Analysis->isConditionalBranch(Inst);
     }
-    InstructionIterator &operator--() {
-      Itr->Prev();
-      return *this;
+
+    /// Returns true if Inst is a condtional move instruction
+    virtual bool isConditionalMove(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
     }
-    InstructionIterator operator++(int) {
-      std::unique_ptr<Impl> Tmp(Itr->Copy());
-      Itr->Next();
-      return InstructionIterator(std::move(Tmp));
+
+    virtual bool isUnconditionalBranch(const MCInst &Inst) const {
+        return Analysis->isUnconditionalBranch(Inst) && !isTailCall(Inst);
     }
-    InstructionIterator operator--(int) {
-      std::unique_ptr<Impl> Tmp(Itr->Copy());
-      Itr->Prev();
-      return InstructionIterator(std::move(Tmp));
+
+    virtual bool isIndirectBranch(const MCInst &Inst) const {
+        return Analysis->isIndirectBranch(Inst);
     }
-    bool operator==(const InstructionIterator &Other) const {
-      return Itr->Compare(*Other.Itr);
+
+    /// Returns true if the instruction is memory indirect call or jump
+    virtual bool isBranchOnMem(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
     }
-    bool operator!=(const InstructionIterator &Other) const {
-      return !Itr->Compare(*Other.Itr);
+
+    /// Returns true if the instruction is register indirect call or jump
+    virtual bool isBranchOnReg(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
     }
-    MCInst &operator*() { return Itr->Deref(); }
-    MCInst *operator->() { return &Itr->Deref(); }
 
-    InstructionIterator &operator=(InstructionIterator &&Other) {
-      Itr = std::move(Other.Itr);
-      return *this;
+    /// Check whether we support inverting this branch
+    virtual bool isUnsupportedBranch(unsigned Opcode) const { return false; }
+
+    /// Return true of the instruction is of pseudo kind.
+    bool isPseudo(const MCInst &Inst) const {
+        return Info->get(Inst.getOpcode()).isPseudo();
     }
-    InstructionIterator &operator=(const InstructionIterator &Other) {
-      if (this != &Other)
-        Itr.reset(Other.Itr->Copy());
-      return *this;
+
+    /// Return true if the relocation type needs to be registered in the
+    /// function. These code relocations are used in disassembly to better
+    /// understand code.
+    ///
+    /// For ARM, they help us decode instruction operands unambiguously, but
+    /// sometimes we might discard them because we already have the necessary
+    /// information in the instruction itself (e.g. we don't need to record CALL
+    /// relocs in ARM because we can fully decode the target from the call
+    /// operand).
+    ///
+    /// For X86, they might be used in scanExternalRefs when we want to skip
+    /// a function but still patch references inside it.
+    virtual bool shouldRecordCodeRelocation(uint64_t RelType) const {
+        llvm_unreachable("not implemented");
+        return false;
     }
-    InstructionIterator() {}
-    InstructionIterator(const InstructionIterator &Other)
-        : Itr(Other.Itr->Copy()) {}
-    InstructionIterator(InstructionIterator &&Other)
-        : Itr(std::move(Other.Itr)) {}
-    explicit InstructionIterator(std::unique_ptr<Impl> Itr)
-        : Itr(std::move(Itr)) {}
 
-    InstructionIterator(InstructionListType::iterator Itr)
-        : Itr(new SeqImpl<InstructionListType::iterator>(Itr)) {}
+    /// Creates x86 pause instruction.
+    virtual void createPause(MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+    }
 
-    template <typename T>
-    InstructionIterator(T *Itr) : Itr(new SeqImpl<T *>(Itr)) {}
+    virtual void createLfence(MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+    }
 
-    InstructionIterator(ArrayRef<MCInst>::iterator Itr)
-        : Itr(new SeqImpl<ArrayRef<MCInst>::iterator>(Itr)) {}
+    virtual void createPushRegister(MCInst &Inst, MCPhysReg Reg,
+                                    unsigned Size) const {
+        llvm_unreachable("not implemented");
+    }
 
-    InstructionIterator(MutableArrayRef<MCInst>::iterator Itr)
-        : Itr(new SeqImpl<MutableArrayRef<MCInst>::iterator>(Itr)) {}
+    virtual void createPopRegister(MCInst &Inst, MCPhysReg Reg,
+                                   unsigned Size) const {
+        llvm_unreachable("not implemented");
+    }
 
-    // TODO: it would be nice to templatize this on the key type.
-    InstructionIterator(std::map<uint32_t, MCInst>::iterator Itr)
-        : Itr(new MapImpl<std::map<uint32_t, MCInst>::iterator>(Itr)) {}
+    virtual void createPushFlags(MCInst &Inst, unsigned Size) const {
+        llvm_unreachable("not implemented");
+    }
 
-  private:
-    std::unique_ptr<Impl> Itr;
-  };
-
-public:
-  MCPlusBuilder(const MCInstrAnalysis *Analysis, const MCInstrInfo *Info,
-                const MCRegisterInfo *RegInfo)
-      : Analysis(Analysis), Info(Info), RegInfo(RegInfo) {
-    // Initialize the default annotation allocator with id 0
-    AnnotationAllocators.emplace(0, AnnotationAllocator());
-    MaxAllocatorId++;
-    // Build alias map
-    initAliases();
-    initSizeMap();
-  }
-
-  /// Create and return target-specific MC symbolizer for the \p Function.
-  virtual std::unique_ptr<MCSymbolizer>
-  createTargetSymbolizer(BinaryFunction &Function) const {
-    return nullptr;
-  }
-
-  /// Initialize a new annotation allocator and return its id
-  AllocatorIdTy initializeNewAnnotationAllocator() {
-    AnnotationAllocators.emplace(MaxAllocatorId, AnnotationAllocator());
-    return MaxAllocatorId++;
-  }
-
-  /// Return the annotation allocator of a given id
-  AnnotationAllocator &getAnnotationAllocator(AllocatorIdTy AllocatorId) {
-    assert(AnnotationAllocators.count(AllocatorId) &&
-           "allocator not initialized");
-    return AnnotationAllocators.find(AllocatorId)->second;
-  }
-
-  // Check if an annotation allocator with the given id exists
-  bool checkAllocatorExists(AllocatorIdTy AllocatorId) {
-    return AnnotationAllocators.count(AllocatorId);
-  }
-
-  /// Free the values allocator within the annotation allocator
-  void freeValuesAllocator(AllocatorIdTy AllocatorId) {
-    AnnotationAllocator &Allocator = getAnnotationAllocator(AllocatorId);
-    for (MCPlus::MCAnnotation *Annotation : Allocator.AnnotationPool)
-      Annotation->~MCAnnotation();
-
-    Allocator.AnnotationPool.clear();
-    Allocator.ValueAllocator.Reset();
-  }
-
-  virtual ~MCPlusBuilder() { freeAnnotations(); }
-
-  /// Free all memory allocated for annotations
-  void freeAnnotations() {
-    for (auto &Element : AnnotationAllocators) {
-      AnnotationAllocator &Allocator = Element.second;
-      for (MCPlus::MCAnnotation *Annotation : Allocator.AnnotationPool)
-        Annotation->~MCAnnotation();
-
-      Allocator.AnnotationPool.clear();
-      Allocator.ValueAllocator.Reset();
-      Allocator.MCInstAllocator.DestroyAll();
-    }
-  }
-
-  using CompFuncTy = std::function<bool(const MCSymbol *, const MCSymbol *)>;
-
-  bool equals(const MCInst &A, const MCInst &B, CompFuncTy Comp) const;
-
-  bool equals(const MCOperand &A, const MCOperand &B, CompFuncTy Comp) const;
-
-  bool equals(const MCExpr &A, const MCExpr &B, CompFuncTy Comp) const;
-
-  virtual bool equals(const MCTargetExpr &A, const MCTargetExpr &B,
-                      CompFuncTy Comp) const;
-
-  virtual bool isBranch(const MCInst &Inst) const {
-    return Analysis->isBranch(Inst);
-  }
-
-  virtual bool isConditionalBranch(const MCInst &Inst) const {
-    return Analysis->isConditionalBranch(Inst);
-  }
-
-  /// Returns true if Inst is a condtional move instruction
-  virtual bool isConditionalMove(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isUnconditionalBranch(const MCInst &Inst) const {
-    return Analysis->isUnconditionalBranch(Inst) && !isTailCall(Inst);
-  }
-
-  virtual bool isIndirectBranch(const MCInst &Inst) const {
-    return Analysis->isIndirectBranch(Inst);
-  }
-
-  /// Returns true if the instruction is memory indirect call or jump
-  virtual bool isBranchOnMem(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Returns true if the instruction is register indirect call or jump
-  virtual bool isBranchOnReg(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Check whether we support inverting this branch
-  virtual bool isUnsupportedBranch(unsigned Opcode) const { return false; }
-
-  /// Return true of the instruction is of pseudo kind.
-  bool isPseudo(const MCInst &Inst) const {
-    return Info->get(Inst.getOpcode()).isPseudo();
-  }
-
-  /// Return true if the relocation type needs to be registered in the function.
-  /// These code relocations are used in disassembly to better understand code.
-  ///
-  /// For ARM, they help us decode instruction operands unambiguously, but
-  /// sometimes we might discard them because we already have the necessary
-  /// information in the instruction itself (e.g. we don't need to record CALL
-  /// relocs in ARM because we can fully decode the target from the call
-  /// operand).
-  ///
-  /// For X86, they might be used in scanExternalRefs when we want to skip
-  /// a function but still patch references inside it.
-  virtual bool shouldRecordCodeRelocation(uint64_t RelType) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Creates x86 pause instruction.
-  virtual void createPause(MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual void createLfence(MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual void createPushRegister(MCInst &Inst, MCPhysReg Reg,
-                                  unsigned Size) const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual void createPopRegister(MCInst &Inst, MCPhysReg Reg,
-                                 unsigned Size) const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual void createPushFlags(MCInst &Inst, unsigned Size) const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual void createPopFlags(MCInst &Inst, unsigned Size) const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual bool createDirectCall(MCInst &Inst, const MCSymbol *Target,
-                                MCContext *Ctx, bool IsTailCall) {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual MCPhysReg getX86R11() const { llvm_unreachable("not implemented"); }
-
-  /// Create increment contents of target by 1 for Instrumentation
-  virtual InstructionListType createInstrIncMemory(const MCSymbol *Target,
-                                                   MCContext *Ctx,
-                                                   bool IsLeaf) const {
-    llvm_unreachable("not implemented");
-    return InstructionListType();
-  }
-
-  /// Return a register number that is guaranteed to not match with
-  /// any real register on the underlying architecture.
-  MCPhysReg getNoRegister() const { return MCRegister::NoRegister; }
-
-  /// Return a register corresponding to a function integer argument \p ArgNo
-  /// if the argument is passed in a register. Or return the result of
-  /// getNoRegister() otherwise. The enumeration starts at 0.
-  ///
-  /// Note: this should depend on a used calling convention.
-  virtual MCPhysReg getIntArgRegister(unsigned ArgNo) const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual bool isIndirectCall(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isCall(const MCInst &Inst) const {
-    return Analysis->isCall(Inst) || isTailCall(Inst);
-  }
-
-  virtual bool isReturn(const MCInst &Inst) const {
-    return Analysis->isReturn(Inst);
-  }
-
-  virtual bool isTerminator(const MCInst &Inst) const {
-    return Analysis->isTerminator(Inst);
-  }
-
-  virtual bool isNoop(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isBreakpoint(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isPrefix(const MCInst &Inst) const { return false; }
-
-  virtual bool isRep(const MCInst &Inst) const { return false; }
-
-  virtual bool deleteREPPrefix(MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isEHLabel(const MCInst &Inst) const {
-    return Inst.getOpcode() == TargetOpcode::EH_LABEL;
-  }
-
-  virtual bool isPop(const MCInst &Inst) const { return false; }
-
-  /// Return true if the instruction is used to terminate an indirect branch.
-  virtual bool isTerminateBranch(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return the width, in bytes, of the memory access performed by \p Inst, if
-  /// this is a pop instruction. Return zero otherwise.
-  virtual int getPopSize(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  virtual bool isPush(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return the width, in bytes, of the memory access performed by \p Inst, if
-  /// this is a push instruction. Return zero otherwise.
-  virtual int getPushSize(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  virtual bool isSUB(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isLEA64r(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isLeave(const MCInst &Inst) const { return false; }
-
-  virtual bool isADRP(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isADR(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual void getADRReg(const MCInst &Inst, MCPhysReg &RegName) const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual bool isMoveMem2Reg(const MCInst &Inst) const { return false; }
-
-  virtual bool isLoad(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isStore(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isCleanRegXOR(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isPacked(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// If non-zero, this is used to fill the executable space with instructions
-  /// that will trap. Defaults to 0.
-  virtual unsigned getTrapFillValue() const { return 0; }
-
-  /// Interface and basic functionality of a MCInstMatcher. The idea is to make
-  /// it easy to match one or more MCInsts against a tree-like pattern and
-  /// extract the fragment operands. Example:
-  ///
-  ///   auto IndJmpMatcher =
-  ///       matchIndJmp(matchAdd(matchAnyOperand(), matchAnyOperand()));
-  ///   if (!IndJmpMatcher->match(...))
-  ///     return false;
-  ///
-  /// This matches an indirect jump whose target register is defined by an
-  /// add to form the target address. Matchers should also allow extraction
-  /// of operands, for example:
-  ///
-  ///   uint64_t Scale;
-  ///   auto IndJmpMatcher = BC.MIA->matchIndJmp(
-  ///       BC.MIA->matchAnyOperand(), BC.MIA->matchImm(Scale),
-  ///       BC.MIA->matchReg(), BC.MIA->matchAnyOperand());
-  ///   if (!IndJmpMatcher->match(...))
-  ///     return false;
-  ///
-  /// Here we are interesting in extracting the scale immediate in an indirect
-  /// jump fragment.
-  ///
-  struct MCInstMatcher {
-    MutableArrayRef<MCInst> InstrWindow;
-    MutableArrayRef<MCInst>::iterator CurInst;
-    virtual ~MCInstMatcher() {}
-
-    /// Returns true if the pattern is matched. Needs MCRegisterInfo and
-    /// MCInstrAnalysis for analysis. InstrWindow contains an array
-    /// where the last instruction is always the instruction to start matching
-    /// against a fragment, potentially matching more instructions before it.
-    /// If OpNum is greater than 0, we will not match against the last
-    /// instruction itself but against an operand of the last instruction given
-    /// by the index OpNum. If this operand is a register, we will immediately
-    /// look for a previous instruction defining this register and match against
-    /// it instead. This parent member function contains common bookkeeping
-    /// required to implement this behavior.
-    virtual bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
-                       MutableArrayRef<MCInst> InInstrWindow, int OpNum) {
-      InstrWindow = InInstrWindow;
-      CurInst = InstrWindow.end();
-
-      if (!next())
-        return false;
-
-      if (OpNum < 0)
-        return true;
+    virtual void createPopFlags(MCInst &Inst, unsigned Size) const {
+        llvm_unreachable("not implemented");
+    }
 
-      if (static_cast<unsigned int>(OpNum) >=
-          MCPlus::getNumPrimeOperands(*CurInst))
+    virtual bool createDirectCall(MCInst &Inst, const MCSymbol *Target,
+                                  MCContext *Ctx, bool IsTailCall) {
+        llvm_unreachable("not implemented");
         return false;
+    }
 
-      const MCOperand &Op = CurInst->getOperand(OpNum);
-      if (!Op.isReg())
-        return true;
+    virtual MCPhysReg getX86R11() const { llvm_unreachable("not implemented"); }
 
-      MCPhysReg Reg = Op.getReg();
-      while (next()) {
-        const MCInstrDesc &InstrDesc = MIA.Info->get(CurInst->getOpcode());
-        if (InstrDesc.hasDefOfPhysReg(*CurInst, Reg, MRI)) {
-          InstrWindow = InstrWindow.slice(0, CurInst - InstrWindow.begin() + 1);
-          return true;
-        }
-      }
-      return false;
+    /// Create increment contents of target by 1 for Instrumentation
+    virtual InstructionListType createInstrIncMemory(const MCSymbol *Target,
+                                                     MCContext      *Ctx,
+                                                     bool IsLeaf) const {
+        llvm_unreachable("not implemented");
+        return InstructionListType();
+    }
+
+    /// Return a register number that is guaranteed to not match with
+    /// any real register on the underlying architecture.
+    MCPhysReg getNoRegister() const { return MCRegister::NoRegister; }
+
+    /// Return a register corresponding to a function integer argument \p ArgNo
+    /// if the argument is passed in a register. Or return the result of
+    /// getNoRegister() otherwise. The enumeration starts at 0.
+    ///
+    /// Note: this should depend on a used calling convention.
+    virtual MCPhysReg getIntArgRegister(unsigned ArgNo) const {
+        llvm_unreachable("not implemented");
+    }
+
+    virtual bool isIndirectCall(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool isCall(const MCInst &Inst) const {
+        return Analysis->isCall(Inst) || isTailCall(Inst);
+    }
+
+    virtual bool isReturn(const MCInst &Inst) const {
+        return Analysis->isReturn(Inst);
+    }
+
+    virtual bool isTerminator(const MCInst &Inst) const {
+        return Analysis->isTerminator(Inst);
+    }
+
+    virtual bool isNoop(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
     }
 
-    /// If successfully matched, calling this function will add an annotation
-    /// to all instructions that were matched. This is used to easily tag
-    /// instructions for deletion and implement match-and-replace operations.
-    virtual void annotate(MCPlusBuilder &MIA, StringRef Annotation) {}
+    virtual bool isBreakpoint(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool isPrefix(const MCInst &Inst) const { return false; }
+
+    virtual bool isRep(const MCInst &Inst) const { return false; }
 
-    /// Moves internal instruction iterator to the next instruction, walking
-    /// backwards for pattern matching (effectively the previous instruction in
-    /// regular order).
-    bool next() {
-      if (CurInst == InstrWindow.begin())
+    virtual bool deleteREPPrefix(MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
-      --CurInst;
-      return true;
     }
-  };
 
-  /// Matches any operand
-  struct AnyOperandMatcher : MCInstMatcher {
-    MCOperand &Op;
-    AnyOperandMatcher(MCOperand &Op) : Op(Op) {}
+    virtual bool isEHLabel(const MCInst &Inst) const {
+        return Inst.getOpcode() == TargetOpcode::EH_LABEL;
+    }
+
+    virtual bool isPop(const MCInst &Inst) const { return false; }
+
+    /// Return true if the instruction is used to terminate an indirect branch.
+    virtual bool isTerminateBranch(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
 
-    bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
-               MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
-      auto I = InInstrWindow.end();
-      if (I == InInstrWindow.begin())
+    /// Return the width, in bytes, of the memory access performed by \p Inst,
+    /// if this is a pop instruction. Return zero otherwise.
+    virtual int getPopSize(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+    virtual bool isPushRBP(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
-      --I;
-      if (OpNum < 0 ||
-          static_cast<unsigned int>(OpNum) >= MCPlus::getNumPrimeOperands(*I))
+    }
+
+    virtual bool isPush(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
-      Op = I->getOperand(OpNum);
-      return true;
     }
-  };
 
-  /// Matches operands that are immediates
-  struct ImmMatcher : MCInstMatcher {
-    uint64_t &Imm;
-    ImmMatcher(uint64_t &Imm) : Imm(Imm) {}
+    /// Return the width, in bytes, of the memory access performed by \p Inst,
+    /// if this is a push instruction. Return zero otherwise.
+    virtual int getPushSize(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    virtual bool isSUB(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
 
-    bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
-               MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
-      if (!MCInstMatcher::match(MRI, MIA, InInstrWindow, OpNum))
+    virtual bool isLEA64r(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
+    }
+
+    virtual bool isLeave(const MCInst &Inst) const { return false; }
 
-      if (OpNum < 0)
+    virtual bool isADRP(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
-      const MCOperand &Op = CurInst->getOperand(OpNum);
-      if (!Op.isImm())
+    }
+
+    virtual bool isADR(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
-      Imm = Op.getImm();
-      return true;
     }
-  };
 
-  /// Matches operands that are MCSymbols
-  struct SymbolMatcher : MCInstMatcher {
-    const MCSymbol *&Sym;
-    SymbolMatcher(const MCSymbol *&Sym) : Sym(Sym) {}
+    virtual void getADRReg(const MCInst &Inst, MCPhysReg &RegName) const {
+        llvm_unreachable("not implemented");
+    }
+
+    virtual bool isMoveMem2Reg(const MCInst &Inst) const { return false; }
+
+    virtual bool isLoad(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
 
-    bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
-               MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
-      if (!MCInstMatcher::match(MRI, MIA, InInstrWindow, OpNum))
+    virtual bool isRetInteger(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
+    }
 
-      if (OpNum < 0)
+    virtual bool isRetFloat(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
-      Sym = MIA.getTargetSymbol(*CurInst, OpNum);
-      return Sym != nullptr;
     }
-  };
 
-  /// Matches operands that are registers
-  struct RegMatcher : MCInstMatcher {
-    MCPhysReg &Reg;
-    RegMatcher(MCPhysReg &Reg) : Reg(Reg) {}
+    virtual bool isPrefetchable(const MCInst &Inst, bool &IsWide) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+    virtual bool isStackBaseSet(const MCInst &inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
 
-    bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
-               MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
-      auto I = InInstrWindow.end();
-      if (I == InInstrWindow.begin())
+    virtual bool isStore(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
-      --I;
-      if (OpNum < 0 ||
-          static_cast<unsigned int>(OpNum) >= MCPlus::getNumPrimeOperands(*I))
+    }
+
+    virtual bool isCleanRegXOR(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
-      const MCOperand &Op = I->getOperand(OpNum);
-      if (!Op.isReg())
+    }
+
+    virtual bool isPacked(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
         return false;
-      Reg = Op.getReg();
-      return true;
     }
-  };
 
-  std::unique_ptr<MCInstMatcher> matchAnyOperand(MCOperand &Op) const {
-    return std::unique_ptr<MCInstMatcher>(new AnyOperandMatcher(Op));
-  }
+    /// If non-zero, this is used to fill the executable space with instructions
+    /// that will trap. Defaults to 0.
+    virtual unsigned getTrapFillValue() const { return 0; }
+
+    /// Interface and basic functionality of a MCInstMatcher. The idea is to
+    /// make it easy to match one or more MCInsts against a tree-like pattern
+    /// and extract the fragment operands. Example:
+    ///
+    ///   auto IndJmpMatcher =
+    ///       matchIndJmp(matchAdd(matchAnyOperand(), matchAnyOperand()));
+    ///   if (!IndJmpMatcher->match(...))
+    ///     return false;
+    ///
+    /// This matches an indirect jump whose target register is defined by an
+    /// add to form the target address. Matchers should also allow extraction
+    /// of operands, for example:
+    ///
+    ///   uint64_t Scale;
+    ///   auto IndJmpMatcher = BC.MIA->matchIndJmp(
+    ///       BC.MIA->matchAnyOperand(), BC.MIA->matchImm(Scale),
+    ///       BC.MIA->matchReg(), BC.MIA->matchAnyOperand());
+    ///   if (!IndJmpMatcher->match(...))
+    ///     return false;
+    ///
+    /// Here we are interesting in extracting the scale immediate in an indirect
+    /// jump fragment.
+    ///
+    struct MCInstMatcher {
+        MutableArrayRef<MCInst>           InstrWindow;
+        MutableArrayRef<MCInst>::iterator CurInst;
+        virtual ~MCInstMatcher() {}
+
+        /// Returns true if the pattern is matched. Needs MCRegisterInfo and
+        /// MCInstrAnalysis for analysis. InstrWindow contains an array
+        /// where the last instruction is always the instruction to start
+        /// matching against a fragment, potentially matching more instructions
+        /// before it. If OpNum is greater than 0, we will not match against the
+        /// last instruction itself but against an operand of the last
+        /// instruction given by the index OpNum. If this operand is a register,
+        /// we will immediately look for a previous instruction defining this
+        /// register and match against it instead. This parent member function
+        /// contains common bookkeeping required to implement this behavior.
+        virtual bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
+                           MutableArrayRef<MCInst> InInstrWindow, int OpNum) {
+            InstrWindow = InInstrWindow;
+            CurInst     = InstrWindow.end();
+
+            if (!next())
+                return false;
+
+            if (OpNum < 0)
+                return true;
+
+            if (static_cast<unsigned int>(OpNum) >=
+                MCPlus::getNumPrimeOperands(*CurInst))
+                return false;
+
+            const MCOperand &Op = CurInst->getOperand(OpNum);
+            if (!Op.isReg())
+                return true;
+
+            MCPhysReg Reg = Op.getReg();
+            while (next()) {
+                const MCInstrDesc &InstrDesc =
+                    MIA.Info->get(CurInst->getOpcode());
+                if (InstrDesc.hasDefOfPhysReg(*CurInst, Reg, MRI)) {
+                    InstrWindow =
+                        InstrWindow.slice(0, CurInst - InstrWindow.begin() + 1);
+                    return true;
+                }
+            }
+            return false;
+        }
+
+        /// If successfully matched, calling this function will add an
+        /// annotation to all instructions that were matched. This is used to
+        /// easily tag instructions for deletion and implement match-and-replace
+        /// operations.
+        virtual void annotate(MCPlusBuilder &MIA, StringRef Annotation) {}
+
+        /// Moves internal instruction iterator to the next instruction, walking
+        /// backwards for pattern matching (effectively the previous instruction
+        /// in regular order).
+        bool next() {
+            if (CurInst == InstrWindow.begin())
+                return false;
+            --CurInst;
+            return true;
+        }
+    };
+
+    /// Matches any operand
+    struct AnyOperandMatcher : MCInstMatcher {
+        MCOperand &Op;
+        AnyOperandMatcher(MCOperand &Op) : Op(Op) {}
+
+        bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
+                   MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
+            auto I = InInstrWindow.end();
+            if (I == InInstrWindow.begin())
+                return false;
+            --I;
+            if (OpNum < 0 || static_cast<unsigned int>(OpNum) >=
+                                 MCPlus::getNumPrimeOperands(*I))
+                return false;
+            Op = I->getOperand(OpNum);
+            return true;
+        }
+    };
+
+    /// Matches operands that are immediates
+    struct ImmMatcher : MCInstMatcher {
+        uint64_t &Imm;
+        ImmMatcher(uint64_t &Imm) : Imm(Imm) {}
+
+        bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
+                   MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
+            if (!MCInstMatcher::match(MRI, MIA, InInstrWindow, OpNum))
+                return false;
+
+            if (OpNum < 0)
+                return false;
+            const MCOperand &Op = CurInst->getOperand(OpNum);
+            if (!Op.isImm())
+                return false;
+            Imm = Op.getImm();
+            return true;
+        }
+    };
+
+    /// Matches operands that are MCSymbols
+    struct SymbolMatcher : MCInstMatcher {
+        const MCSymbol *&Sym;
+        SymbolMatcher(const MCSymbol *&Sym) : Sym(Sym) {}
+
+        bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
+                   MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
+            if (!MCInstMatcher::match(MRI, MIA, InInstrWindow, OpNum))
+                return false;
+
+            if (OpNum < 0)
+                return false;
+            Sym = MIA.getTargetSymbol(*CurInst, OpNum);
+            return Sym != nullptr;
+        }
+    };
+
+    /// Matches operands that are registers
+    struct RegMatcher : MCInstMatcher {
+        MCPhysReg &Reg;
+        RegMatcher(MCPhysReg &Reg) : Reg(Reg) {}
+
+        bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIA,
+                   MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
+            auto I = InInstrWindow.end();
+            if (I == InInstrWindow.begin())
+                return false;
+            --I;
+            if (OpNum < 0 || static_cast<unsigned int>(OpNum) >=
+                                 MCPlus::getNumPrimeOperands(*I))
+                return false;
+            const MCOperand &Op = I->getOperand(OpNum);
+            if (!Op.isReg())
+                return false;
+            Reg = Op.getReg();
+            return true;
+        }
+    };
+
+    std::unique_ptr<MCInstMatcher> matchAnyOperand(MCOperand &Op) const {
+        return std::unique_ptr<MCInstMatcher>(new AnyOperandMatcher(Op));
+    }
+
+    std::unique_ptr<MCInstMatcher> matchAnyOperand() const {
+        static MCOperand Unused;
+        return std::unique_ptr<MCInstMatcher>(new AnyOperandMatcher(Unused));
+    }
+
+    std::unique_ptr<MCInstMatcher> matchReg(MCPhysReg &Reg) const {
+        return std::unique_ptr<MCInstMatcher>(new RegMatcher(Reg));
+    }
+
+    std::unique_ptr<MCInstMatcher> matchReg() const {
+        static MCPhysReg Unused;
+        return std::unique_ptr<MCInstMatcher>(new RegMatcher(Unused));
+    }
 
-  std::unique_ptr<MCInstMatcher> matchAnyOperand() const {
-    static MCOperand Unused;
-    return std::unique_ptr<MCInstMatcher>(new AnyOperandMatcher(Unused));
-  }
+    std::unique_ptr<MCInstMatcher> matchImm(uint64_t &Imm) const {
+        return std::unique_ptr<MCInstMatcher>(new ImmMatcher(Imm));
+    }
 
-  std::unique_ptr<MCInstMatcher> matchReg(MCPhysReg &Reg) const {
-    return std::unique_ptr<MCInstMatcher>(new RegMatcher(Reg));
-  }
+    std::unique_ptr<MCInstMatcher> matchImm() const {
+        static uint64_t Unused;
+        return std::unique_ptr<MCInstMatcher>(new ImmMatcher(Unused));
+    }
 
-  std::unique_ptr<MCInstMatcher> matchReg() const {
-    static MCPhysReg Unused;
-    return std::unique_ptr<MCInstMatcher>(new RegMatcher(Unused));
-  }
+    std::unique_ptr<MCInstMatcher> matchSymbol(const MCSymbol *&Sym) const {
+        return std::unique_ptr<MCInstMatcher>(new SymbolMatcher(Sym));
+    }
 
-  std::unique_ptr<MCInstMatcher> matchImm(uint64_t &Imm) const {
-    return std::unique_ptr<MCInstMatcher>(new ImmMatcher(Imm));
-  }
+    std::unique_ptr<MCInstMatcher> matchSymbol() const {
+        static const MCSymbol *Unused;
+        return std::unique_ptr<MCInstMatcher>(new SymbolMatcher(Unused));
+    }
 
-  std::unique_ptr<MCInstMatcher> matchImm() const {
-    static uint64_t Unused;
-    return std::unique_ptr<MCInstMatcher>(new ImmMatcher(Unused));
-  }
+    virtual std::unique_ptr<MCInstMatcher>
+    matchIndJmp(std::unique_ptr<MCInstMatcher> Target) const {
+        llvm_unreachable("not implemented");
+        return nullptr;
+    }
 
-  std::unique_ptr<MCInstMatcher> matchSymbol(const MCSymbol *&Sym) const {
-    return std::unique_ptr<MCInstMatcher>(new SymbolMatcher(Sym));
-  }
+    virtual std::unique_ptr<MCInstMatcher>
+    matchIndJmp(std::unique_ptr<MCInstMatcher> Base,
+                std::unique_ptr<MCInstMatcher> Scale,
+                std::unique_ptr<MCInstMatcher> Index,
+                std::unique_ptr<MCInstMatcher> Offset) const {
+        llvm_unreachable("not implemented");
+        return nullptr;
+    }
 
-  std::unique_ptr<MCInstMatcher> matchSymbol() const {
-    static const MCSymbol *Unused;
-    return std::unique_ptr<MCInstMatcher>(new SymbolMatcher(Unused));
-  }
+    virtual std::unique_ptr<MCInstMatcher>
+    matchAdd(std::unique_ptr<MCInstMatcher> A,
+             std::unique_ptr<MCInstMatcher> B) const {
+        llvm_unreachable("not implemented");
+        return nullptr;
+    }
 
-  virtual std::unique_ptr<MCInstMatcher>
-  matchIndJmp(std::unique_ptr<MCInstMatcher> Target) const {
-    llvm_unreachable("not implemented");
-    return nullptr;
-  }
+    virtual std::unique_ptr<MCInstMatcher>
+    matchLoadAddr(std::unique_ptr<MCInstMatcher> Target) const {
+        llvm_unreachable("not implemented");
+        return nullptr;
+    }
 
-  virtual std::unique_ptr<MCInstMatcher>
-  matchIndJmp(std::unique_ptr<MCInstMatcher> Base,
+    virtual std::unique_ptr<MCInstMatcher>
+    matchLoad(std::unique_ptr<MCInstMatcher> Base,
               std::unique_ptr<MCInstMatcher> Scale,
               std::unique_ptr<MCInstMatcher> Index,
               std::unique_ptr<MCInstMatcher> Offset) const {
-    llvm_unreachable("not implemented");
-    return nullptr;
-  }
-
-  virtual std::unique_ptr<MCInstMatcher>
-  matchAdd(std::unique_ptr<MCInstMatcher> A,
-           std::unique_ptr<MCInstMatcher> B) const {
-    llvm_unreachable("not implemented");
-    return nullptr;
-  }
-
-  virtual std::unique_ptr<MCInstMatcher>
-  matchLoadAddr(std::unique_ptr<MCInstMatcher> Target) const {
-    llvm_unreachable("not implemented");
-    return nullptr;
-  }
-
-  virtual std::unique_ptr<MCInstMatcher>
-  matchLoad(std::unique_ptr<MCInstMatcher> Base,
-            std::unique_ptr<MCInstMatcher> Scale,
-            std::unique_ptr<MCInstMatcher> Index,
-            std::unique_ptr<MCInstMatcher> Offset) const {
-    llvm_unreachable("not implemented");
-    return nullptr;
-  }
-
-  /// \brief Given a branch instruction try to get the address the branch
-  /// targets. Return true on success, and the address in Target.
-  virtual bool evaluateBranch(const MCInst &Inst, uint64_t Addr, uint64_t Size,
-                              uint64_t &Target) const {
-    return Analysis->evaluateBranch(Inst, Addr, Size, Target);
-  }
-
-  /// Return true if one of the operands of the \p Inst instruction uses
-  /// PC-relative addressing.
-  /// Note that PC-relative branches do not fall into this category.
-  virtual bool hasPCRelOperand(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return a number of the operand representing a memory.
-  /// Return -1 if the instruction doesn't have an explicit memory field.
-  virtual int getMemoryOperandNo(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return -1;
-  }
-
-  /// Return true if the instruction is encoded using EVEX (AVX-512).
-  virtual bool hasEVEXEncoding(const MCInst &Inst) const { return false; }
-
-  /// Return true if a pair of instructions represented by \p Insts
-  /// could be fused into a single uop.
-  virtual bool isMacroOpFusionPair(ArrayRef<MCInst> Insts) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  struct X86MemOperand {
-    unsigned BaseRegNum;
-    int64_t ScaleImm;
-    unsigned IndexRegNum;
-    int64_t DispImm;
-    unsigned SegRegNum;
-    const MCExpr *DispExpr = nullptr;
-  };
-
-  /// Given an instruction with (compound) memory operand, evaluate and return
-  /// the corresponding values. Note that the operand could be in any position,
-  /// but there is an assumption there's only one compound memory operand.
-  /// Return true upon success, return false if the instruction does not have
-  /// a memory operand.
-  ///
-  /// Since a Displacement field could be either an immediate or an expression,
-  /// the function sets either \p DispImm or \p DispExpr value.
-  virtual std::optional<X86MemOperand>
-  evaluateX86MemoryOperand(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return std::nullopt;
-  }
-
-  /// Given an instruction with memory addressing attempt to statically compute
-  /// the address being accessed. Return true on success, and the address in
-  /// \p Target.
-  ///
-  /// For RIP-relative addressing the caller is required to pass instruction
-  /// \p Address and \p Size.
-  virtual bool evaluateMemOperandTarget(const MCInst &Inst, uint64_t &Target,
-                                        uint64_t Address = 0,
-                                        uint64_t Size = 0) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return operand iterator pointing to displacement in the compound memory
-  /// operand if such exists. Return Inst.end() otherwise.
-  virtual MCInst::iterator getMemOperandDisp(MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return Inst.end();
-  }
-
-  /// Analyze \p Inst and return true if this instruction accesses \p Size
-  /// bytes of the stack frame at position \p StackOffset. \p IsLoad and
-  /// \p IsStore are set accordingly. If both are set, it means it is a
-  /// instruction that reads and updates the same memory location. \p Reg is set
-  /// to the source register in case of a store or destination register in case
-  /// of a load. If the store does not use a source register, \p SrcImm will
-  /// contain the source immediate and \p IsStoreFromReg will be set to false.
-  /// \p Simple is false if the instruction is not fully understood by
-  /// companion functions "replaceMemOperandWithImm" or
-  /// "replaceMemOperandWithReg".
-  virtual bool isStackAccess(const MCInst &Inst, bool &IsLoad, bool &IsStore,
-                             bool &IsStoreFromReg, MCPhysReg &Reg,
-                             int32_t &SrcImm, uint16_t &StackPtrReg,
-                             int64_t &StackOffset, uint8_t &Size,
-                             bool &IsSimple, bool &IsIndexed) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Convert a stack accessing load/store instruction in \p Inst to a PUSH
-  /// or POP saving/restoring the source/dest reg in \p Inst. The original
-  /// stack offset in \p Inst is ignored.
-  virtual void changeToPushOrPop(MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Identify stack adjustment instructions -- those that change the stack
-  /// pointer by adding or subtracting an immediate.
-  virtual bool isStackAdjustment(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Use \p Input1 or Input2 as the current value for the input
-  /// register and put in \p Output the changes incurred by executing
-  /// \p Inst. Return false if it was not possible to perform the
-  /// evaluation. evaluateStackOffsetExpr is restricted to operations
-  /// that have associativity with addition. Its intended usage is for
-  /// evaluating stack offset changes. In these cases, expressions
-  /// appear in the form of (x + offset) OP constant, where x is an
-  /// unknown base (such as stack base) but offset and constant are
-  /// known. In these cases, \p Output represents the new stack offset
-  /// after executing \p Inst. Because we don't know x, we can't
-  /// evaluate operations such as multiply or AND/OR, e.g. (x +
-  /// offset) OP constant is not the same as x + (offset OP constant).
-  virtual bool
-  evaluateStackOffsetExpr(const MCInst &Inst, int64_t &Output,
-                          std::pair<MCPhysReg, int64_t> Input1,
-                          std::pair<MCPhysReg, int64_t> Input2) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isRegToRegMove(const MCInst &Inst, MCPhysReg &From,
-                              MCPhysReg &To) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual MCPhysReg getStackPointer() const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  virtual MCPhysReg getFramePointer() const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  virtual MCPhysReg getFlagsReg() const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  /// Return true if \p Inst is a instruction that copies either the frame
-  /// pointer or the stack pointer to another general purpose register or
-  /// writes it to a memory location.
-  virtual bool escapesVariable(const MCInst &Inst, bool HasFramePointer) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Discard operand \p OpNum replacing it by a new MCOperand that is a
-  /// MCExpr referencing \p Symbol + \p Addend.
-  virtual bool setOperandToSymbolRef(MCInst &Inst, int OpNum,
-                                     const MCSymbol *Symbol, int64_t Addend,
-                                     MCContext *Ctx, uint64_t RelType) const;
-
-  /// Replace an immediate operand in the instruction \p Inst with a reference
-  /// of the passed \p Symbol plus \p Addend. If the instruction does not have
-  /// an immediate operand or has more than one - then return false. Otherwise
-  /// return true.
-  virtual bool replaceImmWithSymbolRef(MCInst &Inst, const MCSymbol *Symbol,
-                                       int64_t Addend, MCContext *Ctx,
-                                       int64_t &Value, uint64_t RelType) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  // Replace Register in Inst with Imm. Returns true if successful
-  virtual bool replaceRegWithImm(MCInst &Inst, unsigned Register,
-                                 int64_t Imm) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  // Replace ToReplace in Inst with ReplaceWith. Returns true if successful
-  virtual bool replaceRegWithReg(MCInst &Inst, unsigned ToReplace,
-                                 unsigned ReplaceWith) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Add \p NewImm to the current immediate operand of \p Inst. If it is a
-  /// memory accessing instruction, this immediate is the memory address
-  /// displacement. Otherwise, the target operand is the first immediate
-  /// operand found in \p Inst. Return false if no imm operand found.
-  virtual bool addToImm(MCInst &Inst, int64_t &Amt, MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Replace the compound memory operand of Inst with an immediate operand.
-  /// The value of the immediate operand is computed by reading the \p
-  /// ConstantData array starting from \p offset and assuming little-endianess.
-  /// Return true on success. The given instruction is modified in place.
-  virtual bool replaceMemOperandWithImm(MCInst &Inst, StringRef ConstantData,
-                                        uint64_t Offset) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Same as replaceMemOperandWithImm, but for registers.
-  virtual bool replaceMemOperandWithReg(MCInst &Inst, MCPhysReg RegNum) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return true if a move instruction moves a register to itself.
-  virtual bool isRedundantMove(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return true if the instruction is a tail call.
-  bool isTailCall(const MCInst &Inst) const;
-
-  /// Return true if the instruction is a call with an exception handling info.
-  virtual bool isInvoke(const MCInst &Inst) const {
-    return isCall(Inst) && getEHInfo(Inst);
-  }
-
-  /// Return true if \p Inst is an instruction that potentially traps when
-  /// working with addresses not aligned to the size of the operand.
-  virtual bool requiresAlignedAddress(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return handler and action info for invoke instruction if present.
-  std::optional<MCPlus::MCLandingPad> getEHInfo(const MCInst &Inst) const;
-
-  /// Add handler and action info for call instruction.
-  void addEHInfo(MCInst &Inst, const MCPlus::MCLandingPad &LP);
-
-  /// Update exception-handling info for the invoke instruction \p Inst.
-  /// Return true on success and false otherwise, e.g. if the instruction is
-  /// not an invoke.
-  bool updateEHInfo(MCInst &Inst, const MCPlus::MCLandingPad &LP);
-
-  /// Return non-negative GNU_args_size associated with the instruction
-  /// or -1 if there's no associated info.
-  int64_t getGnuArgsSize(const MCInst &Inst) const;
-
-  /// Add the value of GNU_args_size to Inst if it already has EH info.
-  void addGnuArgsSize(MCInst &Inst, int64_t GnuArgsSize,
-                      AllocatorIdTy AllocId = 0);
+        llvm_unreachable("not implemented");
+        return nullptr;
+    }
+
+    /// \brief Given a branch instruction try to get the address the branch
+    /// targets. Return true on success, and the address in Target.
+    virtual bool evaluateBranch(const MCInst &Inst, uint64_t Addr,
+                                uint64_t Size, uint64_t &Target) const {
+        return Analysis->evaluateBranch(Inst, Addr, Size, Target);
+    }
+
+    /// Return true if one of the operands of the \p Inst instruction uses
+    /// PC-relative addressing.
+    /// Note that PC-relative branches do not fall into this category.
+    virtual bool hasPCRelOperand(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Return a number of the operand representing a memory.
+    /// Return -1 if the instruction doesn't have an explicit memory field.
+    virtual int getMemoryOperandNo(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return -1;
+    }
+
+    /// Return true if the instruction is encoded using EVEX (AVX-512).
+    virtual bool hasEVEXEncoding(const MCInst &Inst) const { return false; }
+
+    /// Return true if a pair of instructions represented by \p Insts
+    /// could be fused into a single uop.
+    virtual bool isMacroOpFusionPair(ArrayRef<MCInst> Insts) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    struct X86MemOperand {
+        unsigned      BaseRegNum;
+        int64_t       ScaleImm;
+        unsigned      IndexRegNum;
+        int64_t       DispImm;
+        unsigned      SegRegNum;
+        const MCExpr *DispExpr = nullptr;
+    };
+
+    /// Given an instruction with (compound) memory operand, evaluate and return
+    /// the corresponding values. Note that the operand could be in any
+    /// position, but there is an assumption there's only one compound memory
+    /// operand. Return true upon success, return false if the instruction does
+    /// not have a memory operand.
+    ///
+    /// Since a Displacement field could be either an immediate or an
+    /// expression, the function sets either \p DispImm or \p DispExpr value.
+    virtual std::optional<X86MemOperand>
+    evaluateX86MemoryOperand(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return std::nullopt;
+    }
+
+    /// Given an instruction with memory addressing attempt to statically
+    /// compute the address being accessed. Return true on success, and the
+    /// address in \p Target.
+    ///
+    /// For RIP-relative addressing the caller is required to pass instruction
+    /// \p Address and \p Size.
+    virtual bool evaluateMemOperandTarget(const MCInst &Inst, uint64_t &Target,
+                                          uint64_t Address = 0,
+                                          uint64_t Size    = 0) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
 
-  /// Return jump table addressed by this instruction.
-  uint64_t getJumpTable(const MCInst &Inst) const;
-
-  /// Return index register for instruction that uses a jump table.
-  uint16_t getJumpTableIndexReg(const MCInst &Inst) const;
-
-  /// Set jump table addressed by this instruction.
-  bool setJumpTable(MCInst &Inst, uint64_t Value, uint16_t IndexReg,
-                    AllocatorIdTy AllocId = 0);
-
-  /// Disassociate instruction with a jump table.
-  bool unsetJumpTable(MCInst &Inst);
-
-  /// Return destination of conditional tail call instruction if \p Inst is one.
-  std::optional<uint64_t> getConditionalTailCall(const MCInst &Inst) const;
-
-  /// Mark the \p Instruction as a conditional tail call, and set its
-  /// destination address if it is known. If \p Instruction was already marked,
-  /// update its destination with \p Dest.
-  bool setConditionalTailCall(MCInst &Inst, uint64_t Dest = 0);
-
-  /// If \p Inst was marked as a conditional tail call convert it to a regular
-  /// branch. Return true if the instruction was converted.
-  bool unsetConditionalTailCall(MCInst &Inst);
-
-  /// Return offset of \p Inst in the original function, if available.
-  std::optional<uint32_t> getOffset(const MCInst &Inst) const;
-
-  /// Return the offset if the annotation is present, or \p Default otherwise.
-  uint32_t getOffsetWithDefault(const MCInst &Inst, uint32_t Default) const;
-
-  /// Set offset of \p Inst in the original function.
-  bool setOffset(MCInst &Inst, uint32_t Offset, AllocatorIdTy AllocatorId = 0);
-
-  /// Remove offset annotation.
-  bool clearOffset(MCInst &Inst);
-
-  /// Return MCSymbol that represents a target of this instruction at a given
-  /// operand number \p OpNum. If there's no symbol associated with
-  /// the operand - return nullptr.
-  virtual const MCSymbol *getTargetSymbol(const MCInst &Inst,
-                                          unsigned OpNum = 0) const {
-    llvm_unreachable("not implemented");
-    return nullptr;
-  }
-
-  /// Return MCSymbol extracted from a target expression
-  virtual const MCSymbol *getTargetSymbol(const MCExpr *Expr) const {
-    return &cast<const MCSymbolRefExpr>(Expr)->getSymbol();
-  }
-
-  /// Return addend that represents an offset from MCSymbol target
-  /// of this instruction at a given operand number \p OpNum.
-  /// If there's no symbol associated with  the operand - return 0
-  virtual int64_t getTargetAddend(const MCInst &Inst,
-                                  unsigned OpNum = 0) const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  /// Return MCSymbol addend extracted from a target expression
-  virtual int64_t getTargetAddend(const MCExpr *Expr) const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  /// Return MCSymbol/offset extracted from a target expression
-  virtual std::pair<const MCSymbol *, uint64_t>
-  getTargetSymbolInfo(const MCExpr *Expr) const {
-    if (auto *SymExpr = dyn_cast<MCSymbolRefExpr>(Expr)) {
-      return std::make_pair(&SymExpr->getSymbol(), 0);
-    } else if (auto *BinExpr = dyn_cast<MCBinaryExpr>(Expr)) {
-      const auto *SymExpr = dyn_cast<MCSymbolRefExpr>(BinExpr->getLHS());
-      const auto *ConstExpr = dyn_cast<MCConstantExpr>(BinExpr->getRHS());
-      if (BinExpr->getOpcode() == MCBinaryExpr::Add && SymExpr && ConstExpr)
-        return std::make_pair(&SymExpr->getSymbol(), ConstExpr->getValue());
-    }
-    return std::make_pair(nullptr, 0);
-  }
-
-  /// Replace displacement in compound memory operand with given \p Operand.
-  virtual bool replaceMemOperandDisp(MCInst &Inst, MCOperand Operand) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return the MCExpr used for absolute references in this target
-  virtual const MCExpr *getTargetExprFor(MCInst &Inst, const MCExpr *Expr,
-                                         MCContext &Ctx,
+    /// Return operand iterator pointing to displacement in the compound memory
+    /// operand if such exists. Return Inst.end() otherwise.
+    virtual MCInst::iterator getMemOperandDisp(MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return Inst.end();
+    }
+
+    /// Analyze \p Inst and return true if this instruction accesses \p Size
+    /// bytes of the stack frame at position \p StackOffset. \p IsLoad and
+    /// \p IsStore are set accordingly. If both are set, it means it is a
+    /// instruction that reads and updates the same memory location. \p Reg is
+    /// set to the source register in case of a store or destination register in
+    /// case of a load. If the store does not use a source register, \p SrcImm
+    /// will contain the source immediate and \p IsStoreFromReg will be set to
+    /// false. \p Simple is false if the instruction is not fully understood by
+    /// companion functions "replaceMemOperandWithImm" or
+    /// "replaceMemOperandWithReg".
+    virtual bool isStackAccess(const MCInst &Inst, bool &IsLoad, bool &IsStore,
+                               bool &IsStoreFromReg, MCPhysReg &Reg,
+                               int32_t &SrcImm, uint16_t &StackPtrReg,
+                               int64_t &StackOffset, uint8_t &Size,
+                               bool &IsSimple, bool &IsIndexed) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Convert a stack accessing load/store instruction in \p Inst to a PUSH
+    /// or POP saving/restoring the source/dest reg in \p Inst. The original
+    /// stack offset in \p Inst is ignored.
+    virtual void changeToPushOrPop(MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Identify stack adjustment instructions -- those that change the stack
+    /// pointer by adding or subtracting an immediate.
+    virtual bool isStackAdjustment(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Use \p Input1 or Input2 as the current value for the input
+    /// register and put in \p Output the changes incurred by executing
+    /// \p Inst. Return false if it was not possible to perform the
+    /// evaluation. evaluateStackOffsetExpr is restricted to operations
+    /// that have associativity with addition. Its intended usage is for
+    /// evaluating stack offset changes. In these cases, expressions
+    /// appear in the form of (x + offset) OP constant, where x is an
+    /// unknown base (such as stack base) but offset and constant are
+    /// known. In these cases, \p Output represents the new stack offset
+    /// after executing \p Inst. Because we don't know x, we can't
+    /// evaluate operations such as multiply or AND/OR, e.g. (x +
+    /// offset) OP constant is not the same as x + (offset OP constant).
+    virtual bool
+    evaluateStackOffsetExpr(const MCInst &Inst, int64_t &Output,
+                            std::pair<MCPhysReg, int64_t> Input1,
+                            std::pair<MCPhysReg, int64_t> Input2) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool isRegToRegMove(const MCInst &Inst, MCPhysReg &From,
+                                MCPhysReg &To) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual MCPhysReg getStackPointer() const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    virtual MCPhysReg getFramePointer() const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    virtual MCPhysReg getFlagsReg() const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    /// Return true if \p Inst is a instruction that copies either the frame
+    /// pointer or the stack pointer to another general purpose register or
+    /// writes it to a memory location.
+    virtual bool escapesVariable(const MCInst &Inst,
+                                 bool          HasFramePointer) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Discard operand \p OpNum replacing it by a new MCOperand that is a
+    /// MCExpr referencing \p Symbol + \p Addend.
+    virtual bool setOperandToSymbolRef(MCInst &Inst, int OpNum,
+                                       const MCSymbol *Symbol, int64_t Addend,
+                                       MCContext *Ctx, uint64_t RelType) const;
+
+    /// Replace an immediate operand in the instruction \p Inst with a reference
+    /// of the passed \p Symbol plus \p Addend. If the instruction does not have
+    /// an immediate operand or has more than one - then return false. Otherwise
+    /// return true.
+    virtual bool replaceImmWithSymbolRef(MCInst &Inst, const MCSymbol *Symbol,
+                                         int64_t Addend, MCContext *Ctx,
+                                         int64_t &Value,
                                          uint64_t RelType) const {
-    return Expr;
-  }
-
-  /// Return a BitVector marking all sub or super registers of \p Reg, including
-  /// itself.
-  virtual const BitVector &getAliases(MCPhysReg Reg,
-                                      bool OnlySmaller = false) const;
-
-  /// Initialize aliases tables.
-  void initAliases();
-
-  /// Initialize register size table.
-  void initSizeMap();
-
-  /// Change \p Regs setting all registers used to pass parameters according
-  /// to the host abi. Do nothing if not implemented.
-  virtual BitVector getRegsUsedAsParams() const {
-    llvm_unreachable("not implemented");
-    return BitVector();
-  }
-
-  /// Change \p Regs setting all registers used as callee-saved according
-  /// to the host abi. Do nothing if not implemented.
-  virtual void getCalleeSavedRegs(BitVector &Regs) const {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Get the default def_in and live_out registers for the function
-  /// Currently only used for the Stoke optimzation
-  virtual void getDefaultDefIn(BitVector &Regs) const {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Similar to getDefaultDefIn
-  virtual void getDefaultLiveOut(BitVector &Regs) const {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Change \p Regs with a bitmask with all general purpose regs
-  virtual void getGPRegs(BitVector &Regs, bool IncludeAlias = true) const {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Change \p Regs with a bitmask with all general purpose regs that can be
-  /// encoded without extra prefix bytes. For x86 only.
-  virtual void getClassicGPRegs(BitVector &Regs) const {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Set of Registers used by the Rep instruction
-  virtual void getRepRegs(BitVector &Regs) const {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Return the register width in bytes (1, 2, 4 or 8)
-  uint8_t getRegSize(MCPhysReg Reg) const { return SizeMap[Reg]; }
-
-  /// For aliased registers, return an alias of \p Reg that has the width of
-  /// \p Size bytes
-  virtual MCPhysReg getAliasSized(MCPhysReg Reg, uint8_t Size) const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  /// For X86, return whether this register is an upper 8-bit register, such as
-  /// AH, BH, etc.
-  virtual bool isUpper8BitReg(MCPhysReg Reg) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// For X86, return whether this instruction has special constraints that
-  /// prevents it from encoding registers that require a REX prefix.
-  virtual bool cannotUseREX(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Modifies the set \p Regs by adding registers \p Inst may rewrite. Caller
-  /// is responsible for passing a valid BitVector with the size equivalent to
-  /// the number of registers in the target.
-  /// Since this function is called many times during clobber analysis, it
-  /// expects the caller to manage BitVector creation to avoid extra overhead.
-  virtual void getClobberedRegs(const MCInst &Inst, BitVector &Regs) const;
-
-  /// Set of all registers touched by this instruction, including implicit uses
-  /// and defs.
-  virtual void getTouchedRegs(const MCInst &Inst, BitVector &Regs) const;
-
-  /// Set of all registers being written to by this instruction -- includes
-  /// aliases but only if they are strictly smaller than the actual reg
-  virtual void getWrittenRegs(const MCInst &Inst, BitVector &Regs) const;
-
-  /// Set of all registers being read by this instruction -- includes aliases
-  /// but only if they are strictly smaller than the actual reg
-  virtual void getUsedRegs(const MCInst &Inst, BitVector &Regs) const;
-
-  /// Set of all src registers -- includes aliases but
-  /// only if they are strictly smaller than the actual reg
-  virtual void getSrcRegs(const MCInst &Inst, BitVector &Regs) const;
-
-  /// Return true if this instruction defines the specified physical
-  /// register either explicitly or implicitly.
-  virtual bool hasDefOfPhysReg(const MCInst &MI, unsigned Reg) const;
-
-  /// Return true if this instruction uses the specified physical
-  /// register either explicitly or implicitly.
-  virtual bool hasUseOfPhysReg(const MCInst &MI, unsigned Reg) const;
-
-  /// Replace displacement in compound memory operand with given \p Label.
-  bool replaceMemOperandDisp(MCInst &Inst, const MCSymbol *Label,
-                             MCContext *Ctx) const {
-    return replaceMemOperandDisp(Inst, Label, 0, Ctx);
-  }
-
-  /// Replace displacement in compound memory operand with given \p Label
-  /// plus addend.
-  bool replaceMemOperandDisp(MCInst &Inst, const MCSymbol *Label,
-                             int64_t Addend, MCContext *Ctx) const {
-    MCInst::iterator MemOpI = getMemOperandDisp(Inst);
-    if (MemOpI == Inst.end())
-      return false;
-    return setOperandToSymbolRef(Inst, MemOpI - Inst.begin(), Label, Addend,
-                                 Ctx, 0);
-  }
-
-  /// Returns how many bits we have in this instruction to encode a PC-rel
-  /// imm.
-  virtual int getPCRelEncodingSize(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  /// Replace instruction opcode to be a tail call instead of jump.
-  virtual bool convertJmpToTailCall(MCInst &Inst) {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Perform any additional actions to transform a (conditional) tail call
-  /// into a (conditional) jump. Assume the target was already replaced with
-  /// a local one, so the default is to do nothing more.
-  virtual bool convertTailCallToJmp(MCInst &Inst) { return true; }
-
-  /// Replace instruction opcode to be a regural call instead of tail call.
-  virtual bool convertTailCallToCall(MCInst &Inst) {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Modify a direct call instruction \p Inst with an indirect call taking
-  /// a destination from a memory location pointed by \p TargetLocation symbol.
-  virtual bool convertCallToIndirectCall(MCInst &Inst,
-                                         const MCSymbol *TargetLocation,
-                                         MCContext *Ctx) {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Morph an indirect call into a load where \p Reg holds the call target.
-  virtual void convertIndirectCallToLoad(MCInst &Inst, MCPhysReg Reg) {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Replace instruction with a shorter version that could be relaxed later
-  /// if needed.
-  virtual bool shortenInstruction(MCInst &Inst,
-                                  const MCSubtargetInfo &STI) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Convert a move instruction into a conditional move instruction, given a
-  /// condition code.
-  virtual bool
-  convertMoveToConditionalMove(MCInst &Inst, unsigned CC,
-                               bool AllowStackMemOp = false,
-                               bool AllowBasePtrStackMemOp = false) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Lower a tail call instruction \p Inst if required by target.
-  virtual bool lowerTailCall(MCInst &Inst) {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Receives a list of MCInst of the basic block to analyze and interpret the
-  /// terminators of this basic block. TBB must be initialized with the original
-  /// fall-through for this BB.
-  virtual bool analyzeBranch(InstructionIterator Begin, InstructionIterator End,
-                             const MCSymbol *&TBB, const MCSymbol *&FBB,
-                             MCInst *&CondBranch, MCInst *&UncondBranch) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Analyze \p Instruction to try and determine what type of indirect branch
-  /// it is.  It is assumed that \p Instruction passes isIndirectBranch().
-  /// \p BB is an array of instructions immediately preceding \p Instruction.
-  /// If \p Instruction can be successfully analyzed, the output parameters
-  /// will be set to the different components of the branch.  \p MemLocInstr
-  /// is the instruction that loads up the indirect function pointer.  It may
-  /// or may not be same as \p Instruction.
-  virtual IndirectBranchType
-  analyzeIndirectBranch(MCInst &Instruction, InstructionIterator Begin,
-                        InstructionIterator End, const unsigned PtrSize,
-                        MCInst *&MemLocInstr, unsigned &BaseRegNum,
-                        unsigned &IndexRegNum, int64_t &DispValue,
-                        const MCExpr *&DispExpr, MCInst *&PCRelBaseOut) const {
-    llvm_unreachable("not implemented");
-    return IndirectBranchType::UNKNOWN;
-  }
-
-  /// Analyze branch \p Instruction in PLT section and try to determine
-  /// associated got entry address.
-  virtual uint64_t analyzePLTEntry(MCInst &Instruction,
-                                   InstructionIterator Begin,
-                                   InstructionIterator End,
-                                   uint64_t BeginPC) const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  virtual bool analyzeVirtualMethodCall(InstructionIterator Begin,
-                                        InstructionIterator End,
-                                        std::vector<MCInst *> &MethodFetchInsns,
-                                        unsigned &VtableRegNum,
-                                        unsigned &BaseRegNum,
-                                        uint64_t &MethodOffset) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual void createLongJmp(InstructionListType &Seq, const MCSymbol *Target,
-                             MCContext *Ctx, bool IsTailCall = false) {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual void createShortJmp(InstructionListType &Seq, const MCSymbol *Target,
-                              MCContext *Ctx, bool IsTailCall = false) {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Return not 0 if the instruction CurInst, in combination with the recent
-  /// history of disassembled instructions supplied by [Begin, End), is a linker
-  /// generated veneer/stub that needs patching. This happens in AArch64 when
-  /// the code is large and the linker needs to generate stubs, but it does
-  /// not put any extra relocation information that could help us to easily
-  /// extract the real target. This function identifies and extract the real
-  /// target in Tgt. The instruction that loads the lower bits of the target
-  /// is put in TgtLowBits, and its pair in TgtHiBits. If the instruction in
-  /// TgtHiBits does not have an immediate operand, but an expression, then
-  /// this expression is put in TgtHiSym and Tgt only contains the lower bits.
-  /// Return value is a total number of instructions that were used to create
-  /// a veneer.
-  virtual uint64_t matchLinkerVeneer(InstructionIterator Begin,
-                                     InstructionIterator End, uint64_t Address,
-                                     const MCInst &CurInst,
-                                     MCInst *&TargetHiBits,
-                                     MCInst *&TargetLowBits,
-                                     uint64_t &Target) const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual bool matchAdrpAddPair(const MCInst &Adrp, const MCInst &Add) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual int getShortJmpEncodingSize() const {
-    llvm_unreachable("not implemented");
-  }
-
-  virtual int getUncondBranchEncodingSize() const {
-    llvm_unreachable("not implemented");
-    return 0;
-  }
-
-  /// Create a no-op instruction.
-  virtual bool createNoop(MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Create a return instruction.
-  virtual bool createReturn(MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Store \p Target absolute address to \p RegName
-  virtual InstructionListType materializeAddress(const MCSymbol *Target,
-                                                 MCContext *Ctx,
-                                                 MCPhysReg RegName,
-                                                 int64_t Addend = 0) const {
-    llvm_unreachable("not implemented");
-    return {};
-  }
-
-  /// Creates a new unconditional branch instruction in Inst and set its operand
-  /// to TBB.
-  ///
-  /// Returns true on success.
-  virtual bool createUncondBranch(MCInst &Inst, const MCSymbol *TBB,
-                                  MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Creates a new call instruction in Inst and sets its operand to
-  /// Target.
-  ///
-  /// Returns true on success.
-  virtual bool createCall(MCInst &Inst, const MCSymbol *Target,
-                          MCContext *Ctx) {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Creates a new tail call instruction in Inst and sets its operand to
-  /// Target.
-  ///
-  /// Returns true on success.
-  virtual bool createTailCall(MCInst &Inst, const MCSymbol *Target,
-                              MCContext *Ctx) {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual void createLongTailCall(InstructionListType &Seq,
-                                  const MCSymbol *Target, MCContext *Ctx) {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Creates a trap instruction in Inst.
-  ///
-  /// Returns true on success.
-  virtual bool createTrap(MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Creates an instruction to bump the stack pointer just like a call.
-  virtual bool createStackPointerIncrement(MCInst &Inst, int Size = 8,
-                                           bool NoFlagsClobber = false) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Creates an instruction to move the stack pointer just like a ret.
-  virtual bool createStackPointerDecrement(MCInst &Inst, int Size = 8,
-                                           bool NoFlagsClobber = false) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Create a store instruction using \p StackReg as the base register
-  /// and \p Offset as the displacement.
-  virtual bool createSaveToStack(MCInst &Inst, const MCPhysReg &StackReg,
-                                 int Offset, const MCPhysReg &SrcReg,
-                                 int Size) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool createLoad(MCInst &Inst, const MCPhysReg &BaseReg, int64_t Scale,
-                          const MCPhysReg &IndexReg, int64_t Offset,
-                          const MCExpr *OffsetExpr,
-                          const MCPhysReg &AddrSegmentReg,
-                          const MCPhysReg &DstReg, int Size) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual void createLoadImmediate(MCInst &Inst, const MCPhysReg Dest,
-                                   uint32_t Imm) const {
-    llvm_unreachable("not implemented");
-  }
-
-  /// Create instruction to increment contents of target by 1
-  virtual bool createIncMemory(MCInst &Inst, const MCSymbol *Target,
-                               MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Create a fragment of code (sequence of instructions) that load a 32-bit
-  /// address from memory, zero-extends it to 64 and jump to it (indirect jump).
-  virtual bool
-  createIJmp32Frag(SmallVectorImpl<MCInst> &Insts, const MCOperand &BaseReg,
-                   const MCOperand &Scale, const MCOperand &IndexReg,
-                   const MCOperand &Offset, const MCOperand &TmpReg) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Create a load instruction using \p StackReg as the base register
-  /// and \p Offset as the displacement.
-  virtual bool createRestoreFromStack(MCInst &Inst, const MCPhysReg &StackReg,
-                                      int Offset, const MCPhysReg &DstReg,
-                                      int Size) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Creates a call frame pseudo instruction. A single operand identifies which
-  /// MCCFIInstruction this MCInst is referring to.
-  ///
-  /// Returns true on success.
-  virtual bool createCFI(MCInst &Inst, int64_t Offset) const {
-    Inst.clear();
-    Inst.setOpcode(TargetOpcode::CFI_INSTRUCTION);
-    Inst.addOperand(MCOperand::createImm(Offset));
-    return true;
-  }
-
-  /// Create an inline version of memcpy(dest, src, 1).
-  virtual InstructionListType createOneByteMemcpy() const {
-    llvm_unreachable("not implemented");
-    return {};
-  }
-
-  /// Create a sequence of instructions to compare contents of a register
-  /// \p RegNo to immediate \Imm and jump to \p Target if they are equal.
-  virtual InstructionListType createCmpJE(MCPhysReg RegNo, int64_t Imm,
-                                          const MCSymbol *Target,
-                                          MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return {};
-  }
-
-  /// Creates inline memcpy instruction. If \p ReturnEnd is true, then return
-  /// (dest + n) instead of dest.
-  virtual InstructionListType createInlineMemcpy(bool ReturnEnd) const {
-    llvm_unreachable("not implemented");
-    return {};
-  }
-
-  /// Create a target-specific relocation out of the \p Fixup.
-  /// Note that not every fixup could be converted into a relocation.
-  virtual std::optional<Relocation>
-  createRelocation(const MCFixup &Fixup, const MCAsmBackend &MAB) const {
-    llvm_unreachable("not implemented");
-    return Relocation();
-  }
-
-  /// Returns true if instruction is a call frame pseudo instruction.
-  virtual bool isCFI(const MCInst &Inst) const {
-    return Inst.getOpcode() == TargetOpcode::CFI_INSTRUCTION;
-  }
-
-  /// Reverses the branch condition in Inst and update its taken target to TBB.
-  ///
-  /// Returns true on success.
-  virtual bool reverseBranchCondition(MCInst &Inst, const MCSymbol *TBB,
-                                      MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool replaceBranchCondition(MCInst &Inst, const MCSymbol *TBB,
-                                      MCContext *Ctx, unsigned CC) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual unsigned getInvertedCondCode(unsigned CC) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual unsigned getCondCodesLogicalOr(unsigned CC1, unsigned CC2) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool isValidCondCode(unsigned CC) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return the conditional code used in a conditional jump instruction.
-  /// Returns invalid code if not conditional jump.
-  virtual unsigned getCondCode(const MCInst &Inst) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Return canonical branch opcode for a reversible branch opcode. For every
-  /// opposite branch opcode pair Op <-> OpR this function returns one of the
-  /// opcodes which is considered a canonical.
-  virtual unsigned getCanonicalBranchCondCode(unsigned CC) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  /// Sets the taken target of the branch instruction to Target.
-  ///
-  /// Returns true on success.
-  virtual bool replaceBranchTarget(MCInst &Inst, const MCSymbol *TBB,
-                                   MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return false;
-  }
-
-  virtual bool createEHLabel(MCInst &Inst, const MCSymbol *Label,
-                             MCContext *Ctx) const {
-    Inst.setOpcode(TargetOpcode::EH_LABEL);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createExpr(
-        MCSymbolRefExpr::create(Label, MCSymbolRefExpr::VK_None, *Ctx)));
-    return true;
-  }
-
-  /// Return annotation index matching the \p Name.
-  std::optional<unsigned> getAnnotationIndex(StringRef Name) const {
-    auto AI = AnnotationNameIndexMap.find(Name);
-    if (AI != AnnotationNameIndexMap.end())
-      return AI->second;
-    return std::nullopt;
-  }
-
-  /// Return annotation index matching the \p Name. Create a new index if the
-  /// \p Name wasn't registered previously.
-  unsigned getOrCreateAnnotationIndex(StringRef Name) {
-    auto AI = AnnotationNameIndexMap.find(Name);
-    if (AI != AnnotationNameIndexMap.end())
-      return AI->second;
-
-    const unsigned Index =
-        AnnotationNameIndexMap.size() + MCPlus::MCAnnotation::kGeneric;
-    AnnotationNameIndexMap.insert(std::make_pair(Name, Index));
-    AnnotationNames.emplace_back(std::string(Name));
-    return Index;
-  }
-
-  /// Store an annotation value on an MCInst.  This assumes the annotation
-  /// is not already present.
-  template <typename ValueType>
-  const ValueType &addAnnotation(MCInst &Inst, unsigned Index,
-                                 const ValueType &Val,
-                                 AllocatorIdTy AllocatorId = 0) {
-    assert(!hasAnnotation(Inst, Index));
-    AnnotationAllocator &Allocator = getAnnotationAllocator(AllocatorId);
-    auto *A = new (Allocator.ValueAllocator)
-        MCPlus::MCSimpleAnnotation<ValueType>(Val);
-
-    if (!std::is_trivial<ValueType>::value)
-      Allocator.AnnotationPool.insert(A);
-    setAnnotationOpValue(Inst, Index, reinterpret_cast<int64_t>(A),
-                         AllocatorId);
-    return A->getValue();
-  }
-
-  /// Store an annotation value on an MCInst.  This assumes the annotation
-  /// is not already present.
-  template <typename ValueType>
-  const ValueType &addAnnotation(MCInst &Inst, StringRef Name,
-                                 const ValueType &Val,
-                                 AllocatorIdTy AllocatorId = 0) {
-    return addAnnotation(Inst, getOrCreateAnnotationIndex(Name), Val,
-                         AllocatorId);
-  }
-
-  /// Get an annotation as a specific value, but if the annotation does not
-  /// exist, create a new annotation with the default constructor for that type.
-  /// Return a non-const ref so caller can freely modify its contents
-  /// afterwards.
-  template <typename ValueType>
-  ValueType &getOrCreateAnnotationAs(MCInst &Inst, unsigned Index,
-                                     AllocatorIdTy AllocatorId = 0) {
-    auto Val =
-        tryGetAnnotationAs<ValueType>(const_cast<const MCInst &>(Inst), Index);
-    if (!Val)
-      Val = addAnnotation(Inst, Index, ValueType(), AllocatorId);
-    return const_cast<ValueType &>(*Val);
-  }
-
-  /// Get an annotation as a specific value, but if the annotation does not
-  /// exist, create a new annotation with the default constructor for that type.
-  /// Return a non-const ref so caller can freely modify its contents
-  /// afterwards.
-  template <typename ValueType>
-  ValueType &getOrCreateAnnotationAs(MCInst &Inst, StringRef Name,
-                                     AllocatorIdTy AllocatorId = 0) {
-    const unsigned Index = getOrCreateAnnotationIndex(Name);
-    return getOrCreateAnnotationAs<ValueType>(Inst, Index, AllocatorId);
-  }
-
-  /// Get an annotation as a specific value. Assumes that the annotation exists.
-  /// Use hasAnnotation() if the annotation may not exist.
-  template <typename ValueType>
-  ValueType &getAnnotationAs(const MCInst &Inst, unsigned Index) const {
-    std::optional<int64_t> Value = getAnnotationOpValue(Inst, Index);
-    assert(Value && "annotation should exist");
-    return reinterpret_cast<MCPlus::MCSimpleAnnotation<ValueType> *>(*Value)
-        ->getValue();
-  }
-
-  /// Get an annotation as a specific value. Assumes that the annotation exists.
-  /// Use hasAnnotation() if the annotation may not exist.
-  template <typename ValueType>
-  ValueType &getAnnotationAs(const MCInst &Inst, StringRef Name) const {
-    const auto Index = getAnnotationIndex(Name);
-    assert(Index && "annotation should exist");
-    return getAnnotationAs<ValueType>(Inst, *Index);
-  }
-
-  /// Get an annotation as a specific value. If the annotation does not exist,
-  /// return the \p DefaultValue.
-  template <typename ValueType>
-  const ValueType &
-  getAnnotationWithDefault(const MCInst &Inst, unsigned Index,
-                           const ValueType &DefaultValue = ValueType()) {
-    if (!hasAnnotation(Inst, Index))
-      return DefaultValue;
-    return getAnnotationAs<ValueType>(Inst, Index);
-  }
-
-  /// Get an annotation as a specific value. If the annotation does not exist,
-  /// return the \p DefaultValue.
-  template <typename ValueType>
-  const ValueType &
-  getAnnotationWithDefault(const MCInst &Inst, StringRef Name,
-                           const ValueType &DefaultValue = ValueType()) {
-    const unsigned Index = getOrCreateAnnotationIndex(Name);
-    return getAnnotationWithDefault<ValueType>(Inst, Index, DefaultValue);
-  }
-
-  /// Check if the specified annotation exists on this instruction.
-  bool hasAnnotation(const MCInst &Inst, unsigned Index) const;
-
-  /// Check if an annotation with a specified \p Name exists on \p Inst.
-  bool hasAnnotation(const MCInst &Inst, StringRef Name) const {
-    const auto Index = getAnnotationIndex(Name);
-    if (!Index)
-      return false;
-    return hasAnnotation(Inst, *Index);
-  }
-
-  /// Get an annotation as a specific value, but if the annotation does not
-  /// exist, return errc::result_out_of_range.
-  template <typename ValueType>
-  ErrorOr<const ValueType &> tryGetAnnotationAs(const MCInst &Inst,
-                                                unsigned Index) const {
-    if (!hasAnnotation(Inst, Index))
-      return make_error_code(std::errc::result_out_of_range);
-    return getAnnotationAs<ValueType>(Inst, Index);
-  }
-
-  /// Get an annotation as a specific value, but if the annotation does not
-  /// exist, return errc::result_out_of_range.
-  template <typename ValueType>
-  ErrorOr<const ValueType &> tryGetAnnotationAs(const MCInst &Inst,
-                                                StringRef Name) const {
-    const auto Index = getAnnotationIndex(Name);
-    if (!Index)
-      return make_error_code(std::errc::result_out_of_range);
-    return tryGetAnnotationAs<ValueType>(Inst, *Index);
-  }
-
-  template <typename ValueType>
-  ErrorOr<ValueType &> tryGetAnnotationAs(MCInst &Inst, unsigned Index) const {
-    if (!hasAnnotation(Inst, Index))
-      return make_error_code(std::errc::result_out_of_range);
-    return const_cast<ValueType &>(getAnnotationAs<ValueType>(Inst, Index));
-  }
-
-  template <typename ValueType>
-  ErrorOr<ValueType &> tryGetAnnotationAs(MCInst &Inst, StringRef Name) const {
-    const auto Index = getAnnotationIndex(Name);
-    if (!Index)
-      return make_error_code(std::errc::result_out_of_range);
-    return tryGetAnnotationAs<ValueType>(Inst, *Index);
-  }
-
-  /// Print each annotation attached to \p Inst.
-  void printAnnotations(const MCInst &Inst, raw_ostream &OS) const;
-
-  /// Remove annotation with a given \p Index.
-  ///
-  /// Return true if the annotation was removed, false if the annotation
-  /// was not present.
-  bool removeAnnotation(MCInst &Inst, unsigned Index);
-
-  /// Remove annotation associated with \p Name.
-  ///
-  /// Return true if the annotation was removed, false if the annotation
-  /// was not present.
-  bool removeAnnotation(MCInst &Inst, StringRef Name) {
-    const auto Index = getAnnotationIndex(Name);
-    if (!Index)
-      return false;
-    return removeAnnotation(Inst, *Index);
-  }
-
-  /// Remove meta-data, but don't destroy it.
-  void stripAnnotations(MCInst &Inst, bool KeepTC = false);
-
-  virtual InstructionListType
-  createInstrumentedIndirectCall(MCInst &&CallInst, MCSymbol *HandlerFuncAddr,
-                                 int CallSiteID, MCContext *Ctx) {
-    llvm_unreachable("not implemented");
-    return InstructionListType();
-  }
-
-  virtual InstructionListType createInstrumentedIndCallHandlerExitBB() const {
-    llvm_unreachable("not implemented");
-    return InstructionListType();
-  }
-
-  virtual InstructionListType
-  createInstrumentedIndTailCallHandlerExitBB() const {
-    llvm_unreachable("not implemented");
-    return InstructionListType();
-  }
-
-  virtual InstructionListType
-  createInstrumentedIndCallHandlerEntryBB(const MCSymbol *InstrTrampoline,
-                                          const MCSymbol *IndCallHandler,
-                                          MCContext *Ctx) {
-    llvm_unreachable("not implemented");
-    return InstructionListType();
-  }
-
-  virtual InstructionListType createNumCountersGetter(MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return {};
-  }
-
-  virtual InstructionListType createInstrLocationsGetter(MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return {};
-  }
-
-  virtual InstructionListType createInstrTablesGetter(MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return {};
-  }
-
-  virtual InstructionListType createInstrNumFuncsGetter(MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return {};
-  }
-
-  virtual InstructionListType createSymbolTrampoline(const MCSymbol *TgtSym,
-                                                     MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return InstructionListType();
-  }
-
-  virtual InstructionListType createDummyReturnFunction(MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return InstructionListType();
-  }
-
-  /// This method takes an indirect call instruction and splits it up into an
-  /// equivalent set of instructions that use direct calls for target
-  /// symbols/addresses that are contained in the Targets vector.  This is done
-  /// by guarding each direct call with a compare instruction to verify that
-  /// the target is correct.
-  /// If the VtableAddrs vector is not empty, the call will have the extra
-  /// load of the method pointer from the vtable eliminated.  When non-empty
-  /// the VtableAddrs vector must be the same size as Targets and include the
-  /// address of a vtable for each corresponding method call in Targets.  The
-  /// MethodFetchInsns vector holds instructions that are used to load the
-  /// correct method for the cold call case.
-  ///
-  /// The return value is a vector of code snippets (essentially basic blocks).
-  /// There is a symbol associated with each snippet except for the first.
-  /// If the original call is not a tail call, the last snippet will have an
-  /// empty vector of instructions.  The label is meant to indicate the basic
-  /// block where all previous snippets are joined, i.e. the instructions that
-  /// would immediate follow the original call.
-  using BlocksVectorTy =
-      std::vector<std::pair<MCSymbol *, InstructionListType>>;
-  struct MultiBlocksCode {
-    BlocksVectorTy Blocks;
-    std::vector<MCSymbol *> Successors;
-  };
-
-  virtual BlocksVectorTy indirectCallPromotion(
-      const MCInst &CallInst,
-      const std::vector<std::pair<MCSymbol *, uint64_t>> &Targets,
-      const std::vector<std::pair<MCSymbol *, uint64_t>> &VtableSyms,
-      const std::vector<MCInst *> &MethodFetchInsns,
-      const bool MinimizeCodeSize, MCContext *Ctx) {
-    llvm_unreachable("not implemented");
-    return BlocksVectorTy();
-  }
-
-  virtual BlocksVectorTy jumpTablePromotion(
-      const MCInst &IJmpInst,
-      const std::vector<std::pair<MCSymbol *, uint64_t>> &Targets,
-      const std::vector<MCInst *> &TargetFetchInsns, MCContext *Ctx) const {
-    llvm_unreachable("not implemented");
-    return BlocksVectorTy();
-  }
-
-  // AliasMap caches a mapping of registers to the set of registers that
-  // alias (are sub or superregs of itself, including itself).
-  std::vector<BitVector> AliasMap;
-  std::vector<BitVector> SmallerAliasMap;
-  // SizeMap caches a mapping of registers to their sizes.
-  std::vector<uint8_t> SizeMap;
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    // Replace Register in Inst with Imm. Returns true if successful
+    virtual bool replaceRegWithImm(MCInst &Inst, unsigned Register,
+                                   int64_t Imm) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    // Replace ToReplace in Inst with ReplaceWith. Returns true if successful
+    virtual bool replaceRegWithReg(MCInst &Inst, unsigned ToReplace,
+                                   unsigned ReplaceWith) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Add \p NewImm to the current immediate operand of \p Inst. If it is a
+    /// memory accessing instruction, this immediate is the memory address
+    /// displacement. Otherwise, the target operand is the first immediate
+    /// operand found in \p Inst. Return false if no imm operand found.
+    virtual bool addToImm(MCInst &Inst, int64_t &Amt, MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Replace the compound memory operand of Inst with an immediate operand.
+    /// The value of the immediate operand is computed by reading the \p
+    /// ConstantData array starting from \p offset and assuming
+    /// little-endianess. Return true on success. The given instruction is
+    /// modified in place.
+    virtual bool replaceMemOperandWithImm(MCInst &Inst, StringRef ConstantData,
+                                          uint64_t Offset) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Same as replaceMemOperandWithImm, but for registers.
+    virtual bool replaceMemOperandWithReg(MCInst   &Inst,
+                                          MCPhysReg RegNum) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Return true if a move instruction moves a register to itself.
+    virtual bool isRedundantMove(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Return true if the instruction is a tail call.
+    bool isTailCall(const MCInst &Inst) const;
+
+    /// Return true if the instruction is a call with an exception handling
+    /// info.
+    virtual bool isInvoke(const MCInst &Inst) const {
+        return isCall(Inst) && getEHInfo(Inst);
+    }
+
+    /// Return true if \p Inst is an instruction that potentially traps when
+    /// working with addresses not aligned to the size of the operand.
+    virtual bool requiresAlignedAddress(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Return handler and action info for invoke instruction if present.
+    std::optional<MCPlus::MCLandingPad> getEHInfo(const MCInst &Inst) const;
+
+    /// Add handler and action info for call instruction.
+    void addEHInfo(MCInst &Inst, const MCPlus::MCLandingPad &LP);
+
+    /// Update exception-handling info for the invoke instruction \p Inst.
+    /// Return true on success and false otherwise, e.g. if the instruction is
+    /// not an invoke.
+    bool updateEHInfo(MCInst &Inst, const MCPlus::MCLandingPad &LP);
+
+    /// Return non-negative GNU_args_size associated with the instruction
+    /// or -1 if there's no associated info.
+    int64_t getGnuArgsSize(const MCInst &Inst) const;
+
+    /// Add the value of GNU_args_size to Inst if it already has EH info.
+    void addGnuArgsSize(MCInst &Inst, int64_t GnuArgsSize,
+                        AllocatorIdTy AllocId = 0);
+
+    /// Return jump table addressed by this instruction.
+    uint64_t getJumpTable(const MCInst &Inst) const;
+
+    /// Return index register for instruction that uses a jump table.
+    uint16_t getJumpTableIndexReg(const MCInst &Inst) const;
+
+    /// Set jump table addressed by this instruction.
+    bool setJumpTable(MCInst &Inst, uint64_t Value, uint16_t IndexReg,
+                      AllocatorIdTy AllocId = 0);
+
+    /// Disassociate instruction with a jump table.
+    bool unsetJumpTable(MCInst &Inst);
+
+    /// Return destination of conditional tail call instruction if \p Inst is
+    /// one.
+    std::optional<uint64_t> getConditionalTailCall(const MCInst &Inst) const;
+
+    /// Mark the \p Instruction as a conditional tail call, and set its
+    /// destination address if it is known. If \p Instruction was already
+    /// marked, update its destination with \p Dest.
+    bool setConditionalTailCall(MCInst &Inst, uint64_t Dest = 0);
+
+    /// If \p Inst was marked as a conditional tail call convert it to a regular
+    /// branch. Return true if the instruction was converted.
+    bool unsetConditionalTailCall(MCInst &Inst);
+
+    /// Return offset of \p Inst in the original function, if available.
+    std::optional<uint32_t> getOffset(const MCInst &Inst) const;
+
+    /// Return the offset if the annotation is present, or \p Default otherwise.
+    uint32_t getOffsetWithDefault(const MCInst &Inst, uint32_t Default) const;
+
+    /// Set offset of \p Inst in the original function.
+    bool setOffset(MCInst &Inst, uint32_t Offset,
+                   AllocatorIdTy AllocatorId = 0);
+
+    /// Remove offset annotation.
+    bool clearOffset(MCInst &Inst);
+
+    /// Return MCSymbol that represents a target of this instruction at a given
+    /// operand number \p OpNum. If there's no symbol associated with
+    /// the operand - return nullptr.
+    virtual const MCSymbol *getTargetSymbol(const MCInst &Inst,
+                                            unsigned      OpNum = 0) const {
+        llvm_unreachable("not implemented");
+        return nullptr;
+    }
+
+    /// Return MCSymbol extracted from a target expression
+    virtual const MCSymbol *getTargetSymbol(const MCExpr *Expr) const {
+        return &cast<const MCSymbolRefExpr>(Expr)->getSymbol();
+    }
+
+    /// Return addend that represents an offset from MCSymbol target
+    /// of this instruction at a given operand number \p OpNum.
+    /// If there's no symbol associated with  the operand - return 0
+    virtual int64_t getTargetAddend(const MCInst &Inst,
+                                    unsigned      OpNum = 0) const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    /// Return MCSymbol addend extracted from a target expression
+    virtual int64_t getTargetAddend(const MCExpr *Expr) const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    /// Return MCSymbol/offset extracted from a target expression
+    virtual std::pair<const MCSymbol *, uint64_t>
+    getTargetSymbolInfo(const MCExpr *Expr) const {
+        if (auto *SymExpr = dyn_cast<MCSymbolRefExpr>(Expr)) {
+            return std::make_pair(&SymExpr->getSymbol(), 0);
+        } else if (auto *BinExpr = dyn_cast<MCBinaryExpr>(Expr)) {
+            const auto *SymExpr = dyn_cast<MCSymbolRefExpr>(BinExpr->getLHS());
+            const auto *ConstExpr = dyn_cast<MCConstantExpr>(BinExpr->getRHS());
+            if (BinExpr->getOpcode() == MCBinaryExpr::Add && SymExpr &&
+                ConstExpr)
+                return std::make_pair(&SymExpr->getSymbol(),
+                                      ConstExpr->getValue());
+        }
+        return std::make_pair(nullptr, 0);
+    }
+
+    /// Replace displacement in compound memory operand with given \p Operand.
+    virtual bool replaceMemOperandDisp(MCInst &Inst, MCOperand Operand) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Return the MCExpr used for absolute references in this target
+    virtual const MCExpr *getTargetExprFor(MCInst &Inst, const MCExpr *Expr,
+                                           MCContext &Ctx,
+                                           uint64_t   RelType) const {
+        return Expr;
+    }
+
+    /// Return a BitVector marking all sub or super registers of \p Reg,
+    /// including itself.
+    virtual const BitVector &getAliases(MCPhysReg Reg,
+                                        bool      OnlySmaller = false) const;
+
+    /// Initialize aliases tables.
+    void initAliases();
+
+    /// Initialize register size table.
+    void initSizeMap();
+
+    /// Change \p Regs setting all registers used to pass parameters according
+    /// to the host abi. Do nothing if not implemented.
+    virtual BitVector getRegsUsedAsParams() const {
+        llvm_unreachable("not implemented");
+        return BitVector();
+    }
+
+    /// Change \p Regs setting all registers used as callee-saved according
+    /// to the host abi. Do nothing if not implemented.
+    virtual void getCalleeSavedRegs(BitVector &Regs) const {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Get the default def_in and live_out registers for the function
+    /// Currently only used for the Stoke optimzation
+    virtual void getDefaultDefIn(BitVector &Regs) const {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Similar to getDefaultDefIn
+    virtual void getDefaultLiveOut(BitVector &Regs) const {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Change \p Regs with a bitmask with all general purpose regs
+    virtual void getGPRegs(BitVector &Regs, bool IncludeAlias = true) const {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Change \p Regs with a bitmask with all general purpose regs that can be
+    /// encoded without extra prefix bytes. For x86 only.
+    virtual void getClassicGPRegs(BitVector &Regs) const {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Set of Registers used by the Rep instruction
+    virtual void getRepRegs(BitVector &Regs) const {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Return the register width in bytes (1, 2, 4 or 8)
+    uint8_t getRegSize(MCPhysReg Reg) const { return SizeMap[Reg]; }
+
+    /// For aliased registers, return an alias of \p Reg that has the width of
+    /// \p Size bytes
+    virtual MCPhysReg getAliasSized(MCPhysReg Reg, uint8_t Size) const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    /// For X86, return whether this register is an upper 8-bit register, such
+    /// as AH, BH, etc.
+    virtual bool isUpper8BitReg(MCPhysReg Reg) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// For X86, return whether this instruction has special constraints that
+    /// prevents it from encoding registers that require a REX prefix.
+    virtual bool cannotUseREX(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Modifies the set \p Regs by adding registers \p Inst may rewrite. Caller
+    /// is responsible for passing a valid BitVector with the size equivalent to
+    /// the number of registers in the target.
+    /// Since this function is called many times during clobber analysis, it
+    /// expects the caller to manage BitVector creation to avoid extra overhead.
+    virtual void getClobberedRegs(const MCInst &Inst, BitVector &Regs) const;
+
+    /// Set of all registers touched by this instruction, including implicit
+    /// uses and defs.
+    virtual void getTouchedRegs(const MCInst &Inst, BitVector &Regs) const;
+
+    /// Set of all registers being written to by this instruction -- includes
+    /// aliases but only if they are strictly smaller than the actual reg
+    virtual void getWrittenRegs(const MCInst &Inst, BitVector &Regs) const;
+
+    /// Set of all registers being read by this instruction -- includes aliases
+    /// but only if they are strictly smaller than the actual reg
+    virtual void getUsedRegs(const MCInst &Inst, BitVector &Regs) const;
+
+    /// Set of all src registers -- includes aliases but
+    /// only if they are strictly smaller than the actual reg
+    virtual void getSrcRegs(const MCInst &Inst, BitVector &Regs) const;
+
+    /// Return true if this instruction defines the specified physical
+    /// register either explicitly or implicitly.
+    virtual bool hasDefOfPhysReg(const MCInst &MI, unsigned Reg) const;
+
+    /// Return true if this instruction uses the specified physical
+    /// register either explicitly or implicitly.
+    virtual bool hasUseOfPhysReg(const MCInst &MI, unsigned Reg) const;
+
+    /// Replace displacement in compound memory operand with given \p Label.
+    bool replaceMemOperandDisp(MCInst &Inst, const MCSymbol *Label,
+                               MCContext *Ctx) const {
+        return replaceMemOperandDisp(Inst, Label, 0, Ctx);
+    }
+
+    /// Replace displacement in compound memory operand with given \p Label
+    /// plus addend.
+    bool replaceMemOperandDisp(MCInst &Inst, const MCSymbol *Label,
+                               int64_t Addend, MCContext *Ctx) const {
+        MCInst::iterator MemOpI = getMemOperandDisp(Inst);
+        if (MemOpI == Inst.end())
+            return false;
+        return setOperandToSymbolRef(Inst, MemOpI - Inst.begin(), Label, Addend,
+                                     Ctx, 0);
+    }
+
+    /// Returns how many bits we have in this instruction to encode a PC-rel
+    /// imm.
+    virtual int getPCRelEncodingSize(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    /// Replace instruction opcode to be a tail call instead of jump.
+    virtual bool convertJmpToTailCall(MCInst &Inst) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Perform any additional actions to transform a (conditional) tail call
+    /// into a (conditional) jump. Assume the target was already replaced with
+    /// a local one, so the default is to do nothing more.
+    virtual bool convertTailCallToJmp(MCInst &Inst) { return true; }
+
+    /// Replace instruction opcode to be a regural call instead of tail call.
+    virtual bool convertTailCallToCall(MCInst &Inst) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Modify a direct call instruction \p Inst with an indirect call taking
+    /// a destination from a memory location pointed by \p TargetLocation
+    /// symbol.
+    virtual bool convertCallToIndirectCall(MCInst         &Inst,
+                                           const MCSymbol *TargetLocation,
+                                           MCContext      *Ctx) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Morph an indirect call into a load where \p Reg holds the call target.
+    virtual void convertIndirectCallToLoad(MCInst &Inst, MCPhysReg Reg) {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Replace instruction with a shorter version that could be relaxed later
+    /// if needed.
+    virtual bool shortenInstruction(MCInst                &Inst,
+                                    const MCSubtargetInfo &STI) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Convert a move instruction into a conditional move instruction, given a
+    /// condition code.
+    virtual bool
+    convertMoveToConditionalMove(MCInst &Inst, unsigned CC,
+                                 bool AllowStackMemOp        = false,
+                                 bool AllowBasePtrStackMemOp = false) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Lower a tail call instruction \p Inst if required by target.
+    virtual bool lowerTailCall(MCInst &Inst) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Receives a list of MCInst of the basic block to analyze and interpret
+    /// the terminators of this basic block. TBB must be initialized with the
+    /// original fall-through for this BB.
+    virtual bool analyzeBranch(InstructionIterator Begin,
+                               InstructionIterator End, const MCSymbol *&TBB,
+                               const MCSymbol *&FBB, MCInst *&CondBranch,
+                               MCInst *&UncondBranch) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Analyze \p Instruction to try and determine what type of indirect branch
+    /// it is.  It is assumed that \p Instruction passes isIndirectBranch().
+    /// \p BB is an array of instructions immediately preceding \p Instruction.
+    /// If \p Instruction can be successfully analyzed, the output parameters
+    /// will be set to the different components of the branch.  \p MemLocInstr
+    /// is the instruction that loads up the indirect function pointer.  It may
+    /// or may not be same as \p Instruction.
+    virtual IndirectBranchType analyzeIndirectBranch(
+        MCInst &Instruction, InstructionIterator Begin, InstructionIterator End,
+        const unsigned PtrSize, MCInst *&MemLocInstr, unsigned &BaseRegNum,
+        unsigned &IndexRegNum, int64_t &DispValue, const MCExpr *&DispExpr,
+        MCInst *&PCRelBaseOut) const {
+        llvm_unreachable("not implemented");
+        return IndirectBranchType::UNKNOWN;
+    }
+
+    /// Analyze branch \p Instruction in PLT section and try to determine
+    /// associated got entry address.
+    virtual uint64_t analyzePLTEntry(MCInst             &Instruction,
+                                     InstructionIterator Begin,
+                                     InstructionIterator End,
+                                     uint64_t            BeginPC) const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    virtual bool
+    analyzeVirtualMethodCall(InstructionIterator Begin, InstructionIterator End,
+                             std::vector<MCInst *> &MethodFetchInsns,
+                             unsigned &VtableRegNum, unsigned &BaseRegNum,
+                             uint64_t &MethodOffset) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual void createLongJmp(InstructionListType &Seq, const MCSymbol *Target,
+                               MCContext *Ctx, bool IsTailCall = false) {
+        llvm_unreachable("not implemented");
+    }
+
+    virtual void createShortJmp(InstructionListType &Seq,
+                                const MCSymbol *Target, MCContext *Ctx,
+                                bool IsTailCall = false) {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Return not 0 if the instruction CurInst, in combination with the recent
+    /// history of disassembled instructions supplied by [Begin, End), is a
+    /// linker generated veneer/stub that needs patching. This happens in
+    /// AArch64 when the code is large and the linker needs to generate stubs,
+    /// but it does not put any extra relocation information that could help us
+    /// to easily extract the real target. This function identifies and extract
+    /// the real target in Tgt. The instruction that loads the lower bits of the
+    /// target is put in TgtLowBits, and its pair in TgtHiBits. If the
+    /// instruction in TgtHiBits does not have an immediate operand, but an
+    /// expression, then this expression is put in TgtHiSym and Tgt only
+    /// contains the lower bits. Return value is a total number of instructions
+    /// that were used to create a veneer.
+    virtual uint64_t matchLinkerVeneer(InstructionIterator Begin,
+                                       InstructionIterator End,
+                                       uint64_t Address, const MCInst &CurInst,
+                                       MCInst  *&TargetHiBits,
+                                       MCInst  *&TargetLowBits,
+                                       uint64_t &Target) const {
+        llvm_unreachable("not implemented");
+    }
+
+    virtual bool matchAdrpAddPair(const MCInst &Adrp, const MCInst &Add) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual int getShortJmpEncodingSize() const {
+        llvm_unreachable("not implemented");
+    }
+
+    virtual int getUncondBranchEncodingSize() const {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+
+    /// Create a no-op instruction.
+    virtual bool createNoop(MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual unsigned getUnusedGPReg(const BitVector &Regs) {
+        llvm_unreachable("not implemented");
+        return 0;
+    }
+    
+    virtual bool setRegToZero(MCInst &Inst, unsigned Reg)  {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+    
+    virtual void createAddRegImm(MCInst &Inst, MCPhysReg Reg, int64_t Value,
+                         unsigned Size) const {
+        llvm_unreachable("not implemented");
+        return;
+    }
+
+    virtual bool copyRegToReg(MCInst &Inst, unsigned From, unsigned To)  {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+
+
+    virtual bool isCmpXchg(const MCInst &Inst) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+
+    virtual bool createConditionalBranchForYield(InstructionListType &Instrs,
+                                                 MCContext *Ctx,
+                                                 int IterTh, const MCSymbol *Target, const MCSymbol *LoopCounterLoc,
+                                                unsigned Reg, int cur_loop_counter_idx = -1, int InductionReg = 0, int InductionConst = 0) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool createPushToRegSet(InstructionListType &Instrs,
+                                    const BitVector     &Regs,
+                                    const MCSymbol *CrtPos, MCContext *Ctx,
+                                    unsigned RegSetIdx, bool UseStack = false,
+                                    bool SkipCtxManage = false) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool createPopFromRegSet(InstructionListType &Instrs,
+                                     const BitVector     &Regs,
+                                     const MCSymbol *CrtPos, MCContext *Ctx,
+                                     unsigned RegSetIdx, bool UseStack = false,
+                                     bool SkipCtxManage = false) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool createPrefetchInstrs(InstructionListType &Instrs,
+                                      const MCInst        &CMInst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool filterValidRegsInPnY(BitVector &Regs) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool getCrtCtxReg(unsigned &Reg, int size) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+    virtual bool createPrefetchContext(InstructionListType &Instrs,
+                                       MCContext           *Ctx) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool isCmp(const MCInst &Inst) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+    
+    virtual bool isTest(const MCInst &Inst) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool isAddByConstant(const MCInst &Inst, unsigned &Reg,
+                                 int &Constant) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool createYieldInstrs(InstructionListType &Instrs,
+                                   const MCInst &CMInst, const MCSymbol *CrtPos,
+                                   MCContext *Ctx, const BitVector &SavedRegs,
+                                   bool RetFloat, bool IsNormalYield = true, int cur_loop_counter_idx = -1) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+    /// Create a return instruction.
+    virtual bool createReturn(MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Store \p Target absolute address to \p RegName
+    virtual InstructionListType materializeAddress(const MCSymbol *Target,
+                                                   MCContext      *Ctx,
+                                                   MCPhysReg       RegName,
+                                                   int64_t Addend = 0) const {
+        llvm_unreachable("not implemented");
+        return {};
+    }
+
+    /// Creates a new unconditional branch instruction in Inst and set its
+    /// operand to TBB.
+    ///
+    /// Returns true on success.
+    virtual bool createUncondBranch(MCInst &Inst, const MCSymbol *TBB,
+                                    MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Creates a new call instruction in Inst and sets its operand to
+    /// Target.
+    ///
+    /// Returns true on success.
+    virtual bool createCall(MCInst &Inst, const MCSymbol *Target,
+                            MCContext *Ctx) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Creates a new tail call instruction in Inst and sets its operand to
+    /// Target.
+    ///
+    /// Returns true on success.
+    virtual bool createTailCall(MCInst &Inst, const MCSymbol *Target,
+                                MCContext *Ctx) {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual void createLongTailCall(InstructionListType &Seq,
+                                    const MCSymbol *Target, MCContext *Ctx) {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Creates a trap instruction in Inst.
+    ///
+    /// Returns true on success.
+    virtual bool createTrap(MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Creates an instruction to bump the stack pointer just like a call.
+    virtual bool
+    createStackPointerIncrement(MCInst &Inst, int Size = 8,
+                                bool NoFlagsClobber = false) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Creates an instruction to move the stack pointer just like a ret.
+    virtual bool
+    createStackPointerDecrement(MCInst &Inst, int Size = 8,
+                                bool NoFlagsClobber = false) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Create a store instruction using \p StackReg as the base register
+    /// and \p Offset as the displacement.
+    virtual bool createSaveToStack(MCInst &Inst, const MCPhysReg &StackReg,
+                                   int Offset, const MCPhysReg &SrcReg,
+                                   int Size) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool createLoad(MCInst &Inst, const MCPhysReg &BaseReg,
+                            int64_t Scale, const MCPhysReg &IndexReg,
+                            int64_t Offset, const MCExpr *OffsetExpr,
+                            const MCPhysReg &AddrSegmentReg,
+                            const MCPhysReg &DstReg, int Size) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual void createLoadImmediate(MCInst &Inst, const MCPhysReg Dest,
+                                     uint32_t Imm) const {
+        llvm_unreachable("not implemented");
+    }
+
+    /// Create instruction to increment contents of target by 1
+    virtual bool createIncMemory(MCInst &Inst, const MCSymbol *Target,
+                                 MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Create a fragment of code (sequence of instructions) that load a 32-bit
+    /// address from memory, zero-extends it to 64 and jump to it (indirect
+    /// jump).
+    virtual bool
+    createIJmp32Frag(SmallVectorImpl<MCInst> &Insts, const MCOperand &BaseReg,
+                     const MCOperand &Scale, const MCOperand &IndexReg,
+                     const MCOperand &Offset, const MCOperand &TmpReg) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Create a load instruction using \p StackReg as the base register
+    /// and \p Offset as the displacement.
+    virtual bool createRestoreFromStack(MCInst &Inst, const MCPhysReg &StackReg,
+                                        int Offset, const MCPhysReg &DstReg,
+                                        int Size) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Creates a call frame pseudo instruction. A single operand identifies
+    /// which MCCFIInstruction this MCInst is referring to.
+    ///
+    /// Returns true on success.
+    virtual bool createCFI(MCInst &Inst, int64_t Offset) const {
+        Inst.clear();
+        Inst.setOpcode(TargetOpcode::CFI_INSTRUCTION);
+        Inst.addOperand(MCOperand::createImm(Offset));
+        return true;
+    }
+
+    /// Create an inline version of memcpy(dest, src, 1).
+    virtual InstructionListType createOneByteMemcpy() const {
+        llvm_unreachable("not implemented");
+        return {};
+    }
+
+    /// Create a sequence of instructions to compare contents of a register
+    /// \p RegNo to immediate \Imm and jump to \p Target if they are equal.
+    virtual InstructionListType createCmpJE(MCPhysReg RegNo, int64_t Imm,
+                                            const MCSymbol *Target,
+                                            MCContext      *Ctx) const {
+        llvm_unreachable("not implemented");
+        return {};
+    }
+
+    /// Creates inline memcpy instruction. If \p ReturnEnd is true, then return
+    /// (dest + n) instead of dest.
+    virtual InstructionListType createInlineMemcpy(bool ReturnEnd) const {
+        llvm_unreachable("not implemented");
+        return {};
+    }
+
+    /// Create a target-specific relocation out of the \p Fixup.
+    /// Note that not every fixup could be converted into a relocation.
+    virtual std::optional<Relocation>
+    createRelocation(const MCFixup &Fixup, const MCAsmBackend &MAB) const {
+        llvm_unreachable("not implemented");
+        return Relocation();
+    }
+
+    /// Returns true if instruction is a call frame pseudo instruction.
+    virtual bool isCFI(const MCInst &Inst) const {
+        return Inst.getOpcode() == TargetOpcode::CFI_INSTRUCTION;
+    }
+
+    /// Reverses the branch condition in Inst and update its taken target to
+    /// TBB.
+    ///
+    /// Returns true on success.
+    virtual bool reverseBranchCondition(MCInst &Inst, const MCSymbol *TBB,
+                                        MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool replaceBranchCondition(MCInst &Inst, const MCSymbol *TBB,
+                                        MCContext *Ctx, unsigned CC) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual unsigned getInvertedCondCode(unsigned CC) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual unsigned getCondCodesLogicalOr(unsigned CC1, unsigned CC2) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool isValidCondCode(unsigned CC) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Return the conditional code used in a conditional jump instruction.
+    /// Returns invalid code if not conditional jump.
+    virtual unsigned getCondCode(const MCInst &Inst) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Return canonical branch opcode for a reversible branch opcode. For every
+    /// opposite branch opcode pair Op <-> OpR this function returns one of the
+    /// opcodes which is considered a canonical.
+    virtual unsigned getCanonicalBranchCondCode(unsigned CC) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    /// Sets the taken target of the branch instruction to Target.
+    ///
+    /// Returns true on success.
+    virtual bool replaceBranchTarget(MCInst &Inst, const MCSymbol *TBB,
+                                     MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return false;
+    }
+
+    virtual bool createEHLabel(MCInst &Inst, const MCSymbol *Label,
+                               MCContext *Ctx) const {
+        Inst.setOpcode(TargetOpcode::EH_LABEL);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(Label, MCSymbolRefExpr::VK_None, *Ctx)));
+        return true;
+    }
+
+    /// Return annotation index matching the \p Name.
+    std::optional<unsigned> getAnnotationIndex(StringRef Name) const {
+        auto AI = AnnotationNameIndexMap.find(Name);
+        if (AI != AnnotationNameIndexMap.end())
+            return AI->second;
+        return std::nullopt;
+    }
+
+    /// Return annotation index matching the \p Name. Create a new index if the
+    /// \p Name wasn't registered previously.
+    unsigned getOrCreateAnnotationIndex(StringRef Name) {
+        auto AI = AnnotationNameIndexMap.find(Name);
+        if (AI != AnnotationNameIndexMap.end())
+            return AI->second;
+
+        const unsigned Index =
+            AnnotationNameIndexMap.size() + MCPlus::MCAnnotation::kGeneric;
+        AnnotationNameIndexMap.insert(std::make_pair(Name, Index));
+        AnnotationNames.emplace_back(std::string(Name));
+        return Index;
+    }
+
+    /// Store an annotation value on an MCInst.  This assumes the annotation
+    /// is not already present.
+    template <typename ValueType>
+    const ValueType &addAnnotation(MCInst &Inst, unsigned Index,
+                                   const ValueType &Val,
+                                   AllocatorIdTy    AllocatorId = 0) {
+        assert(!hasAnnotation(Inst, Index));
+        AnnotationAllocator &Allocator = getAnnotationAllocator(AllocatorId);
+        auto                *A         = new (Allocator.ValueAllocator)
+            MCPlus::MCSimpleAnnotation<ValueType>(Val);
+
+        if (!std::is_trivial<ValueType>::value)
+            Allocator.AnnotationPool.insert(A);
+        setAnnotationOpValue(Inst, Index, reinterpret_cast<int64_t>(A),
+                             AllocatorId);
+        return A->getValue();
+    }
+
+    /// Store an annotation value on an MCInst.  This assumes the annotation
+    /// is not already present.
+    template <typename ValueType>
+    const ValueType &addAnnotation(MCInst &Inst, StringRef Name,
+                                   const ValueType &Val,
+                                   AllocatorIdTy    AllocatorId = 0) {
+        return addAnnotation(Inst, getOrCreateAnnotationIndex(Name), Val,
+                             AllocatorId);
+    }
+
+    /// Get an annotation as a specific value, but if the annotation does not
+    /// exist, create a new annotation with the default constructor for that
+    /// type. Return a non-const ref so caller can freely modify its contents
+    /// afterwards.
+    template <typename ValueType>
+    ValueType &getOrCreateAnnotationAs(MCInst &Inst, unsigned Index,
+                                       AllocatorIdTy AllocatorId = 0) {
+        auto Val = tryGetAnnotationAs<ValueType>(
+            const_cast<const MCInst &>(Inst), Index);
+        if (!Val)
+            Val = addAnnotation(Inst, Index, ValueType(), AllocatorId);
+        return const_cast<ValueType &>(*Val);
+    }
+
+    /// Get an annotation as a specific value, but if the annotation does not
+    /// exist, create a new annotation with the default constructor for that
+    /// type. Return a non-const ref so caller can freely modify its contents
+    /// afterwards.
+    template <typename ValueType>
+    ValueType &getOrCreateAnnotationAs(MCInst &Inst, StringRef Name,
+                                       AllocatorIdTy AllocatorId = 0) {
+        const unsigned Index = getOrCreateAnnotationIndex(Name);
+        return getOrCreateAnnotationAs<ValueType>(Inst, Index, AllocatorId);
+    }
+
+    /// Get an annotation as a specific value. Assumes that the annotation
+    /// exists. Use hasAnnotation() if the annotation may not exist.
+    template <typename ValueType>
+    ValueType &getAnnotationAs(const MCInst &Inst, unsigned Index) const {
+        std::optional<int64_t> Value = getAnnotationOpValue(Inst, Index);
+        assert(Value && "annotation should exist");
+        return reinterpret_cast<MCPlus::MCSimpleAnnotation<ValueType> *>(*Value)
+            ->getValue();
+    }
+
+    /// Get an annotation as a specific value. Assumes that the annotation
+    /// exists. Use hasAnnotation() if the annotation may not exist.
+    template <typename ValueType>
+    ValueType &getAnnotationAs(const MCInst &Inst, StringRef Name) const {
+        const auto Index = getAnnotationIndex(Name);
+        assert(Index && "annotation should exist");
+        return getAnnotationAs<ValueType>(Inst, *Index);
+    }
+
+    /// Get an annotation as a specific value. If the annotation does not exist,
+    /// return the \p DefaultValue.
+    template <typename ValueType>
+    const ValueType &
+    getAnnotationWithDefault(const MCInst &Inst, unsigned Index,
+                             const ValueType &DefaultValue = ValueType()) {
+        if (!hasAnnotation(Inst, Index))
+            return DefaultValue;
+        return getAnnotationAs<ValueType>(Inst, Index);
+    }
+
+    /// Get an annotation as a specific value. If the annotation does not exist,
+    /// return the \p DefaultValue.
+    template <typename ValueType>
+    const ValueType &
+    getAnnotationWithDefault(const MCInst &Inst, StringRef Name,
+                             const ValueType &DefaultValue = ValueType()) {
+        const unsigned Index = getOrCreateAnnotationIndex(Name);
+        return getAnnotationWithDefault<ValueType>(Inst, Index, DefaultValue);
+    }
+
+    /// Check if the specified annotation exists on this instruction.
+    bool hasAnnotation(const MCInst &Inst, unsigned Index) const;
+
+    /// Check if an annotation with a specified \p Name exists on \p Inst.
+    bool hasAnnotation(const MCInst &Inst, StringRef Name) const {
+        const auto Index = getAnnotationIndex(Name);
+        if (!Index)
+            return false;
+        return hasAnnotation(Inst, *Index);
+    }
+
+    /// Get an annotation as a specific value, but if the annotation does not
+    /// exist, return errc::result_out_of_range.
+    template <typename ValueType>
+    ErrorOr<const ValueType &> tryGetAnnotationAs(const MCInst &Inst,
+                                                  unsigned      Index) const {
+        if (!hasAnnotation(Inst, Index))
+            return make_error_code(std::errc::result_out_of_range);
+        return getAnnotationAs<ValueType>(Inst, Index);
+    }
+
+    /// Get an annotation as a specific value, but if the annotation does not
+    /// exist, return errc::result_out_of_range.
+    template <typename ValueType>
+    ErrorOr<const ValueType &> tryGetAnnotationAs(const MCInst &Inst,
+                                                  StringRef     Name) const {
+        const auto Index = getAnnotationIndex(Name);
+        if (!Index)
+            return make_error_code(std::errc::result_out_of_range);
+        return tryGetAnnotationAs<ValueType>(Inst, *Index);
+    }
+
+    template <typename ValueType>
+    ErrorOr<ValueType &> tryGetAnnotationAs(MCInst  &Inst,
+                                            unsigned Index) const {
+        if (!hasAnnotation(Inst, Index))
+            return make_error_code(std::errc::result_out_of_range);
+        return const_cast<ValueType &>(getAnnotationAs<ValueType>(Inst, Index));
+    }
+
+    template <typename ValueType>
+    ErrorOr<ValueType &> tryGetAnnotationAs(MCInst   &Inst,
+                                            StringRef Name) const {
+        const auto Index = getAnnotationIndex(Name);
+        if (!Index)
+            return make_error_code(std::errc::result_out_of_range);
+        return tryGetAnnotationAs<ValueType>(Inst, *Index);
+    }
+
+    /// Print each annotation attached to \p Inst.
+    void printAnnotations(const MCInst &Inst, raw_ostream &OS) const;
+
+    /// Remove annotation with a given \p Index.
+    ///
+    /// Return true if the annotation was removed, false if the annotation
+    /// was not present.
+    bool removeAnnotation(MCInst &Inst, unsigned Index);
+
+    /// Remove annotation associated with \p Name.
+    ///
+    /// Return true if the annotation was removed, false if the annotation
+    /// was not present.
+    bool removeAnnotation(MCInst &Inst, StringRef Name) {
+        const auto Index = getAnnotationIndex(Name);
+        if (!Index)
+            return false;
+        return removeAnnotation(Inst, *Index);
+    }
+
+    /// Remove meta-data, but don't destroy it.
+    void stripAnnotations(MCInst &Inst, bool KeepTC = false);
+
+    virtual InstructionListType
+    createInstrumentedIndirectCall(MCInst &&CallInst, MCSymbol *HandlerFuncAddr,
+                                   int CallSiteID, MCContext *Ctx) {
+        llvm_unreachable("not implemented");
+        return InstructionListType();
+    }
+
+    virtual InstructionListType createInstrumentedIndCallHandlerExitBB() const {
+        llvm_unreachable("not implemented");
+        return InstructionListType();
+    }
+
+    virtual InstructionListType
+    createInstrumentedIndTailCallHandlerExitBB() const {
+        llvm_unreachable("not implemented");
+        return InstructionListType();
+    }
+
+    virtual InstructionListType
+    createInstrumentedIndCallHandlerEntryBB(const MCSymbol *InstrTrampoline,
+                                            const MCSymbol *IndCallHandler,
+                                            MCContext      *Ctx) {
+        llvm_unreachable("not implemented");
+        return InstructionListType();
+    }
+
+    virtual InstructionListType createNumCountersGetter(MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return {};
+    }
+
+    virtual InstructionListType
+    createInstrLocationsGetter(MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return {};
+    }
+
+    virtual InstructionListType createInstrTablesGetter(MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return {};
+    }
+
+    virtual InstructionListType
+    createInstrNumFuncsGetter(MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return {};
+    }
+
+    virtual InstructionListType createSymbolTrampoline(const MCSymbol *TgtSym,
+                                                       MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return InstructionListType();
+    }
+
+    virtual InstructionListType
+    createDummyReturnFunction(MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return InstructionListType();
+    }
+
+    /// This method takes an indirect call instruction and splits it up into an
+    /// equivalent set of instructions that use direct calls for target
+    /// symbols/addresses that are contained in the Targets vector.  This is
+    /// done by guarding each direct call with a compare instruction to verify
+    /// that the target is correct. If the VtableAddrs vector is not empty, the
+    /// call will have the extra load of the method pointer from the vtable
+    /// eliminated.  When non-empty the VtableAddrs vector must be the same size
+    /// as Targets and include the address of a vtable for each corresponding
+    /// method call in Targets.  The MethodFetchInsns vector holds instructions
+    /// that are used to load the correct method for the cold call case.
+    ///
+    /// The return value is a vector of code snippets (essentially basic
+    /// blocks). There is a symbol associated with each snippet except for the
+    /// first. If the original call is not a tail call, the last snippet will
+    /// have an empty vector of instructions.  The label is meant to indicate
+    /// the basic block where all previous snippets are joined, i.e. the
+    /// instructions that would immediate follow the original call.
+    using BlocksVectorTy =
+        std::vector<std::pair<MCSymbol *, InstructionListType>>;
+    struct MultiBlocksCode {
+        BlocksVectorTy          Blocks;
+        std::vector<MCSymbol *> Successors;
+    };
+
+    virtual BlocksVectorTy indirectCallPromotion(
+        const MCInst                                       &CallInst,
+        const std::vector<std::pair<MCSymbol *, uint64_t>> &Targets,
+        const std::vector<std::pair<MCSymbol *, uint64_t>> &VtableSyms,
+        const std::vector<MCInst *>                        &MethodFetchInsns,
+        const bool MinimizeCodeSize, MCContext *Ctx) {
+        llvm_unreachable("not implemented");
+        return BlocksVectorTy();
+    }
+
+    virtual BlocksVectorTy jumpTablePromotion(
+        const MCInst                                       &IJmpInst,
+        const std::vector<std::pair<MCSymbol *, uint64_t>> &Targets,
+        const std::vector<MCInst *> &TargetFetchInsns, MCContext *Ctx) const {
+        llvm_unreachable("not implemented");
+        return BlocksVectorTy();
+    }
+
+    // AliasMap caches a mapping of registers to the set of registers that
+    // alias (are sub or superregs of itself, including itself).
+    std::vector<BitVector> AliasMap;
+    std::vector<BitVector> SmallerAliasMap;
+    // SizeMap caches a mapping of registers to their sizes.
+    std::vector<uint8_t> SizeMap;
 };
 
 MCPlusBuilder *createX86MCPlusBuilder(const MCInstrAnalysis *,
@@ -2001,7 +2151,7 @@ MCPlusBuilder *createAArch64MCPlusBuilder(const MCInstrAnalysis *,
                                           const MCInstrInfo *,
                                           const MCRegisterInfo *);
 
-} // namespace bolt
-} // namespace llvm
+}   // namespace bolt
+}   // namespace llvm
 
 #endif
diff --git a/bolt/include/bolt/Passes/BinaryPasses.h b/bolt/include/bolt/Passes/BinaryPasses.h
index 63cef59cfd4e..c3ed04ef8dea 100644
--- a/bolt/include/bolt/Passes/BinaryPasses.h
+++ b/bolt/include/bolt/Passes/BinaryPasses.h
@@ -16,11 +16,14 @@
 #include "bolt/Core/BinaryContext.h"
 #include "bolt/Core/BinaryFunction.h"
 #include "bolt/Core/DynoStats.h"
+#include "bolt/Passes/DataflowInfoManager.h"
 #include "llvm/Support/CommandLine.h"
+
 #include <atomic>
 #include <map>
 #include <set>
 #include <string>
+#include <unordered_map>
 #include <unordered_set>
 
 namespace llvm {
@@ -28,189 +31,192 @@ namespace bolt {
 
 /// An optimization/analysis pass that runs on functions.
 class BinaryFunctionPass {
-protected:
-  bool PrintPass;
+  protected:
+    bool PrintPass;
 
-  explicit BinaryFunctionPass(const bool PrintPass) : PrintPass(PrintPass) {}
+    explicit BinaryFunctionPass(const bool PrintPass) : PrintPass(PrintPass) {}
 
-  /// Control whether a specific function should be skipped during
-  /// optimization.
-  virtual bool shouldOptimize(const BinaryFunction &BF) const;
+    /// Control whether a specific function should be skipped during
+    /// optimization.
+    virtual bool shouldOptimize(const BinaryFunction &BF) const;
 
-public:
-  virtual ~BinaryFunctionPass() = default;
+  public:
+    virtual ~BinaryFunctionPass() = default;
 
-  /// The name of this pass
-  virtual const char *getName() const = 0;
+    /// The name of this pass
+    virtual const char *getName() const = 0;
 
-  /// Control whether debug info is printed after this pass is completed.
-  bool printPass() const { return PrintPass; }
+    /// Control whether debug info is printed after this pass is completed.
+    bool printPass() const { return PrintPass; }
 
-  /// Control whether debug info is printed for an individual function after
-  /// this pass is completed (printPass() must have returned true).
-  virtual bool shouldPrint(const BinaryFunction &BF) const;
+    /// Control whether debug info is printed for an individual function after
+    /// this pass is completed (printPass() must have returned true).
+    virtual bool shouldPrint(const BinaryFunction &BF) const;
 
-  /// Execute this pass on the given functions.
-  virtual void runOnFunctions(BinaryContext &BC) = 0;
+    /// Execute this pass on the given functions.
+    virtual void runOnFunctions(BinaryContext &BC) = 0;
 };
 
 /// A pass to print program-wide dynostats.
 class DynoStatsPrintPass : public BinaryFunctionPass {
-protected:
-  DynoStats PrevDynoStats;
-  std::string Title;
-
-public:
-  DynoStatsPrintPass(const DynoStats &PrevDynoStats, const char *Title)
-      : BinaryFunctionPass(false), PrevDynoStats(PrevDynoStats), Title(Title) {}
-
-  const char *getName() const override {
-    return "print dyno-stats after optimizations";
-  }
-
-  bool shouldPrint(const BinaryFunction &BF) const override { return false; }
-
-  void runOnFunctions(BinaryContext &BC) override {
-    const DynoStats NewDynoStats =
-        getDynoStats(BC.getBinaryFunctions(), BC.isAArch64());
-    const bool Changed = (NewDynoStats != PrevDynoStats);
-    outs() << "BOLT-INFO: program-wide dynostats " << Title
-           << (Changed ? "" : " (no change)") << ":\n\n"
-           << PrevDynoStats;
-    if (Changed) {
-      outs() << '\n';
-      NewDynoStats.print(outs(), &PrevDynoStats, BC.InstPrinter.get());
+  protected:
+    DynoStats   PrevDynoStats;
+    std::string Title;
+
+  public:
+    DynoStatsPrintPass(const DynoStats &PrevDynoStats, const char *Title)
+        : BinaryFunctionPass(false), PrevDynoStats(PrevDynoStats),
+          Title(Title) {}
+
+    const char *getName() const override {
+        return "print dyno-stats after optimizations";
+    }
+
+    bool shouldPrint(const BinaryFunction &BF) const override { return false; }
+
+    void runOnFunctions(BinaryContext &BC) override {
+        const DynoStats NewDynoStats =
+            getDynoStats(BC.getBinaryFunctions(), BC.isAArch64());
+        const bool Changed = (NewDynoStats != PrevDynoStats);
+        outs() << "BOLT-INFO: program-wide dynostats " << Title
+               << (Changed ? "" : " (no change)") << ":\n\n"
+               << PrevDynoStats;
+        if (Changed) {
+            outs() << '\n';
+            NewDynoStats.print(outs(), &PrevDynoStats, BC.InstPrinter.get());
+        }
+        outs() << '\n';
     }
-    outs() << '\n';
-  }
 };
 
 /// The pass normalizes CFG by performing the following transformations:
 ///   * removes empty basic blocks
 ///   * merges duplicate edges and updates jump instructions
 class NormalizeCFG : public BinaryFunctionPass {
-  std::atomic<uint64_t> NumBlocksRemoved{0};
-  std::atomic<uint64_t> NumDuplicateEdgesMerged{0};
+    std::atomic<uint64_t> NumBlocksRemoved{0};
+    std::atomic<uint64_t> NumDuplicateEdgesMerged{0};
 
-  void runOnFunction(BinaryFunction &BF);
+    void runOnFunction(BinaryFunction &BF);
 
-public:
-  NormalizeCFG(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    NormalizeCFG(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "normalize CFG"; }
+    const char *getName() const override { return "normalize CFG"; }
 
-  void runOnFunctions(BinaryContext &) override;
+    void runOnFunctions(BinaryContext &) override;
 };
 
 /// Detect and eliminate unreachable basic blocks. We could have those
 /// filled with nops and they are used for alignment.
 class EliminateUnreachableBlocks : public BinaryFunctionPass {
-  std::unordered_set<const BinaryFunction *> Modified;
-  std::atomic<unsigned> DeletedBlocks{0};
-  std::atomic<uint64_t> DeletedBytes{0};
-  void runOnFunction(BinaryFunction &Function);
-
-public:
-  EliminateUnreachableBlocks(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
-
-  const char *getName() const override { return "eliminate-unreachable"; }
-  bool shouldPrint(const BinaryFunction &BF) const override {
-    return BinaryFunctionPass::shouldPrint(BF) && Modified.count(&BF) > 0;
-  }
-  void runOnFunctions(BinaryContext &) override;
+    std::unordered_set<const BinaryFunction *> Modified;
+    std::atomic<unsigned>                      DeletedBlocks{0};
+    std::atomic<uint64_t>                      DeletedBytes{0};
+    void runOnFunction(BinaryFunction &Function);
+
+  public:
+    EliminateUnreachableBlocks(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
+
+    const char *getName() const override { return "eliminate-unreachable"; }
+    bool        shouldPrint(const BinaryFunction &BF) const override {
+        return BinaryFunctionPass::shouldPrint(BF) && Modified.count(&BF) > 0;
+    }
+    void runOnFunctions(BinaryContext &) override;
 };
 
 // Reorder the basic blocks for each function based on hotness.
 class ReorderBasicBlocks : public BinaryFunctionPass {
-public:
-  /// Choose which strategy should the block layout heuristic prioritize when
-  /// facing conflicting goals.
-  enum LayoutType : char {
-    /// LT_NONE - do not change layout of basic blocks
-    LT_NONE = 0, /// no reordering
-    /// LT_REVERSE - reverse the order of basic blocks, meant for testing
-    /// purposes. The first basic block is left intact and the rest are
-    /// put in the reverse order.
-    LT_REVERSE,
-    /// LT_OPTIMIZE - optimize layout of basic blocks based on profile.
-    LT_OPTIMIZE,
-    /// LT_OPTIMIZE_BRANCH is an implementation of what is suggested in Pettis'
-    /// paper (PLDI '90) about block reordering, trying to minimize branch
-    /// mispredictions.
-    LT_OPTIMIZE_BRANCH,
-    /// LT_OPTIMIZE_CACHE piggybacks on the idea from Ispike paper (CGO '04)
-    /// that suggests putting frequently executed chains first in the layout.
-    LT_OPTIMIZE_CACHE,
-    // CACHE_PLUS and EXT_TSP are synonyms, emit warning of deprecation.
-    LT_OPTIMIZE_CACHE_PLUS,
-    /// Block reordering guided by the extended TSP metric.
-    LT_OPTIMIZE_EXT_TSP,
-    /// Create clusters and use random order for them.
-    LT_OPTIMIZE_SHUFFLE,
-  };
-
-private:
-  /// Run the specified layout algorithm on the given function. Returns `true`
-  /// if the order of blocks was changed.
-  bool modifyFunctionLayout(BinaryFunction &Function, LayoutType Type,
-                            bool MinBranchClusters) const;
-
-public:
-  explicit ReorderBasicBlocks(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
-
-  bool shouldOptimize(const BinaryFunction &BF) const override;
-
-  const char *getName() const override { return "reorder-blocks"; }
-  bool shouldPrint(const BinaryFunction &BF) const override;
-  void runOnFunctions(BinaryContext &BC) override;
+  public:
+    /// Choose which strategy should the block layout heuristic prioritize when
+    /// facing conflicting goals.
+    enum LayoutType : char {
+        /// LT_NONE - do not change layout of basic blocks
+        LT_NONE = 0,   /// no reordering
+        /// LT_REVERSE - reverse the order of basic blocks, meant for testing
+        /// purposes. The first basic block is left intact and the rest are
+        /// put in the reverse order.
+        LT_REVERSE,
+        /// LT_OPTIMIZE - optimize layout of basic blocks based on profile.
+        LT_OPTIMIZE,
+        /// LT_OPTIMIZE_BRANCH is an implementation of what is suggested in
+        /// Pettis'
+        /// paper (PLDI '90) about block reordering, trying to minimize branch
+        /// mispredictions.
+        LT_OPTIMIZE_BRANCH,
+        /// LT_OPTIMIZE_CACHE piggybacks on the idea from Ispike paper (CGO '04)
+        /// that suggests putting frequently executed chains first in the
+        /// layout.
+        LT_OPTIMIZE_CACHE,
+        // CACHE_PLUS and EXT_TSP are synonyms, emit warning of deprecation.
+        LT_OPTIMIZE_CACHE_PLUS,
+        /// Block reordering guided by the extended TSP metric.
+        LT_OPTIMIZE_EXT_TSP,
+        /// Create clusters and use random order for them.
+        LT_OPTIMIZE_SHUFFLE,
+    };
+
+  private:
+    /// Run the specified layout algorithm on the given function. Returns `true`
+    /// if the order of blocks was changed.
+    bool modifyFunctionLayout(BinaryFunction &Function, LayoutType Type,
+                              bool MinBranchClusters) const;
+
+  public:
+    explicit ReorderBasicBlocks(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
+
+    bool shouldOptimize(const BinaryFunction &BF) const override;
+
+    const char *getName() const override { return "reorder-blocks"; }
+    bool        shouldPrint(const BinaryFunction &BF) const override;
+    void        runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Sync local branches with CFG.
 class FixupBranches : public BinaryFunctionPass {
-public:
-  explicit FixupBranches(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit FixupBranches(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "fix-branches"; }
-  void runOnFunctions(BinaryContext &BC) override;
+    const char *getName() const override { return "fix-branches"; }
+    void        runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Fix the CFI state and exception handling information after all other
 /// passes have completed.
 class FinalizeFunctions : public BinaryFunctionPass {
-public:
-  explicit FinalizeFunctions(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit FinalizeFunctions(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "finalize-functions"; }
-  void runOnFunctions(BinaryContext &BC) override;
+    const char *getName() const override { return "finalize-functions"; }
+    void        runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Perform any necessary adjustments for functions that do not fit into their
 /// original space in non-relocation mode.
 class CheckLargeFunctions : public BinaryFunctionPass {
-public:
-  explicit CheckLargeFunctions(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit CheckLargeFunctions(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "check-large-functions"; }
+    const char *getName() const override { return "check-large-functions"; }
 
-  void runOnFunctions(BinaryContext &BC) override;
+    void runOnFunctions(BinaryContext &BC) override;
 
-  bool shouldOptimize(const BinaryFunction &BF) const override;
+    bool shouldOptimize(const BinaryFunction &BF) const override;
 };
 
 /// Convert and remove all BOLT-related annotations before LLVM code emission.
 class LowerAnnotations : public BinaryFunctionPass {
-public:
-  explicit LowerAnnotations(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit LowerAnnotations(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "lower-annotations"; }
-  void runOnFunctions(BinaryContext &BC) override;
+    const char *getName() const override { return "lower-annotations"; }
+    void        runOnFunctions(BinaryContext &BC) override;
 };
 
 /// An optimization to simplify conditional tail calls by removing
@@ -254,82 +260,83 @@ public:
 /// We assume that the target of the conditional branch is the
 /// first successor.
 class SimplifyConditionalTailCalls : public BinaryFunctionPass {
-  uint64_t NumCandidateTailCalls{0};
-  uint64_t NumTailCallsPatched{0};
-  uint64_t CTCExecCount{0};
-  uint64_t CTCTakenCount{0};
-  uint64_t NumOrigForwardBranches{0};
-  uint64_t NumOrigBackwardBranches{0};
-  uint64_t NumDoubleJumps{0};
-  uint64_t DeletedBlocks{0};
-  uint64_t DeletedBytes{0};
-  std::unordered_set<const BinaryFunction *> Modified;
-  std::set<const BinaryBasicBlock *> BeenOptimized;
-
-  bool shouldRewriteBranch(const BinaryBasicBlock *PredBB,
-                           const MCInst &CondBranch, const BinaryBasicBlock *BB,
-                           const bool DirectionFlag);
-
-  uint64_t fixTailCalls(BinaryFunction &BF);
-
-public:
-  explicit SimplifyConditionalTailCalls(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
-
-  const char *getName() const override {
-    return "simplify-conditional-tail-calls";
-  }
-  bool shouldPrint(const BinaryFunction &BF) const override {
-    return BinaryFunctionPass::shouldPrint(BF) && Modified.count(&BF) > 0;
-  }
-  void runOnFunctions(BinaryContext &BC) override;
+    uint64_t                                   NumCandidateTailCalls{0};
+    uint64_t                                   NumTailCallsPatched{0};
+    uint64_t                                   CTCExecCount{0};
+    uint64_t                                   CTCTakenCount{0};
+    uint64_t                                   NumOrigForwardBranches{0};
+    uint64_t                                   NumOrigBackwardBranches{0};
+    uint64_t                                   NumDoubleJumps{0};
+    uint64_t                                   DeletedBlocks{0};
+    uint64_t                                   DeletedBytes{0};
+    std::unordered_set<const BinaryFunction *> Modified;
+    std::set<const BinaryBasicBlock *>         BeenOptimized;
+
+    bool shouldRewriteBranch(const BinaryBasicBlock *PredBB,
+                             const MCInst           &CondBranch,
+                             const BinaryBasicBlock *BB,
+                             const bool              DirectionFlag);
+
+    uint64_t fixTailCalls(BinaryFunction &BF);
+
+  public:
+    explicit SimplifyConditionalTailCalls(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
+
+    const char *getName() const override {
+        return "simplify-conditional-tail-calls";
+    }
+    bool shouldPrint(const BinaryFunction &BF) const override {
+        return BinaryFunctionPass::shouldPrint(BF) && Modified.count(&BF) > 0;
+    }
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Convert instructions to the form with the minimum operand width.
 class ShortenInstructions : public BinaryFunctionPass {
-  uint64_t shortenInstructions(BinaryFunction &Function);
+    uint64_t shortenInstructions(BinaryFunction &Function);
 
-public:
-  explicit ShortenInstructions(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit ShortenInstructions(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "shorten-instructions"; }
+    const char *getName() const override { return "shorten-instructions"; }
 
-  void runOnFunctions(BinaryContext &BC) override;
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Perform simple peephole optimizations.
 class Peepholes : public BinaryFunctionPass {
-public:
-  enum PeepholeOpts : char {
-    PEEP_NONE = 0x0,
-    PEEP_DOUBLE_JUMPS = 0x2,
-    PEEP_TAILCALL_TRAPS = 0x4,
-    PEEP_USELESS_BRANCHES = 0x8,
-    PEEP_ALL = 0xf
-  };
-
-private:
-  uint64_t NumDoubleJumps{0};
-  uint64_t TailCallTraps{0};
-  uint64_t NumUselessCondBranches{0};
-
-  /// Add trap instructions immediately after indirect tail calls to prevent
-  /// the processor from decoding instructions immediate following the
-  /// tailcall.
-  void addTailcallTraps(BinaryFunction &Function);
-
-  /// Remove useless duplicate successors.  When the conditional
-  /// successor is the same as the unconditional successor, we can
-  /// remove the conditional successor and branch instruction.
-  void removeUselessCondBranches(BinaryFunction &Function);
-
-public:
-  explicit Peepholes(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
-
-  const char *getName() const override { return "peepholes"; }
-  void runOnFunctions(BinaryContext &BC) override;
+  public:
+    enum PeepholeOpts : char {
+        PEEP_NONE             = 0x0,
+        PEEP_DOUBLE_JUMPS     = 0x2,
+        PEEP_TAILCALL_TRAPS   = 0x4,
+        PEEP_USELESS_BRANCHES = 0x8,
+        PEEP_ALL              = 0xf
+    };
+
+  private:
+    uint64_t NumDoubleJumps{0};
+    uint64_t TailCallTraps{0};
+    uint64_t NumUselessCondBranches{0};
+
+    /// Add trap instructions immediately after indirect tail calls to prevent
+    /// the processor from decoding instructions immediate following the
+    /// tailcall.
+    void addTailcallTraps(BinaryFunction &Function);
+
+    /// Remove useless duplicate successors.  When the conditional
+    /// successor is the same as the unconditional successor, we can
+    /// remove the conditional successor and branch instruction.
+    void removeUselessCondBranches(BinaryFunction &Function);
+
+  public:
+    explicit Peepholes(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
+
+    const char *getName() const override { return "peepholes"; }
+    void        runOnFunctions(BinaryContext &BC) override;
 };
 
 /// An optimization to simplify loads from read-only sections.The pass converts
@@ -344,32 +351,32 @@ public:
 /// when the target address points somewhere inside a read-only section.
 ///
 class SimplifyRODataLoads : public BinaryFunctionPass {
-  uint64_t NumLoadsSimplified{0};
-  uint64_t NumDynamicLoadsSimplified{0};
-  uint64_t NumLoadsFound{0};
-  uint64_t NumDynamicLoadsFound{0};
-  std::unordered_set<const BinaryFunction *> Modified;
-
-  bool simplifyRODataLoads(BinaryFunction &BF);
-
-public:
-  explicit SimplifyRODataLoads(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
-
-  const char *getName() const override { return "simplify-read-only-loads"; }
-  bool shouldPrint(const BinaryFunction &BF) const override {
-    return BinaryFunctionPass::shouldPrint(BF) && Modified.count(&BF) > 0;
-  }
-  void runOnFunctions(BinaryContext &BC) override;
+    uint64_t                                   NumLoadsSimplified{0};
+    uint64_t                                   NumDynamicLoadsSimplified{0};
+    uint64_t                                   NumLoadsFound{0};
+    uint64_t                                   NumDynamicLoadsFound{0};
+    std::unordered_set<const BinaryFunction *> Modified;
+
+    bool simplifyRODataLoads(BinaryFunction &BF);
+
+  public:
+    explicit SimplifyRODataLoads(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
+
+    const char *getName() const override { return "simplify-read-only-loads"; }
+    bool        shouldPrint(const BinaryFunction &BF) const override {
+        return BinaryFunctionPass::shouldPrint(BF) && Modified.count(&BF) > 0;
+    }
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Assign output sections to all functions.
 class AssignSections : public BinaryFunctionPass {
-public:
-  explicit AssignSections() : BinaryFunctionPass(false) {}
+  public:
+    explicit AssignSections() : BinaryFunctionPass(false) {}
 
-  const char *getName() const override { return "assign-sections"; }
-  void runOnFunctions(BinaryContext &BC) override;
+    const char *getName() const override { return "assign-sections"; }
+    void        runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Compute and report to the user the imbalance in flow equations for all
@@ -378,103 +385,118 @@ public:
 /// for blocks of interest (excluding prologues, epilogues, and BB frequency
 /// lower than 100).
 class PrintProfileStats : public BinaryFunctionPass {
-public:
-  explicit PrintProfileStats(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit PrintProfileStats(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "profile-stats"; }
-  bool shouldPrint(const BinaryFunction &) const override { return false; }
-  void runOnFunctions(BinaryContext &BC) override;
+    const char *getName() const override { return "profile-stats"; }
+    bool shouldPrint(const BinaryFunction &) const override { return false; }
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Prints a list of the top 100 functions sorted by a set of
 /// dyno stats categories.
 class PrintProgramStats : public BinaryFunctionPass {
-public:
-  explicit PrintProgramStats(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit PrintProgramStats(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "print-stats"; }
-  bool shouldPrint(const BinaryFunction &) const override { return false; }
-  void runOnFunctions(BinaryContext &BC) override;
+    const char *getName() const override { return "print-stats"; }
+    bool shouldPrint(const BinaryFunction &) const override { return false; }
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Pass for lowering any instructions that we have raised and that have
 /// to be lowered.
 class InstructionLowering : public BinaryFunctionPass {
-public:
-  explicit InstructionLowering(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit InstructionLowering(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "inst-lowering"; }
+    const char *getName() const override { return "inst-lowering"; }
 
-  void runOnFunctions(BinaryContext &BC) override;
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Pass for stripping 'repz' from 'repz retq' sequence of instructions.
 class StripRepRet : public BinaryFunctionPass {
-public:
-  explicit StripRepRet(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit StripRepRet(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "strip-rep-ret"; }
+    const char *getName() const override { return "strip-rep-ret"; }
 
-  void runOnFunctions(BinaryContext &BC) override;
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Pass for inlining calls to memcpy using 'rep movsb' on X86.
 class InlineMemcpy : public BinaryFunctionPass {
-public:
-  explicit InlineMemcpy(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit InlineMemcpy(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
 
-  const char *getName() const override { return "inline-memcpy"; }
+    const char *getName() const override { return "inline-memcpy"; }
 
-  void runOnFunctions(BinaryContext &BC) override;
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Pass for specializing memcpy for a size of 1 byte.
 class SpecializeMemcpy1 : public BinaryFunctionPass {
-private:
-  std::vector<std::string> Spec;
+  private:
+    std::vector<std::string> Spec;
 
-  /// Return indices of the call sites to optimize. Count starts at 1.
-  /// Returns an empty set for all call sites in the function.
-  std::set<size_t> getCallSitesToOptimize(const BinaryFunction &) const;
+    /// Return indices of the call sites to optimize. Count starts at 1.
+    /// Returns an empty set for all call sites in the function.
+    std::set<size_t> getCallSitesToOptimize(const BinaryFunction &) const;
 
-public:
-  explicit SpecializeMemcpy1(const cl::opt<bool> &PrintPass,
-                             cl::list<std::string> &Spec)
-      : BinaryFunctionPass(PrintPass), Spec(Spec) {}
+  public:
+    explicit SpecializeMemcpy1(const cl::opt<bool>   &PrintPass,
+                               cl::list<std::string> &Spec)
+        : BinaryFunctionPass(PrintPass), Spec(Spec) {}
 
-  bool shouldOptimize(const BinaryFunction &BF) const override;
+    bool shouldOptimize(const BinaryFunction &BF) const override;
 
-  const char *getName() const override { return "specialize-memcpy"; }
+    const char *getName() const override { return "specialize-memcpy"; }
 
-  void runOnFunctions(BinaryContext &BC) override;
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 /// Pass to remove nops in code
 class RemoveNops : public BinaryFunctionPass {
-  void runOnFunction(BinaryFunction &Function);
+    void runOnFunction(BinaryFunction &Function);
+
+  public:
+    explicit RemoveNops(const cl::opt<bool> &PrintPass)
+        : BinaryFunctionPass(PrintPass) {}
+
+    const char *getName() const override { return "remove-nops"; }
+
+    /// Pass entry point
+    void runOnFunctions(BinaryContext &BC) override;
+};
+
+
+class CheckFloatRet : public BinaryFunctionPass {
+  private:
+    void                         runOnFunction(BinaryFunction &Function);
+    std::unique_ptr<RegAnalysis> RA;
 
-public:
-  explicit RemoveNops(const cl::opt<bool> &PrintPass)
-      : BinaryFunctionPass(PrintPass) {}
+  public:
+    explicit CheckFloatRet() : BinaryFunctionPass(true) {}
 
-  const char *getName() const override { return "remove-nops"; }
+    const char *getName() const override { return "check-float-ret"; }
 
-  /// Pass entry point
-  void runOnFunctions(BinaryContext &BC) override;
+    /// Pass entry point
+    void runOnFunctions(BinaryContext &BC) override;
 };
 
 enum FrameOptimizationType : char {
-  FOP_NONE, /// Don't perform FOP.
-  FOP_HOT,  /// Perform FOP on hot functions.
-  FOP_ALL   /// Perform FOP on all functions.
+    FOP_NONE,   /// Don't perform FOP.
+    FOP_HOT,    /// Perform FOP on hot functions.
+    FOP_ALL     /// Perform FOP on all functions.
 };
 
-} // namespace bolt
-} // namespace llvm
+}   // namespace bolt
+}   // namespace llvm
 
 #endif
diff --git a/bolt/include/bolt/Passes/DataflowAnalysis.h b/bolt/include/bolt/Passes/DataflowAnalysis.h
index 2afaa6d3043a..6b36a8e2250a 100644
--- a/bolt/include/bolt/Passes/DataflowAnalysis.h
+++ b/bolt/include/bolt/Passes/DataflowAnalysis.h
@@ -194,6 +194,8 @@ protected:
     llvm_unreachable("Unimplemented method");
   }
 
+  void postDoConfluence(StateTy &StateOut) {}
+
   /// In case of a forwards dataflow, compute the in set for the first
   /// instruction in a Landing Pad considering all out sets for associated
   /// throw sites.
@@ -202,7 +204,7 @@ protected:
   /// landing pads.
   void doConfluenceWithLP(StateTy &StateOut, const StateTy &StateIn,
                           const MCInst &Invoke) {
-    return derived().doConfluence(StateOut, StateIn);
+    derived().doConfluence(StateOut, StateIn);
   }
 
   /// Returns the out set of an instruction given its in set.
@@ -363,13 +365,16 @@ public:
           if (P.isInst() && BC.MIB->isInvoke(*P.getInst()))
             derived().doConfluenceWithLP(StateAtEntry, *getStateAt(P),
                                          *P.getInst());
-          else
+          else {
             derived().doConfluence(StateAtEntry, *getStateAt(P));
+          }
         });
+        derived().postDoConfluence(StateAtEntry);
       } else {
         doForAllSuccsOrPreds(*BB, [&](ProgramPoint P) {
           derived().doConfluence(StateAtEntry, *getStateAt(P));
         });
+        derived().postDoConfluence(StateAtEntry);
       }
 
       bool Changed = false;
diff --git a/bolt/include/bolt/Passes/LivenessAnalysis.h b/bolt/include/bolt/Passes/LivenessAnalysis.h
index 5280fc293c87..2c937527d42f 100644
--- a/bolt/include/bolt/Passes/LivenessAnalysis.h
+++ b/bolt/include/bolt/Passes/LivenessAnalysis.h
@@ -74,7 +74,7 @@ protected:
     // values).
     if (BB.succ_size() == 0) {
       BitVector State(NumRegs, false);
-      if (opts::AssumeABI) {
+      if (opts::AssumeABI) { // [SAM let's turn it on.
         BC.MIB->getDefaultLiveOut(State);
         BC.MIB->getCalleeSavedRegs(State);
       } else {
@@ -129,11 +129,12 @@ protected:
 
       BitVector Used = BitVector(NumRegs, false);
       if (IsCall) {
-        RA.getInstUsedRegsList(Point, Used, /*GetClobbers*/ true);
+        RA.getInstUsedRegsList(Point, Used, /*GetClobbers*/ false); // SAM: changed from true to false
         if (RA.isConservative(Used)) {
           Used = BC.MIB->getRegsUsedAsParams();
-          BC.MIB->getDefaultLiveOut(Used);
-        }
+          //BC.MIB->getDefaultLiveOut(Used);
+        } 
       }
       const MCInstrDesc &InstInfo = BC.MII->get(Point.getOpcode());
       for (unsigned I = 0, E = Point.getNumOperands(); I != E; ++I) {
diff --git a/bolt/include/bolt/Passes/PrefetchAndYield.h b/bolt/include/bolt/Passes/PrefetchAndYield.h
new file mode 100644
index 000000000000..d66726820d0d
--- /dev/null
+++ b/bolt/include/bolt/Passes/PrefetchAndYield.h
@@ -0,0 +1,418 @@
+#ifndef BOLT_PASSES_PREFETCHANDYIELD_H
+#define BOLT_PASSES_PREFETCHANDYIELD_H
+
+#include "bolt/Passes/BinaryPasses.h"
+#include "bolt/Passes/YieldDistanceAnalysis.h"
+#include "llvm/Support/CommandLine.h"
+#include <sstream>
+
+using namespace llvm;
+
+namespace opts {
+extern cl::opt<bool> DisableAsymPushPop;
+extern cl::opt<bool> DisableLO;
+extern cl::opt<bool> DisableFUR;
+extern cl::opt<bool> DisableCtxPrefetch;
+extern cl::opt<bool> DisablePseudoInline;
+extern cl::opt<bool> InstScavenger;
+extern cl::opt<int>  BoundYieldDistance;
+extern cl::opt<int>  IterTh;
+}   // namespace opts
+
+namespace llvm {
+namespace bolt {
+
+extern std::stringstream pny_log_sstream;
+
+#define PY_PREPARE_LOG                                                         \
+    pny_log_sstream.str("");                                                   \
+    pny_log_sstream.clear();
+#define PY_LOG(x)    pny_log_sstream << x;
+#define PY_PRINT_LOG errs() << pny_log_sstream.str();
+
+typedef BinaryBasicBlock::iterator IterType;
+
+// used to force dependency between scan/instrumentations
+struct {
+    bool PseudoInlineScanDone = false;
+} PreScanProgress;
+
+struct {
+    bool PseudoInlineInstDone = false;
+} PreInstrumentationProgress;
+
+struct {
+    bool InitialScanDone         = false;
+    bool FilterUnusedRegScanDone = false;
+    bool PrefetchCrtCtxScantDone = false;
+    bool LoopOptScanDone         = false;
+} ScanProgress;
+
+struct {
+    bool DefaultInstDone         = false;
+    bool FilterUnusedRegInstDone = false;
+    bool PrefetchCrtCtxInstDone  = false;
+    bool LoopOptInstDone         = false;
+} InstrumentationProgress;
+
+// [SAM] our opt pass
+class PnYSubScanner {
+  protected:
+    BinaryFunction  &BF;
+    virtual void     annotateBB(BinaryBasicBlock &BB) {}
+    virtual void     annotateInst(MCInst &Inst, BinaryBasicBlock &BB) {}
+    virtual IterType annotateInst(IterType Iter, BinaryBasicBlock &BB) {
+        return Iter;
+    }
+
+  public:
+    PnYSubScanner(BinaryFunction &Function) : BF(Function) {}
+    void scanBB() {
+        for (auto &BB : BF)
+            annotateBB(BB);
+    }
+    void scanInst() {
+        for (auto &BB : BF) {
+            for (auto &Inst : BB) {
+                annotateInst(Inst, BB);
+            }
+        }
+    }
+    void scanInstIter() {
+        for (auto &BB : BF) {
+            for (auto Iter = BB.begin(); Iter != BB.end(); ++Iter) {
+                Iter = annotateInst(Iter, BB);
+            }
+        }
+    }
+
+    virtual void printStat() {}
+};
+
+class PnYSubInstrumenter {
+  protected:
+    BinaryFunction  &BF;
+    virtual IterType modifyInst(IterType &Iter, BinaryBasicBlock &BB) {
+        return Iter;
+    }
+
+    IterType insertInstructions(InstructionListType &Instrs,
+                                BinaryBasicBlock &BB, IterType Iter) {
+        for (MCInst &NewInst : Instrs) {
+            Iter = BB.insertInstruction(Iter, NewInst);
+            ++Iter;
+        }
+        return Iter;
+    }
+
+  public:
+    PnYSubInstrumenter(BinaryFunction &Function) : BF(Function) {}
+    void instrumentInst() {
+        for (auto &BB : BF) {
+            for (auto Iter = BB.begin(); Iter != BB.end(); ++Iter) {
+                Iter = modifyInst(Iter, BB);
+            }
+        }
+    }
+    void printStat() {}
+};
+
+struct IMEntry {
+    BinaryBasicBlock *BB;
+    BitVector         RegsToSave;
+};
+typedef std::map<uint64_t, IMEntry> InstMap;
+class IMHelper {
+  public:
+    static void addInstPoint(InstMap &IM, MCInst *Inst, BinaryBasicBlock *BB,
+                             BitVector RegsToSave) {
+        IMEntry IME;
+        IME.BB              = BB;
+        IME.RegsToSave      = RegsToSave;
+        IM[(uint64_t) Inst] = IME;
+    }
+};
+
+///// Subscanners
+class FilterUnusedRegScanner : public PnYSubScanner {
+  private:
+    BinaryContext &BC;
+    InstMap       &IM;
+    BitVector      UsedRegs;
+
+  public:
+    FilterUnusedRegScanner(BinaryFunction &Function, InstMap &IM)
+        : PnYSubScanner(Function), BC(Function.getBinaryContext()), IM(IM) {
+        UsedRegs = BitVector(BC.MRI->getNumRegs(), false);
+        assert(ScanProgress.InitialScanDone);
+        ScanProgress.FilterUnusedRegScanDone = true;
+    }
+    void updateState();
+
+  private:
+    void annotateInst(MCInst &Inst, BinaryBasicBlock &BB) override;
+};
+
+class PrefetchCrtCtxScanner : public PnYSubScanner {
+  private:
+    const BinaryContext &BC;
+
+  public:
+    PrefetchCrtCtxScanner(BinaryFunction &Function)
+        : PnYSubScanner(Function), BC(Function.getBinaryContext()) {
+        assert(ScanProgress.InitialScanDone);
+        ScanProgress.PrefetchCrtCtxScantDone = true;
+    }
+
+  private:
+    IterType annotateInst(IterType Iter, BinaryBasicBlock &BB) override;
+};
+
+class LoopOptScanner : public PnYSubScanner {
+  private:
+    BinaryContext      &BC;
+    InstMap            &IM;
+    DataflowInfoManager Info;
+    RegAnalysis        *RA;
+
+  public:
+    LoopOptScanner(BinaryFunction &Function, InstMap &IM, RegAnalysis *RA)
+        : PnYSubScanner(Function), BC(Function.getBinaryContext()), IM(IM),
+          Info(BF, RA, nullptr), RA(RA) {
+        assert(ScanProgress.InitialScanDone);
+        ScanProgress.LoopOptScanDone = true;
+    }
+    void printStat() override;
+
+  private:
+    void annotateInst(MCInst &Inst, BinaryBasicBlock &BB) override;
+    struct StatEntry {
+        int RegsToEvacNum;
+        int RegsToAsymPushNum;
+        int RegsToAsymPopNum;
+    };
+    std::map<uint64_t, StatEntry> StatMap;
+};
+
+class PseudoInlinePreScanner : public PnYSubScanner {
+  private:
+    BinaryContext &BC;
+
+  public:
+    PseudoInlinePreScanner(BinaryFunction &Function)
+        : PnYSubScanner(Function), BC(Function.getBinaryContext()) {
+        PreScanProgress.PseudoInlineScanDone = true;
+    }
+    void printStat() override;
+
+  private:
+    void annotateInst(MCInst &Inst, BinaryBasicBlock &BB) override;
+    int  StatPseudoInlineNum = 0;
+};
+
+class PseudoInlineScanner : public PnYSubScanner {
+  private:
+    BinaryContext &BC;
+
+  public:
+    PseudoInlineScanner(BinaryFunction &Function)
+        : PnYSubScanner(Function), BC(Function.getBinaryContext()) {}
+
+  private:
+    void annotateInst(MCInst &Inst, BinaryBasicBlock &BB) override;
+};
+
+class BoundYieldDistanceScanner : public PnYSubScanner {
+  public:
+    BoundYieldDistanceScanner(BinaryFunction &Function, RegAnalysis *RA);
+    void printStat();
+
+  private:
+    void     annotateInst(MCInst &Inst, BinaryBasicBlock &BB) override;
+    void     loopAnalysis();
+    void     performLoopAnalysis(BinaryLoop *Loop);
+    uint64_t MAX_YIELD_DISTANCE = 0;
+    std::map<const BinaryBasicBlock *, int> YieldCntMap;
+    std::map<const MCInst *, int>           IterationBoundMap;
+    BinaryContext                          &BC;
+    std::unique_ptr<YieldDistanceAnalysis>  YDA;
+    uint64_t                                YdCnt = 0;
+    RegAnalysis                            *RA;
+};
+
+///// SubInstrumenters
+class FilterUnusedRegInstrumenter : public PnYSubInstrumenter {
+  public:
+    FilterUnusedRegInstrumenter(BinaryFunction &Function)
+        : PnYSubInstrumenter(Function) {
+        assert(InstrumentationProgress.DefaultInstDone);
+        if (!opts::DisablePseudoInline)
+            assert(PreInstrumentationProgress.PseudoInlineInstDone);
+        InstrumentationProgress.FilterUnusedRegInstDone = true;
+    }
+    void instrumentFunc();
+    void printStat();
+
+  private:
+    int StatFilteredRegNum = 0;
+};
+
+class PrefetchCrtCtxInstrumenter : public PnYSubInstrumenter {
+  private:
+    BinaryContext &BC;
+
+  public:
+    PrefetchCrtCtxInstrumenter(BinaryFunction &Function)
+        : PnYSubInstrumenter(Function), BC(Function.getBinaryContext()) {
+
+        assert(InstrumentationProgress.DefaultInstDone);
+        InstrumentationProgress.PrefetchCrtCtxInstDone = true;
+    }
+    void printStat() {}
+
+  private:
+    IterType modifyInst(IterType &Iter, BinaryBasicBlock &BB) override;
+};
+
+class LoopOptInstrumenter : public PnYSubInstrumenter {
+  private:
+    BinaryContext  &BC;
+    const MCSymbol *CrtPos;
+
+  public:
+    LoopOptInstrumenter(BinaryFunction &Function)
+        : PnYSubInstrumenter(Function), BC(Function.getBinaryContext()) {
+
+        if (opts::InstScavenger)
+            CrtPos = BC.getBinaryDataByName("crt_pos")->getSymbol();
+        else
+            CrtPos = nullptr;
+
+        assert(InstrumentationProgress.DefaultInstDone);
+        if (!opts::DisablePseudoInline)
+            assert(PreInstrumentationProgress.PseudoInlineInstDone);
+        InstrumentationProgress.LoopOptInstDone = true;
+    }
+    void instrumentFunc();
+};
+
+class PseudoInlinePreInstrumenter : public PnYSubInstrumenter {
+  private:
+    BinaryContext                               &BC;
+    std::map<const MCSymbol *, const MCSymbol *> NamePatchMap;
+
+  public:
+    PseudoInlinePreInstrumenter(BinaryFunction &Function)
+        : PnYSubInstrumenter(Function), BC(Function.getBinaryContext()) {
+        PreInstrumentationProgress.PseudoInlineInstDone = true;
+    }
+
+  private:
+    IterType modifyInst(IterType &Iter, BinaryBasicBlock &BB) override;
+};
+
+class PseudoInlineInstrumenter : public PnYSubInstrumenter {
+  private:
+    BinaryContext &BC;
+
+  public:
+    PseudoInlineInstrumenter(BinaryFunction &Function)
+        : PnYSubInstrumenter(Function), BC(Function.getBinaryContext()) {}
+
+  private:
+    IterType modifyInst(IterType &Iter, BinaryBasicBlock &BB) override;
+};
+
+class BoundYieldDistanceInstrumenter : public PnYSubInstrumenter {
+  private:
+    BinaryContext &BC;
+    InstMap       &IM;
+    RegAnalysis   *RA;
+
+  public:
+    BoundYieldDistanceInstrumenter(BinaryFunction &Function, InstMap &IM,
+                                   RegAnalysis *RA)
+        : PnYSubInstrumenter(Function), IM(IM), RA(RA),
+          BC(Function.getBinaryContext()) {}
+    void instrumentFunc();
+    void printStat();
+};
+
+class PnYInstrumenter;
+class PnYScanner {
+  private:
+    friend class PnYInstrumenter;
+    InstMap         IM;   // list of (inst, RegsToSave) pair for each PnY point
+    BinaryFunction &BF;
+    RegAnalysis    *RA;
+
+  public:
+    PnYScanner(BinaryFunction &Function, RegAnalysis *RA)
+        : BF(Function), RA(RA) {
+        PY_LOG("PnYScanner begins...\n");
+
+        // set false all progress
+        memset(&ScanProgress, 0, sizeof(ScanProgress));
+    }
+    bool hasInstPoint() { return BF.HasPnYPoint; }
+    void scan();
+    void preScan();
+};
+
+class PnYInstrumenter {
+  private:
+    PnYScanner  &scanner;
+    RegAnalysis *RA;
+
+  public:
+    PnYInstrumenter(PnYScanner &s, RegAnalysis *RA) : scanner(s), RA(RA) {
+        PY_LOG("PnYInstrumenter begins...\n");
+
+        // set false all progress
+        memset(&InstrumentationProgress, 0, sizeof(InstrumentationProgress));
+    };
+
+    void instrument();
+    void preInstrument();
+};
+
+class PrefetchAndYield : public BinaryFunctionPass {
+  private:
+    void preScanOnFunction(BinaryFunction &Function);
+    void preInstrumentationOnFunction(BinaryFunction &Function);
+    void runOnFunction(BinaryFunction &Function);
+
+    // Pass maintains the global (in contrast to each function) state
+    RegAnalysis *RA;
+
+  public:
+    explicit PrefetchAndYield() : BinaryFunctionPass(true) {}
+
+    const char *getName() const override { return "prefetch-and-yield"; }
+
+    // Pass entry point
+    // PrefetchAndYield pass happens sequetially on each function not make
+    // implementation overly complex
+    void runOnFunctions(BinaryContext &BC) override;
+};
+
+/*
+class BoundYieldDistance : public BinaryFunctionPass {
+  private:
+    void runOnFunction(BinaryFunction &Function);
+
+  public:
+    explicit BoundYieldDistance() : BinaryFunctionPass(true) {}
+
+    const char *getName() const override { return "bound-yield-distance"; }
+
+    void runOnFunctions(BinaryContext &BC) override;
+};
+*/
+
+extern unsigned       next_regset_idx;
+extern const unsigned maximum_regset_size;
+}   // namespace bolt
+}   // namespace llvm
+
+#endif   // BOLT_PASSES_PREFETCHANDYIELD_H
\ No newline at end of file
diff --git a/bolt/include/bolt/Passes/ReachingDefOrUse.h b/bolt/include/bolt/Passes/ReachingDefOrUse.h
index f38d1a373e18..f42eb3040739 100644
--- a/bolt/include/bolt/Passes/ReachingDefOrUse.h
+++ b/bolt/include/bolt/Passes/ReachingDefOrUse.h
@@ -98,7 +98,14 @@ protected:
     BitVector YClobbers = BitVector(this->BC.MRI->getNumRegs(), false);
     RA.getInstClobberList(*X, XClobbers);
     // In defs, write after write -> kills first write
-    // In uses, write after access (read or write) -> kills access
+    // x = 3
+    // x = 1 # kills x = 3
+    // b = x + 3 # 
+    // In uses(backward), write after access (read or write) -> kills access
+    // [SAM]
+    // x = 3
+    // x = 1 
+    // b = x # x = 3 does not reach here, x = 1 reaches here
     if (Def)
       RA.getInstClobberList(*Y, YClobbers);
     else
diff --git a/bolt/include/bolt/Passes/RegAnalysis.h b/bolt/include/bolt/Passes/RegAnalysis.h
index da814cbeed7b..8d5930ba93b7 100644
--- a/bolt/include/bolt/Passes/RegAnalysis.h
+++ b/bolt/include/bolt/Passes/RegAnalysis.h
@@ -60,7 +60,7 @@ public:
 
   /// Print stats about the quality of our analysis
   void printStats();
-
+  void printMaps();
 private:
   BinaryContext &BC;
 
diff --git a/bolt/include/bolt/Passes/YieldDistanceAnalysis.h b/bolt/include/bolt/Passes/YieldDistanceAnalysis.h
new file mode 100644
index 000000000000..0219f00c765c
--- /dev/null
+++ b/bolt/include/bolt/Passes/YieldDistanceAnalysis.h
@@ -0,0 +1,464 @@
+//===- bolt/Passes/YieldDistanceAnalysis.h ---------------------------*- C++
+//-*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef BOLT_PASSES_YIELDDISTANCEANALYSIS_H
+#define BOLT_PASSES_YIELDDISTANCEANALYSIS_H
+
+#include "bolt/Passes/DataflowAnalysis.h"
+#include "bolt/Passes/RegAnalysis.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include "llvm/Support/CommandLine.h"
+
+#include <map>
+
+namespace opts {
+extern llvm::cl::opt<bool> AssumeABI;
+extern llvm::cl::opt<bool> TimeOpts;
+}   // namespace opts
+
+namespace llvm {
+namespace bolt {
+
+using PredecessorWeight = std::map<const BinaryBasicBlock *, float>;
+
+struct YieldDistance {
+    const MCInst           *YieldPoint;
+    uint64_t                Distance;
+    const BinaryBasicBlock *CurBB;
+
+    bool operator==(const YieldDistance &YD) const {
+        return (YieldPoint == YD.YieldPoint) && (Distance == YD.Distance) &&
+               (CurBB == YD.CurBB);
+    }
+
+    bool operator!=(const YieldDistance &YD) const {
+        return (YieldPoint != YD.YieldPoint) || (Distance != YD.Distance) ||
+               (CurBB != YD.CurBB);
+    }
+
+    friend raw_ostream &operator<<(raw_ostream &OS, const YieldDistance &YD) {
+        OS << "YieldPoint: " << YD.YieldPoint << " Distance: " << YD.Distance
+           << " CurBB: " << YD.CurBB;
+        return OS;
+    }
+};
+// typedef std::vector<YieldDistance> YDVector;
+class YDVector {
+  private:
+    std::vector<YieldDistance> data;
+
+  public:
+    std::map<const MCInst *, std::vector<YieldDistance>>
+        aggregate;   // tmp vector for join
+
+    YDVector() : data(), aggregate() {}
+
+    void           push_back(const YieldDistance &YD) { data.push_back(YD); }
+    YieldDistance &operator[](int i) { return data[i]; }
+    YieldDistance &operator[](size_t i) { return data[i]; }
+    const YieldDistance &operator[](int i) const { return data[i]; }
+    const YieldDistance &operator[](size_t i) const { return data[i]; }
+
+    size_t size() const { return data.size(); }
+    bool   empty() const { return data.empty(); }
+    void   clear() { data.clear(); }
+
+    friend raw_ostream &operator<<(raw_ostream &OS, const YDVector &YDV) {
+        for (auto &entry : YDV.data) {
+            OS << entry << "\n";
+        }
+        return OS;
+    }
+
+    bool operator==(const YDVector &YDV) const {
+        if (data.size() != YDV.data.size()) {
+            return false;
+        }
+        for (int i = 0; i < data.size(); i++) {
+            if (data[i] != YDV.data[i]) {
+                return false;
+            }
+        }
+        return true;
+    }
+
+    bool operator!=(const YDVector &YDV) const {
+        if (data.size() != YDV.data.size()) {
+            return true;
+        }
+        for (int i = 0; i < data.size(); i++) {
+            if (data[i] != YDV.data[i]) {
+                return true;
+            }
+        }
+        return false;
+    }
+
+    // iterator operations
+    typedef std::vector<YieldDistance>::iterator       iterator;
+    typedef std::vector<YieldDistance>::const_iterator const_iterator;
+    iterator       begin() { return data.begin(); }
+    iterator       end() { return data.end(); }
+    const_iterator begin() const { return data.begin(); }
+    const_iterator end() const { return data.end(); }
+};
+
+class YDVectorPrinter {
+  public:
+    void print(raw_ostream &OS, const BitVector &State) const {}
+    explicit YDVectorPrinter(const BinaryContext &BC) : BC(BC) {}
+
+  private:
+    const BinaryContext &BC;
+};
+
+class YieldDistanceAnalysis
+    : public DataflowAnalysis<YieldDistanceAnalysis, YDVector, false,
+                              YDVectorPrinter> {
+    using Parent = DataflowAnalysis<YieldDistanceAnalysis, YDVector, false,
+                                    YDVectorPrinter>;
+    friend class DataflowAnalysis<YieldDistanceAnalysis, YDVector, false,
+                                  YDVectorPrinter>;
+    BinaryFunction &BF;
+    int IPC;
+  public:
+    YieldDistanceAnalysis(BinaryFunction &BF, uint64_t CycleTh, int IPC = 1,
+                          MCPlusBuilder::AllocatorIdTy AllocId = 0)
+        : BF(BF), Parent(BF, AllocId), CycleTh(CycleTh), IPC(IPC) {}
+    ~YieldDistanceAnalysis(){};
+
+    void run() { Parent::run(); }
+    bool isYieldPoint(const MCInst &Point) const {
+        if (YieldPointMap.find(&Point) != YieldPointMap.end())
+            if (YieldPointMap.at(&Point) == 1)
+                return true;
+        return false;
+    }
+
+    // ProfileData
+    std::map<const BinaryBasicBlock *, uint64_t>          LatencyProfile;
+    std::map<const BinaryBasicBlock *, PredecessorWeight> PredecessorProfile;
+
+  protected:
+    std::map<const BinaryBasicBlock *, uint64_t> VisitCount;
+    std::map<const MCInst *, uint64_t>           YieldPointMap;
+    uint64_t                                     CycleTh;
+
+    void preflight() {
+        std::vector<LatProfEntry>  Lv = BC.LatProfList;
+        std::vector<PredProfEntry> Pv = BC.PredProfList;
+
+        if (Lv.empty() || Pv.empty()) {
+            return;
+        }
+
+        std::map<uint64_t, LatProfEntry *> LatencyProfileTmpMap;
+        for (auto &entry : Lv) {
+            if (LatencyProfileTmpMap.find(entry.begin) ==
+                LatencyProfileTmpMap.end()) {
+                LatencyProfileTmpMap[entry.begin] = &entry;
+            } else if (LatencyProfileTmpMap[entry.begin]->end > entry.end) {
+                LatencyProfileTmpMap[entry.begin] = &entry;
+            }
+        }
+
+        for (auto &BB : Func) {
+            MCInst &FirstInst = *(BB.begin());
+            MCInst &LastInst  = *(BB.rbegin());
+            // check if annotation exists
+            if (!BC.MIB->hasAnnotation(FirstInst, "AbsoluteInstrAddr") ||
+                !BC.MIB->hasAnnotation(LastInst, "AbsoluteInstrAddr")) {
+                LatencyProfile[&BB] = 0;
+                continue;
+            }
+            uint64_t begin_addr = BC.MIB->getAnnotationAs<uint64_t>(
+                FirstInst, "AbsoluteInstrAddr");
+            uint64_t end_addr = BC.MIB->getAnnotationAs<uint64_t>(
+                LastInst, "AbsoluteInstrAddr");
+            if (LatencyProfileTmpMap.find(begin_addr) !=
+                LatencyProfileTmpMap.end()) {
+                if (LatencyProfileTmpMap[begin_addr]->end == end_addr)
+                    LatencyProfile[&BB] = LatencyProfileTmpMap[begin_addr]->lat;
+                else
+                    LatencyProfile[&BB] = BB.size() / IPC;
+            } else {
+                LatencyProfile[&BB] = BB.size() / IPC;
+            }
+        }
+
+        std::map<uint64_t, std::vector<PredProfEntry *>> PredProfileTmpMap;
+        std::map<uint64_t, const BinaryBasicBlock *>     BBEndAddrMap;
+        for (auto &entry : Pv) {
+            if (PredProfileTmpMap.find(entry.target) == PredProfileTmpMap.end())
+                PredProfileTmpMap[entry.target] =
+                    std::vector<PredProfEntry *>();
+            PredProfileTmpMap[entry.target].push_back(&entry);
+        }
+        for (auto &BB : Func) {
+            MCInst &LastInst = *(BB.rbegin());
+            if (!BC.MIB->hasAnnotation(LastInst, "AbsoluteInstrAddr")) {
+                continue;
+            }
+
+            uint64_t end_addr = BC.MIB->getAnnotationAs<uint64_t>(
+                LastInst, "AbsoluteInstrAddr");
+            BBEndAddrMap[end_addr] = &BB;
+        }
+        for (auto &BB : Func) {
+            MCInst &FirstInst = *(BB.begin());
+            if (!BC.MIB->hasAnnotation(FirstInst, "AbsoluteInstrAddr")) {
+                continue;
+            }
+
+            uint64_t begin_addr = BC.MIB->getAnnotationAs<uint64_t>(
+                FirstInst, "AbsoluteInstrAddr");
+
+            if (PredProfileTmpMap.find(begin_addr) != PredProfileTmpMap.end()) {
+                PredecessorWeight PW;
+                for (auto &entry : PredProfileTmpMap[begin_addr]) {
+                    uint64_t pred_addr = entry->src;
+                    if (BBEndAddrMap.find(pred_addr) != BBEndAddrMap.end()) {
+                        PW[BBEndAddrMap[pred_addr]] = entry->counts;
+                    }
+                }
+                PredecessorProfile[&BB] = PW;
+            }
+        }
+
+        // print LatencyProfile and PredecessorProfile
+        // errs() << "LatencyProfile:\n";
+        // for (auto &entry : LatencyProfile) {
+        //    errs() << entry.first->getName() << " " << entry.first << " " <<
+        //    entry.second << "\n";
+        //    // print successors
+        //    errs() << "    Successors: ";
+        //    for (auto &succ : entry.first->successors()) {
+        //        errs() << succ->getName() << " ";
+        //    }
+        //    errs() << "\n";
+        //}
+        // errs() << "PredecessorProfile:\n";
+        // for (auto &entry : PredecessorProfile) {
+        //    errs() << entry.first->getName() << " ";
+        //    for (auto &entry2 : entry.second) {
+        //        errs() << entry2.first->getName() << " " << entry2.second << "
+        //        ";
+        //    }
+        //    errs() << "\n";
+        //}
+    }
+
+    YDVector getStartingStateAtBB(const BinaryBasicBlock &BB) {
+        const MCInst        *FirstInst = &(*(BB.begin()));
+        struct YieldDistance YD;
+        YDVector             YDV;
+
+        YD.YieldPoint = FirstInst;
+        YD.Distance   = 0;
+        YD.CurBB      = &BB;
+        YDV.push_back(YD);
+        return YDV;
+    }
+
+    YDVector getStartingStateAtPoint(const MCInst &Point) { return YDVector(); }
+
+    // Called when a BB is selected for the target of analysis
+    // StateOut = State of Current BB generated from last visit (if it is first
+    // visit, initial state) StateIn = State of the Last Instruction of a
+    // Predecessor BB
+    void doConfluence(YDVector &StateOut, const YDVector &StateIn) {
+        const BinaryBasicBlock *OutBB =
+            (const BinaryBasicBlock *) StateOut[0].CurBB;
+
+        if (VisitCount.find(OutBB) != VisitCount.end()) {
+            return;   // no change to StateOut if already visited
+        }
+
+        assert(!StateOut.empty() && "StateOut should not be empty");
+        if (StateIn.empty()) {
+            return;   // no change to StateOut
+        }
+
+        // need profile data
+        for (auto &entry : StateIn)
+            StateOut.aggregate[entry.YieldPoint].push_back(entry);
+    }
+    void postDoConfluence(YDVector &StateOut) {
+        const BinaryBasicBlock *BB = StateOut[0].CurBB;
+        // three cases
+        // 1) all predecessors holding a YP have PRED_PROF --> weighted sum
+        // 2) some holding a YP have PRED_PROF --> ignore the ones without
+        // PRED_PROF, and then weighted sum 3) none holding a YP have
+        // PRED_PROF--> 1/n * distance for each
+        for (auto &AggEntry : StateOut.aggregate) {
+            // check if any predecessor has PRED_PROF
+            bool has_pred_prof = false;
+            auto AggVector     = AggEntry.second;
+
+            // errs() << "    postDoConfluece: " << AggEntry.first << " ";
+            // for (auto &entry : AggVector) {
+            //     errs() << entry.Distance << " ";
+            // }
+            // errs() << "\n";
+
+            for (auto &entry : AggVector) {
+                if (PredecessorProfile[BB].find(entry.CurBB) !=
+                    PredecessorProfile[BB].end()) {
+                    has_pred_prof = true;
+                    break;
+                }
+            }
+            if (!has_pred_prof) {
+                // case 3
+                // errs() << "Case 3\n";
+                YieldDistance YD;
+                YD.CurBB    = BB;
+                YD.Distance = 0;
+                for (auto &entry : AggVector) {
+                    YD.YieldPoint = entry.YieldPoint;
+                    YD.Distance +=
+                        entry.Distance * float(1.0 / AggVector.size());
+                }
+                StateOut.push_back(YD);
+                continue;
+            }
+
+            // case 1 and 2
+            // Zeroing entries without PRED_PROF
+            // errs() << "Case 1 2\n";
+            for (auto &entry : AggVector) {
+                if (PredecessorProfile[BB].find(entry.CurBB) ==
+                    PredecessorProfile[BB].end()) {
+                    entry.Distance = 0;
+                }
+            }
+
+            // caculate the sum of weight first
+            float sum_weight = 0;
+            for (auto &entry : AggVector) {
+                if (PredecessorProfile[BB].find(entry.CurBB) !=
+                    PredecessorProfile[BB].end()) {
+                    sum_weight += PredecessorProfile[BB][entry.CurBB];
+                }
+            }
+            // weighted sum
+            YieldDistance YD;
+            YD.CurBB    = BB;
+            YD.Distance = 0;
+            for (auto &entry : AggVector) {
+                if (PredecessorProfile[BB].find(entry.CurBB) !=
+                    PredecessorProfile[BB].end()) {
+                    YD.YieldPoint = entry.YieldPoint;
+                    YD.Distance += entry.Distance *
+                                   (float(PredecessorProfile[BB][entry.CurBB]) /
+                                    sum_weight);
+                }
+            }
+            StateOut.push_back(YD);
+        }
+        StateOut.aggregate.clear();
+    }
+
+    // transfer function
+    YDVector computeNext(const MCInst &Point, const YDVector &Cur) {
+        const BinaryBasicBlock *BB =
+            (const BinaryBasicBlock *) Cur[0]
+                .CurBB;   // Cur is not empty in any case
+
+        if (VisitCount.find(BB) == VisitCount.end()) {
+            VisitCount[BB] = 1;
+            // print status
+            // if (BB) {
+            //    errs() << "InputState: " << "\n";
+            // errs() << "BB: " << BB->getName() << "\n";
+            //    for (auto &entry : Cur) {
+            //        errs() << "   "  << entry << "\n";
+            //    }
+            //}
+        } else {
+            return Cur;   // no change to StateOut if already visited
+        }
+
+        YDVector Next;
+        bool     above_th = false;
+
+        for (auto &entry : Cur) {
+            struct YieldDistance YD;
+            YD.YieldPoint = entry.YieldPoint;
+            if (LatencyProfile.find(BB) == LatencyProfile.end()) {
+                YD.Distance = entry.Distance + LatencyProfile[BB];
+            } else {
+                YD.Distance = entry.Distance + BB->size() / IPC;
+            }
+            YD.CurBB = BB;
+            if (YD.Distance > CycleTh) {
+                above_th = true;
+            }
+
+            Next.push_back(YD);
+        }
+
+        if (!above_th) {
+            return Next;
+        }
+
+        errs() << "Above threshold happend!\n";
+
+        Next.clear();
+
+        // select yield point within BB..
+        // from the end of the BB, find the non-branch instruction
+
+        // const MCInst &LastPoint = *(BB->rbegin());
+        // YieldPointMap[&LastPoint] = 1;
+        const MCInst &FirstPoint = *(BB->begin());
+        // check if BB is in loop
+        auto &BLI = BF.getLoopInfo();
+        BinaryLoop *Loop = BLI.getLoopFor(BB);
+        if (Loop) {
+            // set 0 if loop depth >=2
+            if (Loop->getLoopDepth() >= 2) {
+                YieldPointMap[&FirstPoint] = 0;
+            } else {
+                YieldPointMap[&FirstPoint] = 1;
+            }
+        } else {
+            YieldPointMap[&FirstPoint] = 1;
+        }
+
+        YieldDistance YD;
+        YD.YieldPoint = &FirstPoint;
+        if (LatencyProfile.find(BB) == LatencyProfile.end()) {
+            YD.Distance = LatencyProfile[BB];
+        } else {
+            YD.Distance = BB->size() / IPC;
+        }
+        YD.CurBB = BB;
+        Next.push_back(YD);
+
+        // if (BB) {
+        //         errs() << "InputState after th: " << "\n";
+        // errs() << "BB: " << BB->getName() << "\n";
+        //        for (auto &entry : Next) {
+        //            errs() << "   "  << entry << "\n";
+        //        }
+        //}
+
+        return Next;
+    }
+
+    StringRef getAnnotationName() const {
+        return StringRef("YieldDistanceAnalysis");
+    }
+};
+
+}   // end namespace bolt
+}   // end namespace llvm
+
+#endif
diff --git a/bolt/include/bolt/Rewrite/RewriteInstance.h b/bolt/include/bolt/Rewrite/RewriteInstance.h
index 6097932d9e6f..4a7f2c8be507 100644
--- a/bolt/include/bolt/Rewrite/RewriteInstance.h
+++ b/bolt/include/bolt/Rewrite/RewriteInstance.h
@@ -23,6 +23,8 @@
 #include <map>
 #include <set>
 #include <unordered_map>
+#include <vector>
+#include <cstdint>
 
 namespace llvm {
 
@@ -55,6 +57,9 @@ public:
   /// Assign profile from \p Filename to this instance.
   Error setProfile(StringRef Filename);
 
+  /// Assign PC list of deliquent loads
+  Error setCMPC(StringRef Filename);
+  Error setScavProf(StringRef, StringRef);
   /// Run all the necessary steps to read, optimize and rewrite the binary.
   Error run();
 
diff --git a/bolt/lib/Core/BinaryContext.cpp b/bolt/lib/Core/BinaryContext.cpp
index 6a9e35e04eec..9cb1f7f3e32d 100644
--- a/bolt/lib/Core/BinaryContext.cpp
+++ b/bolt/lib/Core/BinaryContext.cpp
@@ -34,13 +34,19 @@
 #include "llvm/MC/MCSubtargetInfo.h"
 #include "llvm/MC/MCSymbol.h"
 #include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Errc.h"
 #include "llvm/Support/Error.h"
+#include "llvm/Support/FileSystem.h"
 #include "llvm/Support/Regex.h"
+
 #include <algorithm>
 #include <functional>
 #include <iterator>
 #include <numeric>
 #include <unordered_set>
+#include <fstream>
+#include <sstream>
+#include <system_error>
 
 using namespace llvm;
 
@@ -72,6 +78,8 @@ PrintMemData("print-mem-data",
   cl::ZeroOrMore,
   cl::cat(BoltCategory));
 
+extern cl::opt<int> ProbTh;
+extern cl::opt<int> RelOhTh;
 } // namespace opts
 
 namespace llvm {
@@ -489,6 +497,116 @@ MemoryContentsType BinaryContext::analyzeMemoryAt(uint64_t Address,
   return MemoryContentsType::UNKNOWN;
 }
 
+Error BinaryContext::setCMPC(StringRef Filename) {
+  if (!sys::fs::exists(Filename))
+    return errorCodeToError(make_error_code(errc::no_such_file_or_directory));
+
+  CMPCList = std::make_unique<std::vector<CMPCEntry>>();
+
+  std::ifstream infile(Filename.str());
+  std::string str;
+  while (std::getline(infile, str)) {
+    std::stringstream ss(str);
+    CMPCEntry entry;
+    std::string symbol_name; // dummy;
+    ss >> symbol_name >> entry.addr >> entry.l2m_prob >> entry.l3m_prob;
+    if (!(ss >> entry.rel_oh_th))
+        entry.rel_oh_th = 100;
+
+    // print
+    outs() << "BOLT-INFO: CMPC entry: " << symbol_name << " " << entry.addr << " " << entry.l2m_prob << " " << entry.l3m_prob << " " << entry.rel_oh_th << "\n";
+
+    CMPCList->push_back(entry);
+  }
+
+  if (opts::Verbosity >= 1) {
+    outs() << "BOLT-INFO: CMPC list: ";
+    for (auto entry : *CMPCList) {
+      outs() << entry.addr << " " << entry.l2m_prob << " " << entry.l3m_prob << "\n";
+    }
+    outs() << "\n";
+  }
+
+  return Error::success();
+}
+
+
+Error BinaryContext::setScavProf(StringRef LatProf, StringRef PredProf) {
+  if (!sys::fs::exists(LatProf))
+    return errorCodeToError(make_error_code(errc::no_such_file_or_directory));
+  
+  if (!sys::fs::exists(PredProf))
+    return errorCodeToError(make_error_code(errc::no_such_file_or_directory));
+  
+  // Latency Prof
+  std::ifstream LatInfile(LatProf.str());
+  std::string str;
+  while (std::getline(LatInfile, str)) {
+    std::stringstream ss(str);
+    LatProfEntry entry;
+    std::string dummy1, dummy2;
+    ss >> dummy1 >> dummy2 >> entry.begin >> entry.end >> entry.lat;
+    outs() << "BOLT-INFO: Latency Prof: " << entry.begin << " " << entry.end << " " << entry.lat << "\n";
+    LatProfList.push_back(entry);
+  }
+
+  std::ifstream PredInfile(PredProf.str());
+  while (std::getline(PredInfile, str)) {
+      std::stringstream ss(str);
+      PredProfEntry entry;
+      std::string dummy1, dummy2, dummy3;
+      ss >> dummy1 >> dummy2 >> entry.target >> dummy3 >> entry.src >> entry.counts;
+      outs() << "BOLT-INFO: Pred Prof: " << entry.target << " " << entry.src << " " << entry.counts << "\n";
+      PredProfList.push_back(entry);
+  }
+}
+
+bool BinaryContext::checkCMPCList(uint64_t Address) const {
+  if (!CMPCList)
+    return false;
+
+  // hard-coded for LO test
+  //if (Address == 240279)
+  //  return true;
+  //else 
+  //  return false;
+  //if (Address == 240426)
+  //  return true;
+  //else
+  //  return false;
+  //if (Address == 165767)
+  //  return false;
+  //if (Address == 209444)
+  //  return false;
+
+  for (auto entry : *CMPCList) {
+    if (entry.addr != Address)
+        continue;
+
+    // threshold
+
+    outs() << "BOLT-INFO: CMPC address: " << Address << "\n";
+    outs() << "BOLT-INFO: CMPC threshold: " << opts::ProbTh << "\n";
+    outs() << "BOLT-INFO: CMPC l2 prob: " << entry.l2m_prob << "\n";
+    outs() << "BOLT-INFO: CMPC l3 prob: " << entry.l3m_prob << "\n";
+    outs() << "BOLT-INFO: CMPC rel oh th: " << entry.rel_oh_th << "\n";
+    if (opts::RelOhTh != 0) {
+        if (entry.rel_oh_th <= opts::RelOhTh)
+            return true;
+        else
+            return false;
+    }
+
+    if (opts::ProbTh < entry.l2m_prob) 
+        return true;
+    if (opts::ProbTh*0.4 < entry.l3m_prob)
+        return true;
+  }
+
+  return false;
+}
+
+
 bool BinaryContext::analyzeJumpTable(const uint64_t Address,
                                      const JumpTable::JumpTableType Type,
                                      const BinaryFunction &BF,
diff --git a/bolt/lib/Core/BinaryFunction.cpp b/bolt/lib/Core/BinaryFunction.cpp
index 89ba3627cdd7..87c6068db10f 100644
--- a/bolt/lib/Core/BinaryFunction.cpp
+++ b/bolt/lib/Core/BinaryFunction.cpp
@@ -61,6 +61,7 @@ extern cl::opt<bool> Instrument;
 extern cl::opt<bool> StrictMode;
 extern cl::opt<bool> UpdateDebugSections;
 extern cl::opt<unsigned> Verbosity;
+extern cl::opt<bool> InstScavenger;
 
 extern bool processAllFunctions();
 
@@ -1109,8 +1110,10 @@ void BinaryFunction::handleIndirectBranch(MCInst &Instruction, uint64_t Size,
   }
   case IndirectBranchType::POSSIBLE_JUMP_TABLE:
   case IndirectBranchType::POSSIBLE_PIC_JUMP_TABLE:
-    if (opts::JumpTables == JTS_NONE)
+    if (opts::JumpTables == JTS_NONE) {
       IsSimple = false;
+      errs() << "BOLT-WARNING: JTS_NONE makes IsSimple = false\n";
+    }
     break;
   case IndirectBranchType::POSSIBLE_FIXED_BRANCH: {
     if (containsAddress(IndirectTarget)) {
@@ -1167,6 +1170,9 @@ bool BinaryFunction::disassemble() {
   auto &Ctx = BC.Ctx;
   auto &MIB = BC.MIB;
 
+  // [SAM] SymbolicDisAM: Symbolic Disassembler
+  // [SAM] Dissassembler: consumes a memory region and provides a list of assembly instructions
+  // [SAM] This line creates a symbolic disassembler for the current binary (considering architecture)
   BC.SymbolicDisAsm->setSymbolizer(MIB->createTargetSymbolizer(*this));
 
   // Insert a label at the beginning of the function. This will be our first
@@ -1176,6 +1182,8 @@ bool BinaryFunction::disassemble() {
   uint64_t Size = 0; // instruction size
   for (uint64_t Offset = 0; Offset < getSize(); Offset += Size) {
     MCInst Instruction;
+    // [SAM] getAddress() returns the starting address of this function.
+    // [SAM] This is where BOLT computes the address of each single instruction
     const uint64_t AbsoluteInstrAddr = getAddress() + Offset;
 
     // Check for data inside code and ignore it
@@ -1184,6 +1192,8 @@ bool BinaryFunction::disassemble() {
       continue;
     }
 
+    // [SAM] This function fills Instruction with the disassembled instruction.
+    // [SAM] but it seems like Instruction only has semantic information and not the address info.
     if (!BC.SymbolicDisAsm->getInstruction(Instruction, Size,
                                            FunctionData.slice(Offset),
                                            AbsoluteInstrAddr, nulls())) {
@@ -1363,6 +1373,66 @@ add_instruction:
       MIB->addAnnotation(Instruction, "Size", static_cast<uint32_t>(Size));
     }
 
+    // [SAM] mark cache miss PCs using annotation (MCPlus format)
+    if (opts::InstScavenger) {
+        MIB->addAnnotation(Instruction, "AbsoluteInstrAddr", AbsoluteInstrAddr);
+    }
+
+    if (BC.checkCMPCList(AbsoluteInstrAddr)) {
+      Instruction.OriginalAddress = AbsoluteInstrAddr; 
+      MIB->addAnnotation(Instruction, "InsertPrefetchAndYield", 1);// + std::to_string(AbsoluteInstrAddr), 1);
+    }
+
+    // manually set yield point for scavenger
+    if (getPrintName() == "_Z7computev") {
+        if (AbsoluteInstrAddr == 9902) {
+            MIB->addAnnotation(Instruction, "InsertYield", 1);
+        }
+    }
+
+    //scan scav    
+    if (getPrintName() == "_Z4scanv") {
+        if (AbsoluteInstrAddr == 10010) {
+            MIB->addAnnotation(Instruction, "InsertYield", 1);
+        }
+    }
+
+    if (getPrintName() == "_Z13do_turnaroundPv") {
+        if (AbsoluteInstrAddr == 4697) {
+            MIB->addAnnotation(Instruction, "InsertYield", 1);
+        }
+    }
+
+    //ptrchase scav
+    if (getPrintName() == "_Z8ptrchasePv") {
+        if (AbsoluteInstrAddr == 5642) {
+            MIB->addAnnotation(Instruction, "InsertPrefetchAndYield", 1); // need prefetch in this case
+        }
+    }
+
+    // ptrchase primary
+    if (getPrintName() == "_Z11ptrchase_tqPv") {
+        if (AbsoluteInstrAddr == 5663) {
+            errs() << "Priamry ptrchase match\n";
+            MIB->addAnnotation(Instruction, "InsertPrefetchAndYield", 1);
+        }
+    }
+
+
+
+
+    if (getPrintName() == "_Z10do_nothingPv") {
+        if (AbsoluteInstrAddr == 4505) {
+            MIB->addAnnotation(Instruction, "InsertYield", 1);
+        }
+    }
+    
+    //prin binary function name
+    //outs() << "BOLT-INFO: Function name: " << getPrintName() << "\n";
+
+    // [SAM] This function stores (Offset in a fuction, MCInst). This information will be used in CFG construction.
     addInstruction(Offset, std::move(Instruction));
   }
 
@@ -1376,6 +1446,7 @@ add_instruction:
 
   if (!IsSimple) {
     clearList(Instructions);
+    errs() << "BOLT-WARNING: Not disassembled because it is not simple\n";
     return false;
   }
 
@@ -2174,6 +2245,14 @@ bool BinaryFunction::buildCFG(MCPlusBuilder::AllocatorIdTy AllocatorId) {
 
   normalizeCFIState();
 
+
+  // [SAM] IIUC, BasicBlock class now holds all the instructions, however, they decoupled address from each instruction.
+  // [SAM] the only address info they have "InputAddressRange" which is a pair of starting and end address of basic block in original code.
+  // [SAM] Do we need to mark each instruction if it's the target for prefetching-yield? 
+  // [SAM] Idea 1: Add a field to Instruction class and keep it in BasicBlock
+
   // Clean-up memory taken by intermediate structures.
   //
   // NB: don't clear Labels list as we may need them if we mark the function
diff --git a/bolt/lib/Passes/CMakeLists.txt b/bolt/lib/Passes/CMakeLists.txt
index 9bc0779d7df2..cb35a1a92c24 100644
--- a/bolt/lib/Passes/CMakeLists.txt
+++ b/bolt/lib/Passes/CMakeLists.txt
@@ -9,6 +9,7 @@ add_llvm_library(LLVMBOLTPasses
   CacheMetrics.cpp
   CallGraph.cpp
   CallGraphWalker.cpp
+  CheckFloatRet.cpp
   DataflowAnalysis.cpp
   DataflowInfoManager.cpp
   FrameAnalysis.cpp
@@ -29,6 +30,15 @@ add_llvm_library(LLVMBOLTPasses
   PatchEntries.cpp
   PettisAndHansen.cpp
   PLTCall.cpp
+  PnYFilterUnusedRegs.cpp
+  PnYInstrument.cpp
+  PnYLoopOptimization.cpp
+  PnYPass.cpp
+  PnYPseudoInline.cpp
+  PnYPrefetchCrtCtx.cpp
+  PnYBoundYieldDistance.cpp
+  PnYScan.cpp
+  YDPass.cpp 
   RegAnalysis.cpp
   RegReAssign.cpp
   ReorderAlgorithm.cpp
diff --git a/bolt/lib/Passes/CheckFloatRet.cpp b/bolt/lib/Passes/CheckFloatRet.cpp
new file mode 100644
index 000000000000..74df27a87d7c
--- /dev/null
+++ b/bolt/lib/Passes/CheckFloatRet.cpp
@@ -0,0 +1,76 @@
+//===----------------------------------------------------------------------===//
+//
+// This file implements the PatchEntries class that is used for patching
+// the original function entry points.
+//
+//===----------------------------------------------------------------------===//
+#include "bolt/Core/ParallelUtilities.h"
+#include "bolt/Passes/BinaryFunctionCallGraph.h"
+#include "bolt/Passes/BinaryPasses.h"
+#include "bolt/Passes/DataflowInfoManager.h"
+#include "llvm/Support/CommandLine.h"
+
+using namespace llvm;
+
+namespace opts {
+extern cl::opt<unsigned>  Verbosity;
+extern cl::OptionCategory BoltOptCategory;
+
+cl::opt<bool> CheckFloatRet(
+    "check-float-ret",
+    cl::desc("Check if the return value of a function is a float/double"),
+    cl::cat(BoltOptCategory));
+}   // namespace opts
+
+namespace llvm {
+namespace bolt {
+
+void
+CheckFloatRet::runOnFunction(BinaryFunction &BF) {
+    const BinaryContext &BC = BF.getBinaryContext();
+
+    for (BinaryBasicBlock &BB : BF) {
+        bool prevInstIsRet = false;
+        bool decided       = false;
+        for (auto iter = BB.rbegin(); iter != BB.rend(); ++iter) {
+            // check if the current instruction is a return
+            MCInst Inst = *iter;
+            if (prevInstIsRet) {
+                if (BC.MIB->isRetInteger(Inst)) {
+                    decided = true;
+                    outs() << "Function " << BF << " returns an integer\n";
+                    break;
+                } else if (BC.MIB->isRetFloat(Inst)) {
+                    decided = true;
+                    outs() << "Function " << BF << " returns a float\n";
+                    BF.setRetFloat();
+                    break;
+                }
+            }
+            if (BC.MIB->isReturn(Inst)) {
+                prevInstIsRet = true;
+            }
+        }
+        if (decided) {
+            break;
+        }
+    }
+}
+
+void
+CheckFloatRet::runOnFunctions(BinaryContext &BC) {
+    ParallelUtilities::WorkFuncTy WorkFun = [&](BinaryFunction &BF) {
+        runOnFunction(BF);
+    };
+
+    ParallelUtilities::PredicateTy SkipFunc = [&](const BinaryFunction &BF) {
+        return false;
+    };
+
+    ParallelUtilities::runOnEachFunction(
+        BC, ParallelUtilities::SchedulingPolicy::SP_INST_LINEAR, WorkFun,
+        SkipFunc, "CheckFloatRet", true);
+}
+
+}   // end namespace bolt
+}   // end namespace llvm
\ No newline at end of file
diff --git a/bolt/lib/Passes/Inliner.cpp b/bolt/lib/Passes/Inliner.cpp
index 8dcb8934f2d2..65cf4dcbdc47 100644
--- a/bolt/lib/Passes/Inliner.cpp
+++ b/bolt/lib/Passes/Inliner.cpp
@@ -227,8 +227,10 @@ void Inliner::findInliningCandidates(BinaryContext &BC) {
     if (!shouldOptimize(Function))
       continue;
     const InliningInfo InlInfo = getInliningInfo(Function);
-    if (InlInfo.Type != INL_NONE)
+    if (InlInfo.Type != INL_NONE) {
       InliningCandidates[&Function] = InlInfo;
+      errs() << "BOLT-INFO: inlining candidate " << Function << '\n';
+    }
   }
 }
 
@@ -407,6 +409,7 @@ bool Inliner::inlineCallsInFunction(BinaryFunction &Function) {
   for (BinaryBasicBlock *BB : Blocks) {
     for (auto InstIt = BB->begin(); InstIt != BB->end();) {
       MCInst &Inst = *InstIt;
+
       if (!BC.MIB->isCall(Inst) || MCPlus::getNumPrimeOperands(Inst) != 1 ||
           !Inst.getOperand(0).isExpr()) {
         ++InstIt;
@@ -420,11 +423,13 @@ bool Inliner::inlineCallsInFunction(BinaryFunction &Function) {
       uint64_t EntryID = 0;
       BinaryFunction *TargetFunction =
           BC.getFunctionForSymbol(TargetSymbol, &EntryID);
+
       if (!TargetFunction || EntryID != 0) {
         ++InstIt;
         continue;
       }
 
+
       // Don't do recursive inlining.
       if (TargetFunction == &Function) {
         ++InstIt;
@@ -443,6 +448,7 @@ bool Inliner::inlineCallsInFunction(BinaryFunction &Function) {
         continue;
       }
 
+
       int64_t SizeAfterInlining;
       if (IsTailCall)
         SizeAfterInlining =
@@ -527,7 +533,6 @@ void Inliner::runOnFunctions(BinaryContext &BC) {
     for (BinaryFunction *Function : ConsideredFunctions) {
       if (opts::InlineLimit && NumInlinedCallSites >= opts::InlineLimit)
         break;
-
       const bool DidInline = inlineCallsInFunction(*Function);
 
       if (DidInline)
diff --git a/bolt/lib/Passes/PatchEntries.cpp b/bolt/lib/Passes/PatchEntries.cpp
index 02a044d8b2f6..3dacb1fd3b6a 100644
--- a/bolt/lib/Passes/PatchEntries.cpp
+++ b/bolt/lib/Passes/PatchEntries.cpp
@@ -109,6 +109,8 @@ void PatchEntries::runOnFunctions(BinaryContext &BC) {
     for (Patch &Patch : PendingPatches) {
       BinaryFunction *PatchFunction = BC.createInjectedBinaryFunction(
           NameResolver::append(Patch.Symbol->getName(), ".org.0"));
+      // print functioname to stderr
+      errs() << "PatchEntries: " << PatchFunction->getPrintName() << "\n";
       // Force the function to be emitted at the given address.
       PatchFunction->setOutputAddress(Patch.Address);
       PatchFunction->setFileOffset(Patch.FileOffset);
diff --git a/bolt/lib/Passes/PnYBoundYieldDistance.cpp b/bolt/lib/Passes/PnYBoundYieldDistance.cpp
new file mode 100644
index 000000000000..96f24f5830e3
--- /dev/null
+++ b/bolt/lib/Passes/PnYBoundYieldDistance.cpp
@@ -0,0 +1,835 @@
+#include <stack>
+
+#include "bolt/Passes/PrefetchAndYield.h"
+
+namespace opts {
+
+extern cl::OptionCategory BoltOptCategory;
+
+cl::opt<bool>
+ScavDisableUseInductionReg("scav-disable-induction-reg",
+  cl::desc("---"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltOptCategory));
+
+cl::opt<bool>
+ScavDisableUseUnusedReg("scav-disable-unused-reg",
+  cl::desc("---"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltOptCategory));
+
+cl::opt<bool>
+ScavDisableYieldInLoops("scav-disable-yield-in-loops",
+  cl::desc("---"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltOptCategory));
+
+cl::opt<bool>
+ScavDisableMemoryLoops("scav-disable-memory-loops",
+  cl::desc("---"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltOptCategory));
+cl::opt<int>
+ScavIpc("scav-ipc",
+  cl::desc("---"),
+  cl::Optional,
+  cl::init(1),
+  cl::cat(BoltOptCategory));
+
+}
+
+namespace llvm {
+namespace bolt {
+
+uint64_t loop_set_counter_idx     = 0;
+uint64_t max_loop_set_counter_idx = 4 * 64 / 2;
+
+BoundYieldDistanceScanner::BoundYieldDistanceScanner(BinaryFunction &BF, RegAnalysis *RA)
+    : PnYSubScanner(BF), BC(BF.getBinaryContext()), RA(RA) {
+    if (BF.empty())
+        return;
+
+    if (!BF.hasLoopInfo()) {
+        BF.calculateLoopInfo();
+    }
+
+
+    errs() << "BoundYieldDistanceScanner: scanning " << BF.getPrintName()
+           << "...\n";
+
+    assert(opts::BoundYieldDistance > 0);
+    MAX_YIELD_DISTANCE = opts::BoundYieldDistance;
+    errs() << "BoundYieldDistanceScanner: MAX_YIELD_DISTANCE = "
+           << MAX_YIELD_DISTANCE << "\n";
+
+    YDA.reset(new YieldDistanceAnalysis(BF, MAX_YIELD_DISTANCE, opts::ScavIpc));
+    YDA->run();
+
+    // populate YieldCntMap
+    for (auto &BB : BF) {
+        for (auto &Inst : BB) {
+            if (YDA->isYieldPoint(Inst)) {
+                if (YieldCntMap.find(&BB) == YieldCntMap.end()) {
+                    YieldCntMap[&BB] = 1;
+                } else {
+                    YieldCntMap[&BB]++;
+                }
+            }
+        }
+    }
+
+    loopAnalysis();
+}
+
+static std::vector<std::vector<BinaryBasicBlock *>>
+findAllPathes(BinaryLoop *Loop, BinaryFunction &BF,
+              const std::map<const BinaryBasicBlock *, int> &YieldCntMap) {
+    // find all pathes from header to latches using DFS
+    // two stop condition: reach latch, reach an instrumented block (loop is
+    // considered as instrumented) latch --> store current path loop --> stop
+    // search this path
+
+    BinaryBasicBlock                            *Header = Loop->getHeader();
+    auto                                        &BLI    = BF.getLoopInfo();
+    std::vector<std::vector<BinaryBasicBlock *>> Out;
+    std::map<BinaryBasicBlock *, int>            Visited;
+    std::stack<std::vector<BinaryBasicBlock *>>  Stack;
+
+    Stack.push(
+        std::vector<BinaryBasicBlock *>({Header}));   // init stack with header
+    while (!Stack.empty()) {
+        std::vector<BinaryBasicBlock *> Path = Stack.top();
+        Stack.pop();
+
+        BinaryBasicBlock *Current = Path.back();
+        if (Loop->isLoopLatch(Current)) {
+            Out.push_back(Path);
+            continue;
+        }
+
+    if (Visited.find(Current) != Visited.end()) {
+        continue;
+        }
+
+        Visited[Current] = 1;
+        for (auto Succ : Current->successors()) {
+            // ignore non-loop successor
+            if (!BLI.getLoopFor(Succ))
+                continue;
+
+            if (BLI.getLoopFor(Succ) != Loop)   // inner loops!
+                continue;
+
+            if (YieldCntMap.find(Succ) != YieldCntMap.end()) {   // instrumented
+                continue;
+            }
+
+            std::vector<BinaryBasicBlock *> NewPath = Path;
+            NewPath.push_back(Succ);
+            Stack.push(NewPath);
+        }
+    }
+
+    return Out;
+}
+
+static unsigned
+findInductionRegister(BinaryLoop *Loop, BinaryFunction &BF, RegAnalysis *RA, int &Const) {
+    if (opts::ScavDisableUseInductionReg)
+        return 0;
+    
+    BinaryContext &BC = BF.getBinaryContext();
+
+    // check all instructions in the loop 
+    // check two conditions:
+    //      (1) register is ADDED by an constant
+    //      (2) it i << Consan
+    //      (3) the register is used in the cmp or test
+    using CandPair = std::pair<unsigned, MCInst *>;
+    std::vector<CandPair> CandVec;
+    std::vector<CandPair> FinalVec;
+    for (auto BB : Loop->blocks()) {
+        for (auto &Inst : *BB) {
+            unsigned Reg;
+            int Constant;
+            if (BC.MIB->isAddByConstant(Inst, Reg, Constant)) {
+                Reg = BC.MIB->getAliasSized(Reg, 8); 
+                CandVec.emplace_back(std::make_pair(Reg, &Inst));
+                
+                // print instruction
+                errs() << "Induction Candidate: ";
+                BC.InstPrinter->printRegName(errs(), Reg);
+                // print Const
+                errs() << " + " << Constant << "\n";
+                errs() << "\n";
+            }
+        }
+    }
+
+    for (auto Cand : CandVec) {
+        unsigned Reg = Cand.first;
+        MCInst *AddInst = Cand.second;
+        bool isIReg = true;
+        for (auto BB : Loop->blocks()) {
+            for (auto &Inst : *BB) {
+                if (AddInst == &Inst)
+                    continue;
+                BitVector Clobbers = BitVector(BC.MRI->getNumRegs(), false);
+                RA->getInstClobberList(Inst, Clobbers);
+                if (Clobbers.test(Reg)) {
+                    isIReg = false;
+                    errs() << BC.InstPrinter->getOpcodeName(Inst.getOpcode()) << "\n";
+                    break;
+                }
+            }
+            if (!isIReg) break;
+        }
+
+        if (!isIReg) {
+            errs() << "Written in other instructions\n";
+            continue;
+        }
+
+        isIReg = false; // for fast check
+
+        for (auto BB : Loop->blocks()) {
+            for (auto &Inst : *BB) {
+                if (BC.MIB->isCmp(Inst) || BC.MIB->isTest(Inst)) {
+                    // check Reg is used
+                    BitVector UsedRegs = BitVector(BC.MRI->getNumRegs(), false);
+                    RA->getInstUsedRegsList(Inst, UsedRegs, false); // getInstUsed returns all aliases smaller than actually used regs
+                    unsigned SmallestAlias = BC.MIB->getAliasSized(Reg, 1);
+                    if (UsedRegs.test(SmallestAlias)) {
+                        isIReg = true;
+                        break;
+                    }
+                }
+            }
+            if (isIReg)
+                break;
+        }
+
+        if (isIReg)
+            FinalVec.emplace_back(std::make_pair(Reg, AddInst));
+    }
+
+    if (FinalVec.size() == 1) {
+        BC.MIB->isAddByConstant(*(FinalVec[0].second), FinalVec[0].first, Const);
+        errs() << "Induction register found: " << FinalVec[0].first << " " << Const <<"\n";
+        return FinalVec[0].first;
+    } else if (FinalVec.size() > 1) {
+        errs() << "Multiple induction registers found, ambiguous\n";
+        return 0;
+    } else {
+        return 0;
+    }
+}
+
+static uint64_t
+computeAveragePathLatency(
+    std::vector<std::vector<BinaryBasicBlock *>>       &Paths,
+    const std::map<const BinaryBasicBlock *, uint64_t> &LatencyProfile,
+    const std::map<const BinaryBasicBlock *, PredecessorWeight>
+        &PredecessorProfile) {
+    // path latency
+    std::vector<uint64_t> PathLatency;
+    std::vector<float>    PathProb;
+    for (auto Path : Paths) {
+        uint64_t Latency = 0;
+        for (auto BB : Path) {
+            if (LatencyProfile.find(BB) == LatencyProfile.end()) {
+                Latency += BB->size() / opts::ScavIpc;
+            } else {
+                Latency += LatencyProfile.at(BB);
+            }
+        }
+        PathLatency.push_back(Latency);
+
+        float Prob = 1;
+        for (auto BBIter = Path.rbegin(); BBIter != Path.rend(); ++BBIter) {
+            if (BBIter + 1 == Path.rend()) {
+                break;
+            }
+
+            BinaryBasicBlock *BB     = *BBIter;
+            BinaryBasicBlock *PrevBB = *(BBIter + 1);
+
+            bool HasPredProf = false;
+
+            if (PredecessorProfile.find(BB) == PredecessorProfile.end()) {
+                // fall back to dumb average
+                Prob *= (1.0 / BB->pred_size());
+                continue;
+            }
+
+            auto PW = PredecessorProfile.at(BB);
+
+            for (auto Pred : BB->predecessors()) {
+                if (PW.find(Pred) != PW.end()) {
+                    HasPredProf = true;
+                    break;
+                }
+            }
+
+            if (!HasPredProf) {
+                Prob *= (1.0 / BB->pred_size());
+                continue;
+            }
+
+            if (PW.find(PrevBB) == PW.end()) {
+                Prob = 0;
+                continue;
+            }
+
+            float Sum = 0;
+            for (auto Pred : BB->predecessors()) {
+                if (PW.find(Pred) == PW.end()) {
+                    continue;
+                }
+                Sum += PW.at(Pred);
+            }
+            Prob *= (PW.at(PrevBB) / Sum);
+        }
+        PathProb.push_back(Prob);
+    }
+
+    float AvgLatency = 0;
+    for (int i = 0; i < PathLatency.size(); i++) {
+        AvgLatency += PathLatency[i] * PathProb[i];
+    }
+
+    if (AvgLatency == 0) {
+        // fall back to dumb average
+        for (int i = 0; i < PathLatency.size(); i++) {
+            AvgLatency += PathLatency[i] / PathLatency.size();
+        }
+    }
+
+    return (uint64_t) AvgLatency;
+}
+
+void
+BoundYieldDistanceScanner::performLoopAnalysis(BinaryLoop *Loop) {
+    const std::map<const BinaryBasicBlock *, uint64_t> &LatencyProfile =
+        YDA->LatencyProfile;
+    const std::map<const BinaryBasicBlock *, PredecessorWeight>
+        &PredecessorProfile = YDA->PredecessorProfile;
+
+    auto                                        &BLI = BF.getLoopInfo();
+    std::vector<std::vector<BinaryBasicBlock *>> Paths =
+        findAllPathes(Loop, BF, YieldCntMap);
+
+    std::vector<BinaryBasicBlock *> Latches;
+    for (auto BB : Loop->blocks()) {
+        if (Loop->isLoopLatch(BB)) {
+            //errs() << "Latch added " << BB->getName() << "\n";
+            Latches.push_back(BB);
+        }
+    }
+
+    std::map<BinaryBasicBlock *, std::vector<std::vector<BinaryBasicBlock *>>>
+        LatchToPaths;
+    for (auto Path : Paths) {
+        BinaryBasicBlock *Latch = Path.back();
+        LatchToPaths[Latch].push_back(Path);
+    }
+
+    // print LatchToPathes
+    //for (auto LTP : LatchToPaths) {
+    //    errs() << "Latch: " << LTP.first->getName() << "\n";
+    //    for (auto Path : LTP.second) {
+    //        for (auto BB : Path) {
+    //            errs() << BB->getName() << " ";
+    //        }
+    //        errs() << "\n";
+    //    }
+    //}
+
+    for (auto Latch : Latches) {
+        uint64_t AvgLatency = computeAveragePathLatency(
+            LatchToPaths[Latch], LatencyProfile, PredecessorProfile);
+
+        if (AvgLatency == 0)
+            continue;
+
+        int IterTh = MAX_YIELD_DISTANCE / AvgLatency;
+        if (IterTh == 0)
+            IterTh = 1;
+
+        errs() << "Path AvgLatency: " << AvgLatency << "\n";
+        errs() << "Iteration Threshold: " << IterTh << "\n";
+
+        auto LastIter = Latch->getLastNonPseudo();
+        while (BC.MIB->isConditionalBranch(*LastIter) ||
+               BC.MIB->isCmpXchg(*LastIter)) {
+            //errs() << "Moved..\n";
+            LastIter++;
+            if (LastIter == Latch->rend()) {
+                errs() << "Latch reaches end\n";
+                continue;
+            }
+        }
+
+        const MCInst *Target = &(*LastIter);
+        errs() << "TargetLastInst: "
+               << BC.InstPrinter->getOpcodeName(Target->getOpcode()) << "\n";
+
+        // absoluteInstrInst
+        uint64_t Addr = BC.MIB->getAnnotationAs<uint64_t>(*Target, "AbsoluteInstrAddr");
+
+        assert(IterationBoundMap.find(Target) == IterationBoundMap.end());
+        if (IterTh < opts::IterTh) {
+            IterationBoundMap[Target] = IterTh;
+            // print loop depth and IterTh
+            errs() << "IterTh: " << IterTh
+                   << " / Loop depth: " << BLI.getLoopDepth(Latch) << "\n";
+        }
+    }
+
+    // induction variable analysis
+    Loop->InductionReg = findInductionRegister(Loop, BF, RA, Loop->InductionRegInitVal);
+}
+
+
+void
+BoundYieldDistanceScanner::loopAnalysis() {
+    // analyze all loops in BF, and insert "conditional" yield if needed using
+    // one of the unused registers For conditioanl yield, we need an unused
+    // register. --> we will piggyback on the existing register analysis code as
+    // much as possible..
+    // loop sanity check
+    // 1. Find the deepest loop in this function. Suppose its depth is D.
+    std::vector<std::vector<BinaryLoop *>> LoopsByDepth;
+    for (int i = 0; i < 10; i++)
+        LoopsByDepth.push_back(std::vector<BinaryLoop *>());
+
+    int                         max_depth = 0;
+    std::map<BinaryLoop *, int> Visited;
+    for (const auto &BB : BF) {
+        auto &BLI = BF.getLoopInfo();
+        // 1. check if current BB is in loop
+        BinaryLoop *Loop = BLI.getLoopFor(&BB);
+        if (Loop == nullptr)
+            continue;
+
+        if (Visited.find(Loop) != Visited.end())
+            continue;
+        Visited[Loop] = 1;
+
+        int depth = BLI.getLoopDepth(&BB);
+        if (depth > max_depth)
+            max_depth = depth;
+        LoopsByDepth[depth].push_back(Loop);
+    }
+    LoopsByDepth.resize(max_depth + 1);
+
+    for (auto Loops = LoopsByDepth.rbegin(); Loops != LoopsByDepth.rend();
+         ++Loops) {
+        for (auto Loop : *Loops) {
+            performLoopAnalysis(Loop);
+        }
+    }
+}
+
+void
+BoundYieldDistanceScanner::annotateInst(MCInst &Inst, BinaryBasicBlock &BB) {
+    if (IterationBoundMap.find(&Inst) != IterationBoundMap.end()) {
+        if (BC.MIB->hasAnnotation(Inst, "InsertYield") ||
+            BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield"))
+            return;
+
+        BC.MIB->addAnnotation(Inst, "InsertYield", 1);
+        BC.MIB->addAnnotation(Inst, "IterationBound", IterationBoundMap[&Inst]);
+        YdCnt++;
+    } else if (YDA->isYieldPoint(Inst)) {
+        if (BC.MIB->hasAnnotation(Inst, "InsertYield") ||
+            BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield"))
+            return;
+
+        BC.MIB->addAnnotation(Inst, "InsertYield", 1);
+        YdCnt++;
+    }
+}
+
+void
+BoundYieldDistanceScanner::printStat() {
+    errs() << BF.getPrintName() << ":\n";
+    errs() << "    BoundYieldDistanceScanner: " << YdCnt
+           << " yields inserted\n";
+}
+
+static void
+createPreheader(BinaryFunction &BF, BinaryLoop *BL) {
+    // assumption: no preheader
+    BinaryContext &BC = BF.getBinaryContext();
+    // find edges to patch
+    BinaryBasicBlock               *LoopHeader = BL->getHeader();
+    std::vector<BinaryBasicBlock *> LoopPredecessors;
+    for (auto PredIter = LoopHeader->pred_begin();
+         PredIter != LoopHeader->pred_end(); PredIter++) {
+        if (!BL->contains(*PredIter))
+            LoopPredecessors.push_back(*PredIter);
+    }
+
+    assert(!LoopPredecessors.empty());
+
+    std::vector<std::pair<BinaryBasicBlock *, BinaryBasicBlock *>> EdgesToPatch;
+
+    for (auto LP : LoopPredecessors) {
+        for (auto SuccIter = LP->succ_begin(); SuccIter != LP->succ_end();
+             SuccIter++) {
+            if (BL->contains(*SuccIter) && LP->succ_size() != 1)
+                EdgesToPatch.push_back(std::make_pair(LP, *SuccIter));
+        }
+    }
+
+    assert(!EdgesToPatch.empty());
+
+    // do patch : add mid block
+    for (auto edge : EdgesToPatch) {
+        BinaryBasicBlock *LPPredecessor = edge.first;
+        BinaryBasicBlock *LPHeader      = edge.second;
+        auto              LastIter      = LPPredecessor->getLastNonPseudo();
+        bool              HeaderIsBranchTarget =
+            BC.MIB->isBranch(*LastIter) &&
+            BC.MIB->getTargetSymbol(*LastIter) == LPHeader->getLabel();
+
+        if (!HeaderIsBranchTarget) {
+            // check if there is another branch in this basic block that points to preheader
+            while (LastIter != LPPredecessor->rend()) {
+                LastIter++;
+                
+                if (LastIter == LPPredecessor->rend())
+                    break;
+                if (BC.MIB->isCFI(*LastIter) || BC.MIB->isPseudo(*LastIter))
+                    continue;
+                
+                if (BC.MIB->isBranch((*LastIter)) &&
+                    BC.MIB->getTargetSymbol((*LastIter)) == LPHeader->getLabel()) {
+                    HeaderIsBranchTarget = true;
+                    break;
+                }
+            } 
+        }
+
+        // create mid block
+        std::unique_ptr<BinaryBasicBlock> MidBlock = BF.createBasicBlock();
+        MidBlock->setIsPseudoPreheader(true);
+        LPPredecessor->replaceSuccessor(LPHeader, MidBlock.get());
+        MidBlock->addSuccessor(LPHeader);
+
+        InstructionListType Seq;
+        if (HeaderIsBranchTarget) {
+            // patch branch target
+            BC.MIB->replaceBranchTarget(*LastIter, MidBlock->getLabel(),
+                                        BC.Ctx.get());
+            BC.MIB->createShortJmp(Seq, LPHeader->getLabel(), BC.Ctx.get());
+            errs() << "LO header is branch target\n"; 
+        } else {
+            MCInst Noop;
+            BC.MIB->createNoop(Noop);
+            Seq.push_back(Noop);
+        }
+        
+        MidBlock->addInstructions(Seq.begin(), Seq.end());
+        // insertBasicBlock? Where should i insert it?
+        // This mid block shouldn't be the fallthrough of any block so
+        // it should be placed at the end of the function
+        std::vector<std::unique_ptr<BinaryBasicBlock>> newBBs;
+        newBBs.emplace_back(std::move(MidBlock));
+        BF.insertBasicBlocks(HeaderIsBranchTarget ? &(*BF.rbegin())
+                                                  : LPPredecessor,
+                             std::move(newBBs));
+    }
+}
+
+static void
+insertYield(InstructionListType &Instrs, const MCInst &CMInst,
+            BinaryBasicBlock *BB, BinaryFunction &BF, BitVector RegsToSave,
+            int IterTh, const MCSymbol *NextBB, RegAnalysis *RA) {
+    bool           UseMemory = false;
+    BinaryContext &BC        = BF.getBinaryContext();
+    unsigned       UnusedGPReg; // = BC.MIB->getUnusedGPReg(RegsToSave);
+    auto          &BLI  = BF.getLoopInfo();
+    BinaryLoop    *Loop = BLI.getLoopFor(BB);
+    assert(Loop);
+
+    // check all instructions in the loop
+
+    bool                      hasYield = false;
+    std::vector<BinaryLoop *> LoopList;
+    LoopList.push_back(Loop);
+    for (auto SubLoopIter = Loop->begin(); SubLoopIter != Loop->end();
+         SubLoopIter++) {
+        LoopList.push_back(*SubLoopIter);
+    }
+    for (auto CurLoop : LoopList) {
+        for (auto BB : CurLoop->blocks()) {
+            for (auto &Inst : *BB) {
+                if (&Inst == &CMInst)
+                    continue;
+
+                if (BC.MIB->hasAnnotation(Inst, "InsertYield") ||
+                    BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield")) {
+                    hasYield = true;
+                    break;
+                }
+            }
+        }
+        if (hasYield)
+            break;
+    }
+
+    // do further
+    BitVector RegsAlreadyUsed = Loop->RegsAlreadyUsed;
+    RegsAlreadyUsed |= RegsToSave;
+    BinaryLoop *Parent = Loop->getParentLoop();
+    while (Parent) {
+        RegsAlreadyUsed |= Parent->RegsAlreadyUsed;
+        Parent = Parent->getParentLoop();
+    }
+
+    UnusedGPReg = BC.MIB->getUnusedGPReg(RegsAlreadyUsed);
+    // unused reg sanity check
+    
+    if (UnusedGPReg == 0) {
+        errs() << "No unused GP register found\n";
+        return;   // skip
+        // exit(1);
+    }
+    
+    bool passSanityCheck = false;
+    while (!passSanityCheck) {
+        errs() << "UnusedReg sanity check ";
+        BC.InstPrinter->printRegName(errs(), UnusedGPReg);
+        errs() << "\n";
+        //define the follow loop as lambda function
+        auto checkUnusedReg = [&](BinaryLoop *Loop) {
+            for (auto BB : Loop->blocks()) {
+                for (auto &Inst : *BB) {
+                    // check used regs
+                    BitVector UsedRegs = BitVector(BC.MRI->getNumRegs(), false);
+                    RA->getInstUsedRegsList(Inst, UsedRegs, true);
+                    if (UsedRegs.test(UnusedGPReg)) {
+                        return false;
+                    }
+                }
+            }
+            return true;
+        };
+        if (checkUnusedReg(Loop)) {
+            passSanityCheck = true;
+        } else {
+            // find a new unused reg
+            errs() << "UnusedReg sanity check failed\n";
+            errs() << "Let's find another one\n";
+            RegsAlreadyUsed.set(UnusedGPReg);
+            UnusedGPReg = BC.MIB->getUnusedGPReg(RegsAlreadyUsed);
+            if (UnusedGPReg == 0) {
+                break;
+            }
+        }
+    }
+
+    
+    
+    if (UnusedGPReg == 0) {
+        errs() << "No unused GP register found\n";
+        return;   // skip
+        // exit(1);
+    }
+    
+
+    BitVector RV = BitVector(BC.MRI->getNumRegs(), false);
+    RV.set(UnusedGPReg);
+
+    Loop->RegsAlreadyUsed |= RV;
+    Parent = Loop->getParentLoop();
+    while (Parent) {
+        Parent->addRegsAlreadyUsed(RV);
+        Parent = Parent->getParentLoop();
+    }
+
+
+    UseMemory = hasYield;
+    if (opts::ScavDisableUseUnusedReg)
+        UseMemory = true;
+    // UseMemory = true;
+    //  print yieldpoint info
+    errs() << "YieldPoint: "
+           << "IterTh: " << IterTh
+           << BC.InstPrinter->getOpcodeName(CMInst.getOpcode())
+           << " / UseMemory: " << UseMemory << "\n";
+
+    if (UseMemory) {
+        if (opts::ScavDisableMemoryLoops)
+            return;
+
+        uint64_t cur_loop_counter_idx = loop_set_counter_idx;
+        loop_set_counter_idx++;
+        assert(loop_set_counter_idx < max_loop_set_counter_idx);
+
+        const MCSymbol *LoopCounterLoc =
+            BC.getBinaryDataByName("loop_counter_loc")->getSymbol();
+        BC.MIB->createConditionalBranchForYield(
+            Instrs, BC.Ctx.get(), IterTh, NextBB, LoopCounterLoc, UnusedGPReg,
+            cur_loop_counter_idx);
+
+        const MCSymbol *CrtPos = BC.getBinaryDataByName("crt_pos")->getSymbol();
+        RegsToSave.set(UnusedGPReg);
+        
+        if (!opts::ScavDisableYieldInLoops) {
+            BC.MIB->createYieldInstrs(Instrs, CMInst, CrtPos, BC.Ctx.get(),
+                                RegsToSave, true, true,
+                                cur_loop_counter_idx);
+        }
+        return;
+    }
+
+    
+    // check if innermost
+    // bool          isInnerMost = false;
+    // auto SubLoops = Loop->getSubLoops();
+    // if (SubLoops.empty())
+    //    isInnerMost = true;
+
+    // preheader
+
+    bool hasPathToFix = false;
+    for (auto pred = Loop->getHeader()->pred_begin();
+         pred != Loop->getHeader()->pred_end(); ++pred) {
+        if (Loop->contains(*pred))
+            continue;
+
+        if ((*pred)->succ_size() == 1) {
+            (*pred)->setIsPseudoPreheader(true);
+            continue;
+        }
+
+        if ((*pred)->succ_size() > 1) {
+            hasPathToFix = true;
+            break;
+        }
+    }
+
+    if (hasPathToFix) {
+        createPreheader(BF, Loop);
+    }
+
+    unsigned IReg = Loop->InductionReg;
+    int IConst = Loop->InductionRegInitVal;
+
+    bool insertToPreheader = false;
+    for (auto BBIter = Loop->getHeader()->pred_begin();
+         BBIter != Loop->getHeader()->pred_end(); ++BBIter) {
+        if ((*BBIter)->isPseudoPreheader()) {
+            auto Preheader = *BBIter;
+            // insert reset "UnusedReg" to zero before the lastinst
+            auto LastIter = Preheader->getLastNonPseudo();
+            // if last iter is conditiona branch choose next
+            if (BC.MIB->isConditionalBranch(*LastIter)) {
+                LastIter++;
+            }
+
+            auto   IterToInsert = std::prev(LastIter.base());    
+            if (IReg == 0) {
+                MCInst SetRegToZero;
+                BC.MIB->setRegToZero(SetRegToZero, UnusedGPReg);
+                // insert SetRegToZero before LastIter
+                Preheader->insertInstruction(IterToInsert, SetRegToZero);
+            } else {
+                MCInst CopyFromIReg;
+                BC.MIB->copyRegToReg(CopyFromIReg, IReg, UnusedGPReg);
+                MCInst AddIterTh;
+                BC.MIB->createAddRegImm(AddIterTh, UnusedGPReg, IterTh * IConst, 8);
+                IterToInsert = Preheader->insertInstruction(IterToInsert, AddIterTh);
+                IterToInsert = Preheader->insertInstruction(IterToInsert, CopyFromIReg);
+            }
+            insertToPreheader = true;
+        }
+    }
+
+    assert(insertToPreheader);
+
+    BC.MIB->createConditionalBranchForYield(Instrs, BC.Ctx.get(), IterTh,
+                                            NextBB, NULL, UnusedGPReg, -1, IReg, IConst);
+    const MCSymbol *CrtPos = BC.getBinaryDataByName("crt_pos")->getSymbol();
+    
+    RegsToSave.set(UnusedGPReg);
+    if (!opts::ScavDisableYieldInLoops) {
+        BC.MIB->createYieldInstrs(Instrs, CMInst, CrtPos, BC.Ctx.get(), RegsToSave,
+                            true, true);
+    }
+}
+
+void
+BoundYieldDistanceInstrumenter::instrumentFunc() {
+    // print name
+    errs() << "FUNCTION_NAME: " << BF.getPrintName() << ":\n";
+    if (BF.getPrintName() == "init") {
+        return;   // skip init
+    }
+
+    std::map<BinaryBasicBlock *, MCInst *> Patches;
+    for (auto &BB : BF) {
+        for (auto Inst = BB.begin(); Inst != BB.end(); ++Inst) {
+            if (BC.MIB->hasAnnotation(*Inst, "IterationBound")) {
+                if (Patches.find(&BB) == Patches.end()) {
+                    Patches[&BB] = &(*Inst);
+                } else {
+                    errs() << "Multiple IterationBound in one BB\n";
+                    assert(0);
+                }
+            }
+        }
+    }
+
+    for (auto &Patch : Patches) {
+        BinaryBasicBlock *BB   = Patch.first;
+        MCInst           *Inst = Patch.second;
+        // BinaryBasicBlock *NewBB;
+        auto                              Iter       = BB->begin();
+        std::unique_ptr<BinaryBasicBlock> NewBB      = BF.createBasicBlock();
+        const MCSymbol                   *NewBBLabel = NewBB->getLabel();
+        for (Iter = BB->begin(); Iter != BB->end(); ++Iter) {
+            if (&(*Iter) == Inst) {
+                break;
+            }
+        }
+
+        InstructionListType Seq;
+        assert(BC.MIB->hasAnnotation(*Inst, "IterationBound"));
+        assert(BC.MIB->hasAnnotation(*Inst, "scanAddr"));
+        uint64_t ScanAddr =
+            BC.MIB->getAnnotationAs<uint64_t>(*Inst, "scanAddr");
+        assert(IM.count(ScanAddr) == 1);
+        BitVector RegsToSave = IM[ScanAddr].RegsToSave;
+        int IterTh = BC.MIB->getAnnotationAs<int>(*Inst, "IterationBound");
+        insertYield(Seq, *Inst, BB, BF, RegsToSave, IterTh, NewBBLabel, RA);
+
+        // do manual split
+        BB->moveAllSuccessorsTo(NewBB.get());
+        BB->addSuccessor(NewBB.get(), 0, 0);
+        NewBB->setCFIState(BB->getCFIStateAtInstr(&*Iter));
+        // BB->adjustNumPseudos(Iter, BB->end(), -1);
+        NewBB->addInstructions(Iter, BB->end());
+        while (Iter != BB->end()) {
+            Iter = BB->eraseInstruction(Iter);
+        }
+
+        std::vector<std::unique_ptr<BinaryBasicBlock>> newBBs;
+        newBBs.emplace_back(std::move(NewBB));
+        BF.insertBasicBlocks(BB, std::move(newBBs));
+
+        for (auto &Inst : Seq) {
+            BB->insertInstruction(BB->end(), Inst);
+        }
+    }
+}
+
+}   // end namespace bolt
+}   // end namespace llvm
\ No newline at end of file
diff --git a/bolt/lib/Passes/PnYFilterUnusedRegs.cpp b/bolt/lib/Passes/PnYFilterUnusedRegs.cpp
new file mode 100644
index 000000000000..c605a1e60e58
--- /dev/null
+++ b/bolt/lib/Passes/PnYFilterUnusedRegs.cpp
@@ -0,0 +1,122 @@
+#include "bolt/Passes/PrefetchAndYield.h"
+
+namespace llvm {
+namespace bolt {
+
+void FilterUnusedRegScanner::annotateInst(MCInst &Inst, BinaryBasicBlock &BB) {
+    const MCInstrDesc &InstInfo = BC.MII->get(Inst.getOpcode());
+    BitVector GPRwithAliases = BitVector(BC.MRI->getNumRegs(), false);
+    BC.MIB->getGPRegs(GPRwithAliases, /*IncludeAlias=*/true);
+    for (int i = 0; i < Inst.getNumOperands(); i++) {
+        auto oper = Inst.getOperand(i);
+        if (oper.isReg()) {
+            unsigned reg = oper.getReg();
+            // check if reg is zero and NoRegister
+            if (GPRwithAliases.test(reg) && reg != 0) {
+                reg = BC.MIB->getAliasSized(reg, 8);
+            }
+
+            if (reg != 0) {
+                UsedRegs.set(reg);
+            }
+        }
+    }
+    for (MCPhysReg ImplicitUse : InstInfo.implicit_uses())
+        UsedRegs.set(ImplicitUse);
+
+    // print the instruction and check if r9 is in the used reg
+    //BC.printInstruction(errs(), Inst);
+    //errs() << "\n";
+    //if (UsedRegs.test(BC.MIB->getIntArgRegister(5))) {
+    //    errs() << "r9 is used\n";
+    //}
+}
+
+void FilterUnusedRegScanner::updateState() {
+    if (UsedRegs.empty())
+        return;
+
+    BitVector Filtered = BitVector(UsedRegs.size(), false);
+
+    for (auto &Entry : IM) {
+        BitVector Residue = Entry.second.RegsToSave;
+        for (int i = 1; i < UsedRegs.size(); i++) {
+            if (UsedRegs[i]) {
+                Residue.reset(i);
+            }
+        }
+
+        Filtered |= Residue;
+        Entry.second.RegsToSave &= UsedRegs;
+    }
+
+    BF.setFilteredRegs(Filtered);
+    
+}
+
+void FilterUnusedRegInstrumenter::instrumentFunc() {
+    BinaryContext &BC       = BF.getBinaryContext();
+    BitVector      Filtered = BF.getFilteredRegs();
+
+    if (Filtered.count() == 0) {
+        return;
+    }
+
+    if (BF.IsPseudoCopy) {
+        PY_LOG("    PseudoCopy filterted regs: " << Filtered.count() << "\n")
+        return; // only save, no instrument
+    }
+
+    assert(next_regset_idx < maximum_regset_size);
+    // Note that only the following registers (9 + 2) can be in residual set (=
+    // default Live out set + callee-saved) RAX RBX RDX RCX RBP XMM0 XMM1 R12 -
+    // R15
+    // ---
+    // The follwing registers (6) are free to use at the beginning/end of the
+    // function RDI RSI R8 - R11 
+    const MCSymbol *CrtPos = NULL;
+    if (opts::InstScavenger)
+        CrtPos = BC.getBinaryDataByName("crt_pos")->getSymbol();
+
+
+    // Insert push instructions one time
+    BinaryBasicBlock &EntryBB   = BF.front();
+    auto              EntryIter = EntryBB.begin();
+
+    if (BC.MIB->isTerminateBranch(*EntryIter)) {
+        EntryIter++;
+    }
+
+    while (BC.MIB->isCFI(*EntryIter)) {
+        EntryIter++;
+    }
+
+    InstructionListType Instrs;
+    BC.MIB->createPushToRegSet(Instrs, Filtered, CrtPos, BC.Ctx.get(),
+                               next_regset_idx);
+    insertInstructions(Instrs, EntryBB, EntryIter);
+
+    // Insert pop instructions before every ret
+    for (BinaryBasicBlock &BB : BF) {
+        for (auto iter = BB.begin(); iter != BB.end(); ++iter) {
+            if (BC.MIB->isReturn(*iter)) {
+                InstructionListType Instrs;
+                BC.MIB->createPopFromRegSet(Instrs, Filtered, CrtPos,
+                                            BC.Ctx.get(), next_regset_idx);
+                iter = insertInstructions(Instrs, BB, iter);
+            }
+        }
+    }
+
+    next_regset_idx++;
+
+    StatFilteredRegNum = Filtered.count();
+}
+
+void FilterUnusedRegInstrumenter::printStat() {
+    PY_LOG("FilterUnusedRegInstrumenter\n")
+    PY_LOG("    Number of FilteredRegs: " << StatFilteredRegNum << "\n")
+}
+
+}   // end namespace bolt
+}   // end namespace llvm
\ No newline at end of file
diff --git a/bolt/lib/Passes/PnYInstrument.cpp b/bolt/lib/Passes/PnYInstrument.cpp
new file mode 100644
index 000000000000..3e9219e10080
--- /dev/null
+++ b/bolt/lib/Passes/PnYInstrument.cpp
@@ -0,0 +1,173 @@
+#include "bolt/Passes/DataflowInfoManager.h"
+#include "bolt/Passes/PrefetchAndYield.h"
+
+namespace opts {
+
+extern cl::OptionCategory BoltOptCategory;
+
+cl::opt<bool>
+SaveAllRegister("save-all-register",
+  cl::desc("---"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltOptCategory));
+
+}
+
+namespace llvm {
+namespace bolt {
+
+class DefaultInstrumenter : public PnYSubInstrumenter {
+  private:
+    InstMap &IM;
+    IterType insertPrefetchAndYield(IterType Iter, BinaryBasicBlock &BB,
+                                    BinaryFunction  &BF,
+                                    const BitVector &RegsToSave,
+                                    bool             IsPrefetch);
+
+  public:
+    DefaultInstrumenter(BinaryFunction &BF, InstMap &IM)
+        : PnYSubInstrumenter(BF), IM(IM) {
+        if (!opts::DisablePseudoInline)
+            assert(PreInstrumentationProgress.PseudoInlineInstDone);
+        InstrumentationProgress.DefaultInstDone = true;
+    }
+    void printStat();
+
+  private:
+    IterType modifyInst(IterType &Iter, BinaryBasicBlock &BB) override;
+};
+
+IterType
+DefaultInstrumenter::modifyInst(IterType &Iter, BinaryBasicBlock &BB) {
+    MCInst              &Inst = *Iter;
+    const BinaryContext &BC   = BF.getBinaryContext();
+    bool PrefetchTarget = BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield");
+    bool YieldTarget = BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield") ||
+                       BC.MIB->hasAnnotation(Inst, "InsertYield");
+    bool BoundIterationTarget = BC.MIB->hasAnnotation(Inst, "IterationBound");
+
+    if (BoundIterationTarget)
+        return Iter;
+
+    if (!PrefetchTarget && !YieldTarget)
+        return Iter;
+
+    assert(BC.MIB->hasAnnotation(Inst, "scanAddr"));
+    uint64_t ScanAddr = BC.MIB->getAnnotationAs<uint64_t>(Inst, "scanAddr");
+
+    assert(IM.count(ScanAddr) == 1);
+
+    BitVector AllGPRs(BC.MRI->getNumRegs(), false);
+    BC.MIB->getGPRegs(AllGPRs, false);
+    AllGPRs |= IM[ScanAddr].RegsToSave;
+    BitVector RegsToSave = opts::SaveAllRegister ? AllGPRs : IM[ScanAddr].RegsToSave;
+    return insertPrefetchAndYield(Iter, BB, BF, RegsToSave, PrefetchTarget);
+}
+
+IterType
+DefaultInstrumenter::insertPrefetchAndYield(IterType Iter, BinaryBasicBlock &BB,
+                                            BinaryFunction  &BF,
+                                            const BitVector &RegsToSave,
+                                            bool             IsPrefetch) {
+    InstructionListType  Instrs;
+    const BinaryContext &BC     = BF.getBinaryContext();
+    MCInst               CMInst = *Iter;
+
+    bool IsIterationBound = BC.MIB->hasAnnotation(CMInst, "IterationBound");
+
+
+    //assert(!(IsIterationBound && IsPrefetch));
+
+
+    if (IsPrefetch) {
+        if (!BC.MIB->createPrefetchInstrs(Instrs, CMInst)) {
+            PY_LOG("\n"
+                   << std::string(
+                          BC.InstPrinter->getOpcodeName(CMInst.getOpcode()))
+                   << " is not prefetchable in current implementation \n");
+
+            return Iter;
+        }
+        PY_LOG("\n"
+               << std::string(BC.InstPrinter->getOpcodeName(CMInst.getOpcode()))
+               << " will be prefetched, " << CMInst.getNumOperands() << "\n");
+    }
+
+    const MCSymbol *CurCrtCtx, *NextCrtCtx;
+    const MCSymbol *CrtPos = NULL;
+    if (opts::InstScavenger)
+        CrtPos = BC.getBinaryDataByName("crt_pos")->getSymbol();
+    BC.MIB->createYieldInstrs(Instrs, CMInst, CrtPos, BC.Ctx.get(), RegsToSave,
+                              true, opts::InstScavenger ? false : true);
+
+    // find the first GPR that is not in RegsToSave
+    // unsigned UnusedGPReg = BC.MIB->getUnusedGPReg(RegsToSave); 
+
+    // if (IsIterationBound && (UnusedGPReg != 0)) {
+    //    int IterTh = BC.MIB->getAnnotationAs<int>(CMInst, "IterationBound");
+    //    BC.MIB->createConditionalBranchForYield(Instrs, BC.Ctx.get(), IterTh, UnusedGPReg);
+    //}
+
+
+    return insertInstructions(Instrs, BB, Iter);
+}
+
+void
+DefaultInstrumenter::printStat() {
+    PY_LOG("DefaultInstrumenter\n")
+    PY_LOG("    Number of PnY Points: " << IM.size() << "\n")
+    for (auto &IMEntry : IM) {
+        PY_LOG("    ScanAddr: " << std::hex << IMEntry.first << "\n")
+        PY_LOG("    Num of RegsToSave: " << IMEntry.second.RegsToSave.count()
+                                         << "\n\n")
+    }
+}
+
+void
+PnYInstrumenter::preInstrument() {
+    BinaryFunction &BF = scanner.BF;
+
+    if (!opts::DisablePseudoInline) {
+        PseudoInlinePreInstrumenter PIPI(BF);
+        PIPI.instrumentInst();
+    }
+}
+
+void
+PnYInstrumenter::instrument() {
+    BinaryFunction &BF = scanner.BF;
+
+    DefaultInstrumenter DI(BF, scanner.IM);
+    DI.instrumentInst();
+    DI.printStat(); 
+
+    if (!opts::DisableFUR) {
+        FilterUnusedRegInstrumenter SRURI(BF);
+        SRURI.instrumentFunc();
+        SRURI.printStat();
+    }
+
+    if (!opts::DisablePseudoInline && !opts::InstScavenger) {
+        PseudoInlineInstrumenter PII(BF);
+        PII.instrumentInst();
+    }
+
+    if (!opts::DisableLO) {
+        LoopOptInstrumenter LOI(BF);
+        LOI.instrumentFunc();
+    }
+
+    if (!opts::DisableCtxPrefetch) {
+        PrefetchCrtCtxInstrumenter PCCI(BF);
+        PCCI.instrumentInst();
+    }
+
+    if (opts::InstScavenger && opts::BoundYieldDistance) {
+        BoundYieldDistanceInstrumenter BYDI(BF, scanner.IM, RA);
+        BYDI.instrumentFunc();
+    }
+}
+
+}   // end namespace bolt
+}   // end namespace llvm
\ No newline at end of file
diff --git a/bolt/lib/Passes/PnYLoopOptimization.cpp b/bolt/lib/Passes/PnYLoopOptimization.cpp
new file mode 100644
index 000000000000..de92b0e75c07
--- /dev/null
+++ b/bolt/lib/Passes/PnYLoopOptimization.cpp
@@ -0,0 +1,431 @@
+#include "bolt/Passes/PrefetchAndYield.h"
+
+namespace llvm {
+namespace bolt {
+
+void
+LoopOptScanner::annotateInst(MCInst &Inst, BinaryBasicBlock &BB) {
+    // dependency: DefaultScanner
+    bool PrefetchTarget = BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield");
+    bool YieldTarget = BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield") ||
+                       BC.MIB->hasAnnotation(Inst, "InsertYield");
+    bool PseudoInlineTarget = BC.MIB->hasAnnotation(Inst, "PseudoInline");
+
+    if (!PrefetchTarget && !YieldTarget && !PseudoInlineTarget)
+        return;
+
+    if (!BF.hasLoopInfo())
+        BF.calculateLoopInfo();
+    auto       &BLI  = BF.getLoopInfo();
+    BinaryLoop *Loop = BLI.getLoopFor(&BB);
+    if (Loop == nullptr)   // not a target
+        return;
+
+    BinaryFunction *CalleeBF = nullptr;
+
+    if (PseudoInlineTarget) {
+        const MCSymbol *CalleeSymbol = BC.MIB->getTargetSymbol(Inst);
+        CalleeBF = BC.getFunctionForSymbol(CalleeSymbol);
+    }
+
+    assert(IM.count((uint64_t)&Inst) == 1 || CalleeBF);
+
+    BitVector RegsToSave     = PseudoInlineTarget ?
+       CalleeBF-> getFilteredRegs() : IM[(uint64_t) &Inst].RegsToSave;
+    BitVector RegsToEvac     = BitVector(RegsToSave.size(), false);
+    BitVector RegsToAsymPush = BitVector(RegsToSave.size(), false);
+    BitVector RegsToAsymPop  = BitVector(RegsToSave.size(), false);
+
+    /**
+     * Note for the use of ReachingDefOrUse:
+     * After ReachingDefOrUse runs, each instruction has a N-dimensional bit
+     * vector. N = total # of instructions in this function. In ReachingDef
+     * (forward) i-th bit of the bit vector is set if the definition in i-th
+     * instruction may reach the current instruction. In ReachingUse (backward)
+     *     i-th bit of the bit vector is set if the register use in i-th
+     * instruction may be the same as the current instruction.
+     */
+    /**
+     * What do we need to check?
+     * For each register in RegsToSave, we need to check if there is any
+     * definition/use of it in the loop of current BB ReachingDefOrUse just
+     * returns all instructions that have a definition/use of the register, so
+     * we need to look into one by one manually
+     */
+    for (unsigned Reg = 1; Reg < RegsToSave.size(); Reg++) {
+        bool DefFound = false;
+        bool UseFound = false;
+        // We use R11 for the pointer to CrtCtx
+        // We give up on saving R11
+        unsigned CCR;
+        BC.MIB->getCrtCtxReg(CCR, 8);
+        if (Reg == CCR) {
+            continue;
+        }
+
+        if (!RegsToSave[Reg]) {
+            continue;
+        }
+
+        ReachingDefOrUse<true>  *RD = new ReachingDefOrUse<true>(*RA, BF, Reg);
+        ReachingDefOrUse<false> *RU = new ReachingDefOrUse<false>(*RA, BF, Reg);
+        RD->run();
+        RU->run();
+
+        // generate Loop list : loop itself + subloops
+        std::vector<BinaryLoop *> LoopList;
+        LoopList.push_back(Loop);
+        for (auto SubLoopIter = Loop->begin(); SubLoopIter != Loop->end();
+             SubLoopIter++) {
+            LoopList.push_back(*SubLoopIter);
+        }
+
+        for (auto CurLoop : LoopList) {
+            for (auto BlockIter = CurLoop->block_begin();
+                BlockIter != CurLoop->block_end(); BlockIter++) {
+                for (auto I = RD->expr_begin(Inst), E = RD->expr_end(); I != E;
+                    ++I) {
+                    assert(*I != nullptr && "Lost pointers");
+                    // *I = MCInst*
+                    auto MatchIter = (*BlockIter)->findInstruction(*I);
+                    if (MatchIter != (*BlockIter)->end()) {
+                        DefFound = true;
+                        break;
+                    }
+                }
+                for (auto I = RU->expr_begin(Inst), E = RU->expr_end(); I != E;
+                    ++I) {
+                    assert(*I != nullptr && "Lost pointers");
+                    // *I = MCInst*
+                    auto MatchIter = (*BlockIter)->findInstruction(*I);
+                    if (MatchIter != (*BlockIter)->end()) {
+                        UseFound = true;
+                        break;
+                    }
+                }
+            }
+        }
+        delete RD;
+        delete RU;
+
+        if (!UseFound && !DefFound) {
+            RegsToSave.reset(Reg);
+            RegsToEvac.set(Reg);
+        }
+        if (!UseFound && DefFound) {
+            RegsToSave.reset(Reg);
+            RegsToAsymPush.set(Reg);
+        }
+        if (!DefFound && UseFound) {
+            RegsToSave.reset(Reg);
+            RegsToAsymPop.set(Reg);
+        }
+    }
+    
+    if (opts::DisableAsymPushPop || PseudoInlineTarget) {
+        RegsToSave |= RegsToAsymPush;
+        RegsToSave |= RegsToAsymPop;
+        RegsToAsymPush.clear();
+        RegsToAsymPop.clear();
+    }
+
+    Loop->addRegsToEvacuate(RegsToEvac);
+    Loop->addRegsToAsymPush(RegsToAsymPush);
+    Loop->addRegsToAsymPop(RegsToAsymPop);
+
+    if (PseudoInlineTarget) {
+        assert(BC.MIB->hasAnnotation(Inst, "RegsFromPseudoInline"));
+        BC.MIB->removeAnnotation(Inst, "RegsFromPseudoInline");
+        BC.MIB->addAnnotation(Inst, "RegsFromPseudoInline", RegsToSave);
+    } else {
+        IM[(uint64_t) &Inst].RegsToSave = RegsToSave;
+    }
+
+    StatMap[(uint64_t) &Inst] = StatEntry({RegsToEvac.count(),
+                                           RegsToAsymPush.count(),
+                                           RegsToAsymPop.count()});
+}
+
+void LoopOptScanner::printStat() {
+    PY_LOG("LoopOptScanner: \n");
+    for (auto &Entry : StatMap) {
+        PY_LOG("  ScanAddr: " << std::hex << Entry.first << "\n");
+        PY_LOG("    # of RegsToEvac: " << Entry.second.RegsToEvacNum << "\n");
+        PY_LOG("    # of RegsToAsymPush: " << Entry.second.RegsToAsymPushNum
+                                           << "\n");
+        PY_LOG("    # of RegsToAsymPop: " << Entry.second.RegsToAsymPopNum
+                                          << "\n");
+    }
+}
+
+void LoopOptInstrumenter::instrumentFunc() {
+    if (!BF.hasLoopInfo())
+        return;
+
+    auto                &BLI = BF.getLoopInfo();
+    const BinaryContext &BC  = BF.getBinaryContext();
+
+    for (auto &BL : BLI.getLoopsInPreorder()) {
+        const BitVector &RegsToEvac     = BL->getRegsToEvacuate();
+        const BitVector &RegsToAsymPush = BL->getRegsToAsymPush();
+        const BitVector &RegsToAsymPop  = BL->getRegsToAsymPop();
+
+        if (RegsToEvac.count() == 0 && RegsToAsymPush.count() == 0 &&
+            RegsToAsymPop.count() == 0)
+            continue;
+
+        assert(next_regset_idx < maximum_regset_size);
+
+        // push the registers at the end of the preheader
+        // TODO: patch non-preheader to be preheader
+
+        // find edges to patch
+        BinaryBasicBlock               *LoopHeader = BL->getHeader();
+        std::vector<BinaryBasicBlock *> LoopPredecessors;
+        for (auto PredIter = LoopHeader->pred_begin();
+             PredIter != LoopHeader->pred_end(); PredIter++) {
+            if (!BL->contains(*PredIter))
+                LoopPredecessors.push_back(*PredIter);
+        }
+
+        // print Loop Header name
+        errs() << "Loop Header: " << LoopHeader->getName() << "\n";
+
+        std::vector<std::pair<BinaryBasicBlock *, BinaryBasicBlock *>>
+            EdgesToPatch;
+
+        for (auto LP : LoopPredecessors) {
+            for (auto SuccIter = LP->succ_begin(); SuccIter != LP->succ_end();
+                 SuccIter++) {
+                if (BL->contains(*SuccIter) && LP->succ_size() != 1) {
+                    EdgesToPatch.push_back(std::make_pair(LP, *SuccIter));
+                    // print edge name
+                    errs() << "Edge: " << LP->getName() << " -> "
+                           << (*SuccIter)->getName() << "\n";
+                }
+            }
+        }
+
+        
+
+        // do patch : add mid block
+        for (auto edge : EdgesToPatch) {
+            BinaryBasicBlock *LPPredecessor = edge.first;
+            BinaryBasicBlock *LPHeader      = edge.second;
+            auto              LastIter      = LPPredecessor->getLastNonPseudo();
+            bool              HeaderIsBranchTarget =
+                BC.MIB->isBranch(*LastIter) &&
+                BC.MIB->getTargetSymbol(*LastIter) == LPHeader->getLabel();
+
+            if (!HeaderIsBranchTarget) {
+                // check if there is another branch in this basic block that points to preheader
+                while (LastIter != LPPredecessor->rend()) {
+                    LastIter++;
+                    // check if this is a valid instruction
+                    if (LastIter == LPPredecessor->rend())
+                        break;
+                    if (BC.MIB->isCFI(*LastIter) || BC.MIB->isPseudo(*LastIter))
+                        continue;
+
+                    if (BC.MIB->isBranch((*LastIter)) &&
+                        BC.MIB->getTargetSymbol((*LastIter)) == LPHeader->getLabel()) {
+                        HeaderIsBranchTarget = true;
+                        break;
+                    }
+                } 
+            }
+
+            // create mid block
+            std::unique_ptr<BinaryBasicBlock> MidBlock = BF.createBasicBlock();
+            MidBlock->setIsPseudoPreheader(true);
+            LPPredecessor->replaceSuccessor(LPHeader, MidBlock.get());
+            MidBlock->addSuccessor(LPHeader);
+
+            InstructionListType Seq;
+            if (HeaderIsBranchTarget) {
+                // patch branch target
+                BC.MIB->replaceBranchTarget(*LastIter, MidBlock->getLabel(),
+                                            BC.Ctx.get());
+                BC.MIB->createShortJmp(Seq, LPHeader->getLabel(), BC.Ctx.get());
+                errs() << "LO header is branch target\n"; 
+            } else {
+                MCInst Noop;
+                BC.MIB->createNoop(Noop);
+                Seq.push_back(Noop);
+            }
+            
+            
+            MidBlock->addInstructions(Seq.begin(), Seq.end());
+             
+            /*
+            errs() << "LO LastInst in PrevBB\n";
+            BC.printInstruction(errs(), *Inst);
+            errs() << "LO PrintPreheader\n";
+            BC.printInstructions(errs(), Seq.begin(), Seq.end());
+            errs() << "LO PrintPreheader End\n";
+            */
+           
+            // insertBasicBlock? Where should i insert it?
+            // This mid block shouldn't be the fallthrough of any block so
+            // it should be placed at the end of the function
+            std::vector<std::unique_ptr<BinaryBasicBlock>> newBBs;
+            newBBs.emplace_back(std::move(MidBlock));
+            BF.insertBasicBlocks(HeaderIsBranchTarget ? &(*BF.rbegin())
+                                                      : LPPredecessor,
+                                 std::move(newBBs));
+        }
+
+        // do push
+        std::vector<BinaryBasicBlock *> LoopPreheaders;
+        for (auto PredIter = LoopHeader->pred_begin();
+             PredIter != LoopHeader->pred_end(); PredIter++) {
+            if (!BL->contains(*PredIter)) {
+                assert((*PredIter)->succ_size() == 1);
+                assert(*((*PredIter)->succ_begin()) == LoopHeader);
+                LoopPreheaders.push_back(*PredIter);
+            }
+        }
+
+        for (auto Preheader : LoopPreheaders) {
+            assert(Preheader->succ_size() == 1);
+            auto    LastIter = Preheader->getLastNonPseudo();
+            MCInst &LastInst = *LastIter;
+            // convert to a forward iterator
+            auto IterToInsert = BC.MIB->isBranch(LastInst)
+                                    ? std::prev(LastIter.base())
+                                    : Preheader->end();
+
+            InstructionListType Instrs;
+            // Both AsymPush, Pop registers should be added
+            // AsymPush regs may not be defined within the loop depending on the
+            // loop condition So should store AsymPush regs together for safety
+            BitVector RegsToPush = RegsToEvac;
+            RegsToPush |= RegsToAsymPush;
+            RegsToPush |= RegsToAsymPop;
+            BC.MIB->createPushToRegSet(Instrs, RegsToPush, CrtPos,
+                                       BC.Ctx.get(), next_regset_idx);
+            insertInstructions(Instrs, *Preheader, IterToInsert);
+        }
+
+        // -------------------------------------
+        // and pop them at the beginning of each exiting inst
+        // We will make all ExitBlocks to be dedicated exits by inserting a mid
+        // block
+
+        // 1. Identify exit edge to patch
+        EdgesToPatch.clear();
+        SmallVector<BinaryBasicBlock *, 4> ExitingBlocks;
+        BL->getExitingBlocks(ExitingBlocks);
+        for (auto ExitingBlock : ExitingBlocks) {
+            for (auto SuccIter = ExitingBlock->succ_begin();
+                 SuccIter != ExitingBlock->succ_end(); ++SuccIter) {
+                if (!BL->contains(*SuccIter) && (*SuccIter)->pred_size() != 1)
+                    EdgesToPatch.push_back(
+                        std::make_pair(ExitingBlock, *SuccIter));
+            }
+        }
+
+        // 2. do patch: create mid block
+        for (auto edge : EdgesToPatch) {
+            BinaryBasicBlock *ExitingBlock = edge.first;
+            BinaryBasicBlock *ExitBlock    = edge.second;
+            auto              LastIter     = ExitingBlock->getLastNonPseudo();
+            MCInst           &Inst         = *LastIter;
+            bool              ExitBlockIsBranchTarget =
+                BC.MIB->isBranch(Inst) &&
+                BC.MIB->getTargetSymbol(Inst) == ExitBlock->getLabel();
+
+            // create mid block
+            std::unique_ptr<BinaryBasicBlock> MidBlock = BF.createBasicBlock();
+            ExitingBlock->replaceSuccessor(ExitBlock, MidBlock.get());
+            MidBlock->addSuccessor(
+                ExitBlock);   // predessor is automatically set by addSuccessor
+
+            InstructionListType Seq;
+            if (ExitBlockIsBranchTarget) {
+                // patch branch target
+                BC.MIB->replaceBranchTarget(Inst, MidBlock->getLabel(),
+                                            BC.Ctx.get());
+                BC.MIB->createShortJmp(Seq, ExitBlock->getLabel(),
+                                       BC.Ctx.get());
+            } else {
+                MCInst Noop;
+                BC.MIB->createNoop(Noop);
+                Seq.push_back(Noop);
+            }
+            MidBlock->addInstructions(Seq.begin(), Seq.end());
+            // insertBasicBlock? Where should i insert it?
+            // This mid block shouldn't be the fallthrough of any block so
+            // it should be placed at the end of the function
+            std::vector<std::unique_ptr<BinaryBasicBlock>> newBBs;
+            newBBs.emplace_back(std::move(MidBlock));
+            BF.insertBasicBlocks(ExitBlockIsBranchTarget ? &(*BF.rbegin())
+                                                         : ExitingBlock,
+                                 std::move(newBBs));
+        }
+
+        // 3. Time to pop
+        // Every exit block now has only one predecessor
+        std::vector<BinaryBasicBlock *> DedicatedExitBlocks;
+        for (auto ExitingBlock : ExitingBlocks) {
+            for (auto SuccIter = ExitingBlock->succ_begin();
+                 SuccIter != ExitingBlock->succ_end(); ++SuccIter) {
+                if (!BL->contains(*SuccIter)) {
+                    assert((*SuccIter)->pred_size() == 1);
+                    DedicatedExitBlocks.push_back(*SuccIter);
+                }
+            }
+        }
+
+        for (auto ExitBlock : DedicatedExitBlocks) {
+            auto FirstIter = ExitBlock->getFirstNonPseudo();
+            assert(next_regset_idx < maximum_regset_size);
+
+            InstructionListType Instrs;
+
+            // Pop only AsymPush regs (who may be redefined within the loop)
+            BitVector RegsToPop = RegsToEvac;
+            RegsToPop |= RegsToAsymPush;
+            BC.MIB->createPopFromRegSet(Instrs, RegsToPop, CrtPos,
+                                        BC.Ctx.get(), next_regset_idx);
+            insertInstructions(Instrs, *ExitBlock, FirstIter);
+        }
+
+        // Patch asym Push/Pop
+        // Find yield points within the loop and patch asym push/pop
+        for (auto BBIter = BL->block_begin(); BBIter != BL->block_end();
+             ++BBIter) {
+            BinaryBasicBlock *BB = *BBIter;
+            for (auto InstIter = BB->begin(); InstIter != BB->end();
+                 ++InstIter) {
+                MCInst &Inst = *InstIter;
+                if (BC.MIB->hasAnnotation(Inst, "FirstCtxAccess")) {
+                    // Patch asym push
+                    if (RegsToAsymPush.empty())
+                        continue;
+
+                    InstructionListType Instrs;
+                    BC.MIB->createPushToRegSet(Instrs, RegsToAsymPush,
+                                               CrtPos, BC.Ctx.get(),
+                                               next_regset_idx, false, true);
+                    InstIter = insertInstructions(Instrs, *BB, InstIter);
+                } else if (BC.MIB->hasAnnotation(Inst, "FirstPop")) {
+                    if (RegsToAsymPop.empty())
+                        continue;
+
+                    InstructionListType Instrs;
+                    BC.MIB->createPopFromRegSet(Instrs, RegsToAsymPop,
+                                                CrtPos, BC.Ctx.get(),
+                                                next_regset_idx, false, true);
+                    InstIter = insertInstructions(Instrs, *BB, InstIter);
+                }
+            }
+        }
+
+        BL->clearRegsToEvacuate();
+        next_regset_idx++;
+    }
+}
+
+}   // end namespace bolt
+}   // end namespace llvm
\ No newline at end of file
diff --git a/bolt/lib/Passes/PnYPass.cpp b/bolt/lib/Passes/PnYPass.cpp
new file mode 100644
index 000000000000..dbb27a50552c
--- /dev/null
+++ b/bolt/lib/Passes/PnYPass.cpp
@@ -0,0 +1,122 @@
+#include <sstream>
+
+#include "bolt/Core/ParallelUtilities.h"
+#include "bolt/Passes/BinaryFunctionCallGraph.h"
+#include "bolt/Passes/BinaryPasses.h"
+#include "bolt/Passes/DataflowInfoManager.h"
+#include "bolt/Passes/PrefetchAndYield.h"
+#include "llvm/Support/CommandLine.h"
+
+namespace llvm {
+namespace bolt {
+unsigned next_regset_idx = 0;
+const unsigned maximum_regset_size = 30;
+std::stringstream pny_log_sstream;
+
+void 
+PrefetchAndYield::preScanOnFunction(BinaryFunction &BF) {
+    PY_PREPARE_LOG
+    PY_LOG("\n=================\n")
+    PY_LOG("PnY PreScan | Function: " << BF.getPrintName() << ":\n")
+    
+
+    PnYScanner s(BF, RA);
+    s.preScan();
+
+    if (s.hasInstPoint()) {
+        PY_PRINT_LOG
+    }
+}
+
+void 
+PrefetchAndYield::preInstrumentationOnFunction(BinaryFunction &BF) {
+    PY_PREPARE_LOG
+    PY_LOG("\n=================\n")
+    PY_LOG("PnY PreInst | Function: " << BF.getPrintName() << ":\n")
+    
+    PnYScanner dummy(BF, nullptr);
+    
+    PnYInstrumenter i(dummy, nullptr);
+    i.preInstrument();
+    
+    if (dummy.hasInstPoint()) {
+        PY_PRINT_LOG
+    }
+}
+
+
+void
+PrefetchAndYield::runOnFunction(BinaryFunction &BF) {
+    PY_PREPARE_LOG
+    PY_LOG("\n=================\n")
+    PY_LOG("PnY main | Function: " << BF.getPrintName() << ":\n")
+    
+    PnYScanner s(BF, RA);
+    s.scan();
+
+    PnYInstrumenter i(s, RA);
+    i.instrument();
+
+    if (s.hasInstPoint()) {
+        PY_PRINT_LOG
+    }
+}
+
+void
+PrefetchAndYield::runOnFunctions(BinaryContext &BC) {
+    std::unique_ptr<BinaryFunctionCallGraph> CG;
+    auto                                    &BFs = BC.getBinaryFunctions();
+
+    CG.reset(new BinaryFunctionCallGraph(buildCallGraph(BC)));
+    RA = new RegAnalysis(BC, &BFs, &*CG);
+
+    ParallelUtilities::WorkFuncTy PreScanFun = [&](BinaryFunction &BF) {
+        preScanOnFunction(BF);
+    };
+
+    ParallelUtilities::WorkFuncTy PreInstrumentFun = [&](BinaryFunction &BF) {
+        preInstrumentationOnFunction(BF);
+    };
+
+    ParallelUtilities::WorkFuncTy WorkFun = [&](BinaryFunction &BF) {
+        runOnFunction(BF);
+    };
+
+    ParallelUtilities::PredicateTy SkipFunc = [&](const BinaryFunction &BF) {
+        return false;
+    };
+    
+    ParallelUtilities::PredicateTy SkipPseudo = [&](const BinaryFunction &BF) {
+        return BF.IsPseudoCopy;
+    };
+    
+    ParallelUtilities::PredicateTy SkipNonPseudo = [&](const BinaryFunction &BF) {
+        return !BF.IsPseudoCopy;
+    };
+
+    if (!opts::InstScavenger) {
+        ParallelUtilities::runOnEachFunction(
+            BC, ParallelUtilities::SchedulingPolicy::SP_INST_LINEAR, PreScanFun,
+            SkipFunc, "PrefetchAndYieldPreScan", /*ForceSequential*/true);
+
+        ParallelUtilities::runOnEachFunction(
+            BC, ParallelUtilities::SchedulingPolicy::SP_INST_LINEAR, PreInstrumentFun,
+            SkipFunc, "PrefetchAndYieldPreInst", /*ForceSequential*/true);
+
+        // for each injected function
+        for (auto BF : BC.getInjectedBinaryFunctions()) {
+            if (BF->IsPseudoCopy) {
+                //print functioname
+                runOnFunction(*BF);
+            }
+        }
+    }
+    ParallelUtilities::runOnEachFunction(
+        BC, ParallelUtilities::SchedulingPolicy::SP_INST_LINEAR, WorkFun,
+        SkipFunc, "PrefetchAndYield", /*ForceSequential*/true);
+    BC.HasRelocations = true;
+
+    delete RA;
+}
+}   // end namespace bolt
+}   // end namespace llvm
\ No newline at end of file
diff --git a/bolt/lib/Passes/PnYPrefetchCrtCtx.cpp b/bolt/lib/Passes/PnYPrefetchCrtCtx.cpp
new file mode 100644
index 000000000000..87f2f570b715
--- /dev/null
+++ b/bolt/lib/Passes/PnYPrefetchCrtCtx.cpp
@@ -0,0 +1,104 @@
+#include <stack>
+
+#include "bolt/Passes/PrefetchAndYield.h"
+
+namespace llvm {
+namespace bolt {
+
+static InstructionListType::iterator
+hasCall(BinaryBasicBlock &BB, const BinaryContext &BC,
+        InstructionListType::iterator begin_iter,
+        InstructionListType::iterator end_iter) {
+    InstructionListType::iterator ret_iter = end_iter;
+    for (auto iter = begin_iter; iter != end_iter; ++iter) {
+        MCInst &Inst = *iter;
+        if (BC.MIB->isCall(Inst))
+            ret_iter = iter;
+    }
+    return ret_iter;
+}
+
+IterType PrefetchCrtCtxScanner::annotateInst(IterType Iter, BinaryBasicBlock &BB) {
+    MCInst     &Inst     = *Iter;
+    if (!BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield") &&
+        !BC.MIB->hasAnnotation(Inst, "InsertYield")) {
+        return Iter;
+    }
+    if (!BF.hasLoopInfo())
+        BF.calculateLoopInfo(); 
+    auto &BLI = BF.getLoopInfo();
+    const int Threshold = 10;
+    int InstOffset = std::distance(BB.begin(), Iter);
+    BinaryLoop *Loop     = BLI.getLoopFor(&BB);
+    
+    std::stack<std::pair<BinaryBasicBlock *, int>> Stack;
+    for (auto pred_iter = BB.pred_begin(); pred_iter != BB.pred_end();
+            ++pred_iter) {
+        Stack.push(std::make_pair(*pred_iter, InstOffset));
+    }
+
+    bool FirstElement = true;
+    int NumOfPrefetches = 0;
+    while (!Stack.empty()) {
+        BinaryBasicBlock *CurBB     = Stack.top().first;
+        int               CurOffset = Stack.top().second;
+        Stack.pop();
+        if (CurBB == &BB && !FirstElement) // don't put the initial block again
+            continue;
+
+        FirstElement = false;
+
+        // check if this block has annotation already
+        bool HasAnnotation = false;
+        for (auto iter_prime = CurBB->begin(); iter_prime != CurBB->end(); ++iter_prime) {
+            if (BC.MIB->hasAnnotation(*iter_prime, "InsertPrefetchContext")) {
+                HasAnnotation = true;
+                break;
+            } else if (BC.MIB->hasAnnotation(*iter_prime, "InsertPrefetchAndYield") ||
+                       BC.MIB->hasAnnotation(*iter_prime, "InsertYield")) {
+                HasAnnotation = true;
+                break;
+            }
+        }
+        if (HasAnnotation)
+            continue;
+
+        auto EndIter = FirstElement ? Iter : CurBB->end();
+        auto CallIter = hasCall(*CurBB, BC, CurBB->begin(), EndIter);
+        if (CallIter != EndIter) {
+            // addAnnotation to the instruction coming after call
+            BC.MIB->addAnnotation(*(++CallIter),
+                                    "InsertPrefetchContext", 1);
+            NumOfPrefetches++;
+            continue;
+        }
+
+        if (CurOffset > Threshold) {
+            BC.MIB->addAnnotation(*(CurBB->begin()),
+                                    "InsertPrefetchContext", 1);
+            NumOfPrefetches++;
+            continue;
+        }
+
+        for (auto pred_iter = CurBB->pred_begin();
+                pred_iter != CurBB->pred_end(); ++pred_iter) {
+            Stack.push(std::make_pair(*pred_iter,
+                                        CurOffset + (*pred_iter)->size()));
+        }
+    }
+
+    return Iter;
+}
+
+IterType PrefetchCrtCtxInstrumenter::modifyInst(IterType &Iter,
+                                                BinaryBasicBlock &BB) {
+    if (!BC.MIB->hasAnnotation(*Iter, "InsertPrefetchContext"))
+        return Iter;
+
+    InstructionListType Instrs;
+    BC.MIB->createPrefetchContext(Instrs, BC.Ctx.get());
+    return insertInstructions(Instrs, BB, Iter);
+}
+
+} // end namespace bolt
+} // end namespace llvm
\ No newline at end of file
diff --git a/bolt/lib/Passes/PnYPseudoInline.cpp b/bolt/lib/Passes/PnYPseudoInline.cpp
new file mode 100644
index 000000000000..852ae3cf3e63
--- /dev/null
+++ b/bolt/lib/Passes/PnYPseudoInline.cpp
@@ -0,0 +1,250 @@
+#include "bolt/Passes/PrefetchAndYield.h"
+
+// Currently PseudoInliner supports a limited case, when
+// (1) a callee is called in a loop
+// (2) a callee directly contains PnY points.
+// It doesn't support recursive case.
+
+namespace llvm {
+namespace bolt {
+
+static const MCSymbol *
+getOriginalSymbolFromPLT(const MCSymbol *PLTSymbol, BinaryContext &BC) {
+    std::string CalleeName = PLTSymbol->getName().str();
+    size_t      pos        = CalleeName.find("@PLT");
+    if (pos != std::string::npos) {
+        CalleeName = CalleeName.substr(0, pos);
+        return BC.Ctx->getOrCreateSymbol(CalleeName);
+    }
+    return PLTSymbol;
+}
+
+static void
+patchLocalSymbol(MCInst                                       &Inst,
+                 std::map<const MCSymbol *, const MCSymbol *> &NamePatchMap,
+                 BinaryContext                                &BC) {
+    for (auto &Op : Inst) {
+        // print Inst type
+        if (Op.isExpr()) {
+            const MCExpr *Expr = Op.getExpr();
+
+            if (Expr->getKind() == MCExpr::SymbolRef) {
+                const MCSymbolRefExpr *SymbolRef =
+                    static_cast<const MCSymbolRefExpr *>(Expr);
+                const MCSymbol *Symbol = &SymbolRef->getSymbol();
+
+                if (NamePatchMap.count(Symbol)) {
+                    const MCSymbol *NewSymbol = NamePatchMap[Symbol];
+                    const MCExpr   *NewExpr =
+                        MCSymbolRefExpr::create(NewSymbol, *BC.Ctx);
+                    Op.setExpr(NewExpr);
+                }
+            }
+        }
+    }
+}
+
+void
+PseudoInlinePreScanner::annotateInst(MCInst &Inst, BinaryBasicBlock &BB) {
+    if (!BC.MIB->isCall(Inst))
+        return;
+
+    if (!BF.hasLoopInfo())
+        BF.calculateLoopInfo();
+    auto       &BLI  = BF.getLoopInfo();
+    BinaryLoop *Loop = BLI.getLoopFor(&BB);
+    if (Loop == nullptr) {   // not a target
+        return;
+    }
+
+    const MCSymbol *CalleeSymbol = BC.MIB->getTargetSymbol(Inst);
+    auto           *CalleeBF     = BC.getFunctionForSymbol(CalleeSymbol);
+
+    if (!CalleeBF) {
+        return;
+    }
+
+    if (CalleeBF->getPLTSymbol()) {
+        CalleeSymbol = getOriginalSymbolFromPLT(CalleeSymbol, BC);
+        if (!CalleeSymbol) {
+            return;
+        }
+        CalleeBF = BC.getFunctionForSymbol(CalleeSymbol);
+        if (!CalleeBF) {
+            return;
+        }
+    }
+
+    if (!CalleeBF->HasPnYPoint) {
+        return;
+    }
+
+    // no resursive call support
+    if (BF.getSymbol()->getName() == CalleeSymbol->getName()) {
+        return;
+    }
+
+    PY_LOG("==PseudoInline target: " << CalleeSymbol->getName().str() << "\n")
+    BC.MIB->addAnnotation(Inst, "PseudoInline", 1);
+    StatPseudoInlineNum++;
+}
+
+void
+PseudoInlinePreScanner::printStat() {
+    PY_LOG("PseudoInlinePreScanner: \n")
+    PY_LOG("    # of pseudo inline: " << StatPseudoInlineNum << "\n")
+}
+
+void
+PseudoInlineScanner::annotateInst(MCInst &Inst, BinaryBasicBlock &BB) {
+    // dependency: FUR scan
+    if (!BC.MIB->hasAnnotation(Inst, "PseudoInline"))
+        return;
+
+    const MCSymbol *CalleeSymbol = BC.MIB->getTargetSymbol(Inst);
+    auto           *CalleeBF     = BC.getFunctionForSymbol(CalleeSymbol);
+    assert(CalleeBF);
+    assert(CalleeBF->IsPseudoCopy);
+
+    BitVector RegsFromPseudoInline = CalleeBF->getFilteredRegs();
+    PY_LOG("Scanner: RegsFromPseudoInline: " << RegsFromPseudoInline.count()
+                                             << "\n")
+    BC.MIB->addAnnotation<BitVector>(Inst, "RegsFromPseudoInline",
+                                     RegsFromPseudoInline);
+}
+
+IterType
+PseudoInlinePreInstrumenter::modifyInst(IterType &Iter, BinaryBasicBlock &BB) {
+
+    if (!BC.MIB->hasAnnotation(*Iter, "PseudoInline")) {
+        return Iter;
+    }
+
+    // argument sanity check is already done in scan phase
+    const MCSymbol *CalleeSymbol = BC.MIB->getTargetSymbol(*Iter);
+    auto           *CalleeBF     = BC.getFunctionForSymbol(CalleeSymbol);
+
+    if (CalleeBF->getPLTSymbol()) {
+        CalleeSymbol = getOriginalSymbolFromPLT(CalleeSymbol, BC);
+        CalleeBF     = BC.getFunctionForSymbol(CalleeSymbol);
+    }
+
+    // setup PseudoInlined copy
+    // TODO: need to patch local jump target
+    if (!CalleeBF->PseudoCopy) {
+        std::string     Title = CalleeSymbol->getName().str() + "_PseudoCopy";
+        BinaryFunction *Func  = BC.createInjectedBinaryFunction(Title);
+        std::vector<std::unique_ptr<BinaryBasicBlock>> BBs;
+        for (auto &BB : *CalleeBF) {
+            BBs.emplace_back(Func->createBasicBlock());
+            for (auto Inst : BB) { // copy by value
+                if (BC.MIB->isCFI(Inst))
+                    continue;
+                bool PnYTarget = BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield");
+                bool YTarget = BC.MIB->hasAnnotation(Inst, "InsertYield");
+                BC.MIB->stripAnnotations(Inst);
+                if (PnYTarget)
+                    BC.MIB->addAnnotation(Inst, "InsertPrefetchAndYield", 1);
+                if (YTarget)
+                    BC.MIB->addAnnotation(Inst, "InsertYield", 1);
+                BBs.back()->addInstruction(Inst);
+            }
+            BBs.back()->setCFIState(0);
+            BBs.back()->setOffset(BinaryBasicBlock::INVALID_OFFSET);
+
+            NamePatchMap[BB.getLabel()] = BBs.back()->getLabel();
+            // print mapping
+            //errs() << BB.getName() << " -> " << BBs.back()->getName() << "\n";
+        }
+
+        Func->insertBasicBlocks(nullptr, std::move(BBs),
+                                /*UpdateLayout=*/true,
+                                /*UpdateCFIState=*/false);
+        for (auto &BB : *Func) {
+            for (auto &Inst : BB) {
+                patchLocalSymbol(Inst, NamePatchMap, BC);
+            }
+        }
+        for (auto &BB : *CalleeBF) {
+            // StringRef to MCSymbol
+            const MCSymbol   *NewBBLabel = NamePatchMap[BB.getLabel()];
+            BinaryBasicBlock *NewBB = Func->getBasicBlockForLabel(NewBBLabel);
+
+            assert(NewBB);
+
+            // for each successor
+            for (auto &Succ : BB.successors()) {
+                const MCSymbol   *NewSuccLabel = NamePatchMap[Succ->getLabel()];
+                BinaryBasicBlock *NewSucc =
+                    Func->getBasicBlockForLabel(NewSuccLabel);
+                assert(NewSucc);
+                NewBB->addSuccessor(NewSucc);
+            }
+        }
+
+        Func->updateState(BinaryFunction::State::CFG_Finalized);
+        Func->IsPseudoCopy   = true;
+        Func->HasPnYPoint    = true;
+        CalleeBF->PseudoCopy = Func;
+    }
+
+    // change call target to PseudoCopy
+    MCInst              NewCallInst;
+    InstructionListType Seq;
+    BC.MIB->createCall(NewCallInst, CalleeBF->PseudoCopy->getSymbol(),
+                       BC.Ctx.get());
+    BC.MIB->addAnnotation(NewCallInst, "PseudoInline", 1);
+    Seq.push_back(NewCallInst);
+    IterType ret = BB.replaceInstruction(Iter, Seq.begin(), Seq.end());
+
+    return ret;
+}
+
+// exception: pseudoinline inside pseudoinline?
+IterType
+PseudoInlineInstrumenter::modifyInst(IterType &Iter, BinaryBasicBlock &BB) {
+    if (!BC.MIB->hasAnnotation(*Iter, "PseudoInline"))
+        return Iter;
+
+    assert(!BF.IsPseudoCopy);
+
+    // get Callee BF
+    const MCSymbol *CalleeSymbol = BC.MIB->getTargetSymbol(*Iter);
+    auto           *CalleeBF     = BC.getFunctionForSymbol(CalleeSymbol);
+    assert(CalleeBF);
+    assert(CalleeBF->IsPseudoCopy);
+
+    BitVector RegsFromPseudoInline =
+        BC.MIB->getAnnotationAs<BitVector>(*Iter, "RegsFromPseudoInline");
+    PY_LOG("RegsFromPseudoInline: " << RegsFromPseudoInline.count() << "\n")
+
+    // create push
+    InstructionListType PushInstrs;
+    for (int i = 1; i < RegsFromPseudoInline.size(); i++) {
+        if (RegsFromPseudoInline[i]) {
+            MCInst PushInst;
+            BC.MIB->createPushRegister(PushInst, i, 8);
+            PushInstrs.push_back(PushInst);
+        }
+    }
+
+    IterType NextIter = insertInstructions(PushInstrs, BB, Iter);
+    // create pop and add after next instruction
+    
+    NextIter++;
+    InstructionListType PopInstrs;
+    for (int i = RegsFromPseudoInline.size() - 1; i > 0; i--) {
+        if (RegsFromPseudoInline[i]) {
+            MCInst PopInst;
+            BC.MIB->createPopRegister(PopInst, i, 8);
+            PopInstrs.push_back(PopInst);
+        }
+    }
+
+    IterType Ret = insertInstructions(PopInstrs, BB, NextIter);
+    Ret--;
+    return Ret;
+}
+
+}   // namespace bolt
+}   // namespace llvm
\ No newline at end of file
diff --git a/bolt/lib/Passes/PnYScan.cpp b/bolt/lib/Passes/PnYScan.cpp
new file mode 100644
index 000000000000..e52a61363674
--- /dev/null
+++ b/bolt/lib/Passes/PnYScan.cpp
@@ -0,0 +1,120 @@
+#include "bolt/Passes/DataflowInfoManager.h"
+#include "bolt/Passes/PrefetchAndYield.h"
+
+namespace llvm {
+namespace bolt {
+
+class InitialScanner : public PnYSubScanner {
+  private:
+    BinaryContext      &BC;
+    InstMap            &IM;
+    DataflowInfoManager Info;
+
+  public:
+    InitialScanner(BinaryFunction &BF, InstMap &IM, RegAnalysis *RA)
+        : PnYSubScanner(BF), BC(BF.getBinaryContext()), IM(IM),
+          Info(BF, RA, nullptr) {
+            ScanProgress.InitialScanDone = true;
+          }
+
+  private:
+    void annotateInst(MCInst &Inst, BinaryBasicBlock &BB) override {
+        bool PrefetchTarget =
+            BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield");
+        bool YieldTarget =
+            BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield") ||
+            BC.MIB->hasAnnotation(Inst, "InsertYield");
+
+        if (!PrefetchTarget && !YieldTarget)
+            return;
+
+        //if (BC.MIB->hasAnnotation(Inst, "scanAddr")) {
+        //    return;
+        //}
+
+        //assert(!BC.MIB->hasAnnotation(Inst, "scanAddr"));
+
+        BitVector RegsToSave = *Info.getLivenessAnalysis().getStateAt(Inst);
+        BC.MIB->filterValidRegsInPnY(RegsToSave);
+        //errs() << "Add ScanAddr: " << (uint64_t) &Inst << "\n";
+        BC.MIB->addAnnotation<uint64_t>(Inst, "scanAddr", (uint64_t) &Inst);
+        IMHelper::addInstPoint(IM, &Inst, &BB, RegsToSave); 
+    }
+};
+
+class InitialPreScanner : public PnYSubScanner {
+  private:
+    BinaryContext      &BC;
+
+  public:
+    InitialPreScanner(BinaryFunction &BF)
+        : PnYSubScanner(BF), BC(BF.getBinaryContext()) {
+    }
+  private:
+    void annotateInst(MCInst &Inst, BinaryBasicBlock &BB) override {
+        bool PrefetchTarget =
+            BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield");
+        bool YieldTarget =
+            BC.MIB->hasAnnotation(Inst, "InsertPrefetchAndYield") ||
+            BC.MIB->hasAnnotation(Inst, "InsertYield");
+
+        if (!PrefetchTarget && !YieldTarget)
+            return;
+
+        BF.HasPnYPoint = true;
+    }
+};
+
+// preScan goes through all functions
+// Currently, only reason to prescan is to support pseudoinlining
+void
+PnYScanner::preScan() {
+    InitialPreScanner IPS(BF);
+    IPS.scanInst();
+
+    if (!opts::DisablePseudoInline) {
+        PseudoInlinePreScanner PIS(BF);
+        PIS.scanInst();
+        PIS.printStat();
+    }
+}
+
+// scan is followed by instrument for each function
+// scan-inst / scan-inst / ...
+void
+PnYScanner::scan() { 
+    if (opts::InstScavenger && opts::BoundYieldDistance) {
+        BoundYieldDistanceScanner BYDS(BF, RA);
+        BYDS.scanInst();
+        BYDS.printStat();
+    }
+
+    InitialScanner IS(BF, IM, RA);
+    IS.scanInst();
+
+    if (!opts::DisableFUR) {
+        FilterUnusedRegScanner FURS(BF, IM);
+        FURS.scanInst();
+        FURS.updateState();
+    }
+
+    if (!opts::DisablePseudoInline && !opts::InstScavenger) {
+        PseudoInlineScanner PIS(BF);
+        PIS.scanInst();
+    }
+
+    if (!opts::DisableCtxPrefetch) {
+        PrefetchCrtCtxScanner PCCS(BF);
+        PCCS.scanInstIter();
+    }
+
+    if (!opts::DisableLO) {
+        LoopOptScanner LOS(BF, IM, RA);
+        LOS.scanInst();
+        LOS.printStat();
+    }
+
+}
+
+}   // end namespace bolt
+}   // end namespace llvm
\ No newline at end of file
diff --git a/bolt/lib/Passes/RegAnalysis.cpp b/bolt/lib/Passes/RegAnalysis.cpp
index eab16cb09032..b79d939adbf2 100644
--- a/bolt/lib/Passes/RegAnalysis.cpp
+++ b/bolt/lib/Passes/RegAnalysis.cpp
@@ -239,5 +239,42 @@ void RegAnalysis::printStats() {
                    (100.0 * CountFunctionsAllClobber / CountDenominator));
 }
 
+void RegAnalysis::printMaps() {
+    // print registers in RegsKilledMap and RegsGenMap
+
+    //BitVector GPRs(BC.MRI->getNumRegs(), false);
+    //BC.MIB->getGPRegs(GPRs, false);
+    for (auto &MapEntry : RegsKilledMap) {
+      const BinaryFunction *Func = MapEntry.first;
+      outs() << "Killed regs set for func: " << Func->getPrintName() << "\n";
+      const BitVector &RegsKilledPrim = MapEntry.second;
+      BitVector RegsKilled = RegsKilledPrim;
+      //filter with GPRs
+      //RegsKilled &= GPRs;
+
+      int RegIdx = RegsKilled.find_first();
+      while (RegIdx != -1) {
+        outs() << "\t" << BC.MRI->getName(RegIdx);
+        RegIdx = RegsKilled.find_next(RegIdx);
+      };
+      outs() << "\n";
+    }
+    for (auto &MapEntry : RegsGenMap) {
+      const BinaryFunction *Func = MapEntry.first;
+      outs() << "Used regs set for func: " << Func->getPrintName() << "\n";
+      const BitVector &RegsUsedPrim = MapEntry.second;
+      BitVector RegsUsed = RegsUsedPrim;
+        //filter with GPRs
+      //RegsUsed &= GPRs;
+      int RegIdx = RegsUsed.find_first();
+      while (RegIdx != -1) {
+        outs() << "\t" << BC.MRI->getName(RegIdx);
+        RegIdx = RegsUsed.find_next(RegIdx);
+      };
+      outs() << "\n";
+    }
+
+}
+
 } // namespace bolt
 } // namespace llvm
diff --git a/bolt/lib/Passes/ThreeWayBranch.cpp b/bolt/lib/Passes/ThreeWayBranch.cpp
index dc320d53fb68..cf30464b9637 100644
--- a/bolt/lib/Passes/ThreeWayBranch.cpp
+++ b/bolt/lib/Passes/ThreeWayBranch.cpp
@@ -26,6 +26,8 @@ bool ThreeWayBranch::shouldRunOnFunction(BinaryFunction &Function) {
   return true;
 }
 
 void ThreeWayBranch::runOnFunction(BinaryFunction &Function) {
   BinaryContext &BC = Function.getBinaryContext();
   MCContext *Ctx = BC.Ctx.get();
@@ -34,6 +36,10 @@ void ThreeWayBranch::runOnFunction(BinaryFunction &Function) {
   BinaryFunction::BasicBlockOrderType BlockLayout(
       Function.getLayout().block_begin(), Function.getLayout().block_end());
   for (BinaryBasicBlock *BB : BlockLayout) {
+
     // The block must be hot
     if (BB->getExecutionCount() == 0 ||
         BB->getExecutionCount() == BinaryBasicBlock::COUNT_NO_PROFILE)
diff --git a/bolt/lib/Passes/YDPass.cpp b/bolt/lib/Passes/YDPass.cpp
new file mode 100644
index 000000000000..d2ebc809e0e2
--- /dev/null
+++ b/bolt/lib/Passes/YDPass.cpp
@@ -0,0 +1,60 @@
+//#include <sstream>
+//
+//#include "bolt/Core/ParallelUtilities.h"
+//#include "bolt/Passes/BinaryFunctionCallGraph.h"
+//#include "bolt/Passes/BinaryPasses.h"
+//#include "bolt/Passes/DataflowInfoManager.h"
+//#include "bolt/Passes/PrefetchAndYield.h"
+//#include "bolt/Passes/YieldDistanceAnalysis.h"
+//#include "llvm/Support/CommandLine.h"
+//
+//namespace llvm {
+//namespace bolt {
+//
+//void
+//BoundYieldDistance::runOnFunction(BinaryFunction &BF) {
+//    YieldDistanceAnalysis yda(BF);
+//    yda.run();
+//
+//    int YieldPointNum = 0; 
+//    for (auto &BB : BF) {
+//        for (auto &Inst : BB) {
+//            if (yda.isYieldPoint(Inst)) {
+//                YieldPointNum++;
+//            }
+//        }
+//    }
+//
+//    // print with functioname
+//    outs() << BF.getPrintName() << " YieldPointNum: " << YieldPointNum << "\n";
+//
+//    // instrument... 
+//    //PnYScanner s(BF, RA);
+//    //s.scan();
+//
+//    //PnYInstrumenter i(s);
+//    //i.instrument();
+//}
+//
+//void
+//BoundYieldDistance::runOnFunctions(BinaryContext &BC) {
+//    auto                                    &BFs = BC.getBinaryFunctions();
+// 
+//    ParallelUtilities::WorkFuncTy WorkFun = [&](BinaryFunction &BF) {
+//        runOnFunction(BF);
+//    };
+//
+//    ParallelUtilities::PredicateTy SkipFunc = [&](const BinaryFunction &BF) {
+//        return false;
+//    };
+//    
+//    ParallelUtilities::runOnEachFunction(
+//        BC, ParallelUtilities::SchedulingPolicy::SP_INST_LINEAR, WorkFun,
+//        SkipFunc, "BoundYieldDistance", /*ForceSequential*/true);
+//    BC.HasRelocations = true;
+//}
+//
+//
+//}   // namespace bolt
+//}   // namespace llvm
+//
\ No newline at end of file
diff --git a/bolt/lib/Rewrite/BinaryPassManager.cpp b/bolt/lib/Rewrite/BinaryPassManager.cpp
index 53a45ab58db3..140a145386bc 100644
--- a/bolt/lib/Rewrite/BinaryPassManager.cpp
+++ b/bolt/lib/Rewrite/BinaryPassManager.cpp
@@ -24,6 +24,7 @@
 #include "bolt/Passes/LoopInversionPass.h"
 #include "bolt/Passes/PLTCall.h"
 #include "bolt/Passes/PatchEntries.h"
+#include "bolt/Passes/PrefetchAndYield.h"
 #include "bolt/Passes/RegReAssign.h"
 #include "bolt/Passes/ReorderData.h"
 #include "bolt/Passes/ReorderFunctions.h"
@@ -51,6 +52,8 @@ extern cl::opt<bool> PrintDynoStats;
 extern cl::opt<bool> DumpDotAll;
 extern cl::opt<std::string> AsmDump;
 extern cl::opt<bolt::PLTCall::OptType> PLT;
+extern cl::opt<std::string> CMPCListFilename;
+extern cl::opt<std::string> LatProfFilename;
 
 static cl::opt<bool>
 DynoStatsAll("dyno-stats-all",
@@ -259,6 +262,9 @@ const char BinaryFunctionPassManager::TimerGroupDesc[] =
 
 void BinaryFunctionPassManager::runPasses() {
   auto &BFs = BC.getBinaryFunctions();
+
+  // [SAM] design: BOLT applies a pass to all functions and then moves on to the next pass!
+  // [SAM] Therefore, each BinaryFunction class should contain all the information needed for the passes.
   for (size_t PassIdx = 0; PassIdx < Passes.size(); PassIdx++) {
     const std::pair<const bool, std::unique_ptr<BinaryFunctionPass>>
         &OptPassPair = Passes[PassIdx];
@@ -312,12 +318,17 @@ void BinaryFunctionPassManager::runPasses() {
   }
 }
 
+// [SAM] This is the place to add our prefetching+yield pass
 void BinaryFunctionPassManager::runAllPasses(BinaryContext &BC) {
   BinaryFunctionPassManager Manager(BC);
 
   const DynoStats InitialDynoStats =
       getDynoStats(BC.getBinaryFunctions(), BC.isAArch64());
 
+  if (!opts::CMPCListFilename.empty())
+    goto skip_all_passes;
+
+
   Manager.registerPass(std::make_unique<AsmDumpPass>(),
                        opts::AsmDump.getNumOccurrences());
 
@@ -453,6 +464,15 @@ void BinaryFunctionPassManager::runAllPasses(BinaryContext &BC) {
     Manager.registerPass(std::make_unique<LongJmpPass>(PrintLongJmp));
   }
 
+skip_all_passes:
+  //Manager.registerPass(std::make_unique<Inliner>(PrintInline));
+
+  Manager.registerPass(std::make_unique<CheckFloatRet>());
+  
+  Manager.registerPass(std::make_unique<PrefetchAndYield>());
+  //if (opts::InstScavenger && !opts::LatProfFilename.empty())
+  //  Manager.registerPass(std::make_unique<BoundYieldDistance>());
+
   // This pass should always run last.*
   Manager.registerPass(std::make_unique<FinalizeFunctions>(PrintFinalized));
 
@@ -471,12 +491,13 @@ void BinaryFunctionPassManager::runAllPasses(BinaryContext &BC) {
   Manager.registerPass(std::make_unique<AssignSections>());
 
   // Patch original function entries
-  if (BC.HasRelocations)
+  //if (BC.HasRelocations)
     Manager.registerPass(std::make_unique<PatchEntries>());
 
   // This pass turns tail calls into jumps which makes them invisible to
   // function reordering. It's unsafe to use any CFG or instruction analysis
   // after this point.
+  
   Manager.registerPass(
       std::make_unique<InstructionLowering>(PrintAfterLowering));
 
diff --git a/bolt/lib/Rewrite/RewriteInstance.cpp b/bolt/lib/Rewrite/RewriteInstance.cpp
index 1c0c68c4cdfe..2cb651774f3e 100644
--- a/bolt/lib/Rewrite/RewriteInstance.cpp
+++ b/bolt/lib/Rewrite/RewriteInstance.cpp
@@ -62,6 +62,7 @@
 #include <memory>
 #include <optional>
 #include <system_error>
+#include <sstream>
 
 #undef  DEBUG_TYPE
 #define DEBUG_TYPE "bolt"
@@ -399,6 +400,16 @@ Error RewriteInstance::setProfile(StringRef Filename) {
   return Error::success();
 }
 
+Error RewriteInstance::setCMPC(StringRef Filename) {
+  return BC->setCMPC(Filename);
+}
+
+Error RewriteInstance::setScavProf(StringRef LatProf, StringRef PredProf) {
+  auto e = BC->setScavProf(LatProf, PredProf);
+  return e;
+}
+
+
 /// Return true if the function \p BF should be disassembled.
 static bool shouldDisassemble(const BinaryFunction &BF) {
   if (BF.isPseudo())
@@ -749,6 +760,7 @@ Error RewriteInstance::run() {
     return Error::success();
   }
 
   selectFunctionsToProcess();
 
   readDebugInfo();
@@ -3122,6 +3134,9 @@ void RewriteInstance::processProfileData() {
 void RewriteInstance::disassembleFunctions() {
   NamedRegionTimer T("disassembleFunctions", "disassemble functions",
                      TimerGroupName, TimerGroupDesc, opts::TimeRewrite);
+
+  // [SAM] Q. Where is BC set up from input binary?
   for (auto &BFI : BC->getBinaryFunctions()) {
     BinaryFunction &Function = BFI.second;
 
@@ -3151,6 +3166,7 @@ void RewriteInstance::disassembleFunctions() {
       continue;
     }
 
+    // [Sam] It seems like "Labels" and "Offset" fields are set in Function object after this call.
     if (!Function.disassemble()) {
       if (opts::processAllFunctions())
         BC->exitWithBugReport("function cannot be properly disassembled. "
@@ -3162,6 +3178,10 @@ void RewriteInstance::disassembleFunctions() {
       // Forcefully ignore the function.
       Function.setIgnored();
       continue;
+    } else {
+        outs() << "BOLT-INFO: disassembled " << Function
+               << "\n";
+ 
     }
 
     if (opts::PrintAll || opts::PrintDisasm)
diff --git a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
index ad80255dcf35..044f3599a364 100644
--- a/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
+++ b/bolt/lib/Target/X86/X86MCPlusBuilder.cpp
@@ -16,6 +16,7 @@
 #include "X86MCSymbolizer.h"
 #include "bolt/Core/MCPlus.h"
 #include "bolt/Core/MCPlusBuilder.h"
+#include "bolt/Passes/DataflowInfoManager.h"
 #include "llvm/BinaryFormat/ELF.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCFixupKindInfo.h"
@@ -37,6 +38,15 @@
 using namespace llvm;
 using namespace bolt;
 
+const uint64_t SizeOfYieldCtx        = 20;
+const uint64_t OffsetToSp            = 0;
+const uint64_t OffsetToIp            = OffsetToSp + 8;
+const uint64_t OffsetToNext          = OffsetToSp + 16 + 2;
+const uint64_t OffsetToSpecialNext   = OffsetToSp + 16;
+const uint64_t SizeOfX86Regs         = 192;
+const uint64_t SizeOfRegset          = 2880;
+const uint64_t OffsetToRegSet        = 35608;
+
 namespace opts {
 
 extern cl::OptionCategory BoltOptCategory;
@@ -46,34 +56,62 @@ static cl::opt<bool> X86StripRedundantAddressSize(
     cl::desc("Remove redundant Address-Size override prefix"), cl::init(true),
     cl::cat(BoltOptCategory));
 
-} // namespace opts
+cl::opt<bool>
+    TestSpecialYield("test-special-yield",
+                     cl::desc("Enable special yield-only instrumentation"),
+                     cl::Optional, cl::init(false), cl::cat(BoltOptCategory));
+
+cl::opt<bool>
+    TestNoJump("test-no-jump",
+                     cl::desc("Enable special yield-only instrumentation"),
+                     cl::Optional, cl::init(false), cl::cat(BoltOptCategory));
+cl::opt<bool>
+    TestNoCmp("test-no-cmp",
+                     cl::desc("Enable special yield-only instrumentation"),
+                     cl::Optional, cl::init(false), cl::cat(BoltOptCategory));
+
+
+
+
+}   // namespace opts
 
 namespace {
 
-unsigned getShortBranchOpcode(unsigned Opcode) {
-  switch (Opcode) {
-  default:
-    return Opcode;
-  case X86::JMP_2: return X86::JMP_1;
-  case X86::JMP_4: return X86::JMP_1;
-  case X86::JCC_2: return X86::JCC_1;
-  case X86::JCC_4: return X86::JCC_1;
-  }
+unsigned
+getShortBranchOpcode(unsigned Opcode) {
+    switch (Opcode) {
+    default:
+        return Opcode;
+    case X86::JMP_2:
+        return X86::JMP_1;
+    case X86::JMP_4:
+        return X86::JMP_1;
+    case X86::JCC_2:
+        return X86::JCC_1;
+    case X86::JCC_4:
+        return X86::JCC_1;
+    }
 }
 
-unsigned getShortArithOpcode(unsigned Opcode) {
-  return X86::getShortOpcodeArith(Opcode);
+unsigned
+getShortArithOpcode(unsigned Opcode) {
+    return X86::getShortOpcodeArith(Opcode);
 }
 
-bool isMOVSX64rm32(const MCInst &Inst) {
-  return Inst.getOpcode() == X86::MOVSX64rm32;
+bool
+isMOVSX64rm32(const MCInst &Inst) {
+    return Inst.getOpcode() == X86::MOVSX64rm32;
 }
 
-bool isADD64rr(const MCInst &Inst) { return Inst.getOpcode() == X86::ADD64rr; }
+bool
+isADD64rr(const MCInst &Inst) {
+    return Inst.getOpcode() == X86::ADD64rr;
+}
 
-bool isADDri(const MCInst &Inst) {
-  return Inst.getOpcode() == X86::ADD64ri32 ||
-         Inst.getOpcode() == X86::ADD64ri8;
+bool
+isADDri(const MCInst &Inst) {
+    return Inst.getOpcode() == X86::ADD64ri32 ||
+           Inst.getOpcode() == X86::ADD64ri8;
 }
 
 #define GET_INSTRINFO_OPERAND_TYPES_ENUM
@@ -82,3537 +120,5117 @@ bool isADDri(const MCInst &Inst) {
 #include "X86GenInstrInfo.inc"
 
 class X86MCPlusBuilder : public MCPlusBuilder {
-public:
-  X86MCPlusBuilder(const MCInstrAnalysis *Analysis, const MCInstrInfo *Info,
-                   const MCRegisterInfo *RegInfo)
-      : MCPlusBuilder(Analysis, Info, RegInfo) {}
-
-  std::unique_ptr<MCSymbolizer>
-  createTargetSymbolizer(BinaryFunction &Function) const override {
-    return std::make_unique<X86MCSymbolizer>(Function);
-  }
-
-  bool isBranch(const MCInst &Inst) const override {
-    return Analysis->isBranch(Inst) && !isTailCall(Inst);
-  }
-
-  bool isNoop(const MCInst &Inst) const override {
-    return X86::isNOP(Inst.getOpcode());
-  }
-
-  unsigned getCondCode(const MCInst &Inst) const override {
-    unsigned Opcode = Inst.getOpcode();
-    if (X86::isJCC(Opcode))
-      return Inst.getOperand(Info->get(Opcode).NumOperands - 1).getImm();
-    return X86::COND_INVALID;
-  }
-
-  unsigned getInvertedCondCode(unsigned CC) const override {
-    switch (CC) {
-    default: return X86::COND_INVALID;
-    case X86::COND_E:  return X86::COND_NE;
-    case X86::COND_NE: return X86::COND_E;
-    case X86::COND_L:  return X86::COND_GE;
-    case X86::COND_LE: return X86::COND_G;
-    case X86::COND_G:  return X86::COND_LE;
-    case X86::COND_GE: return X86::COND_L;
-    case X86::COND_B:  return X86::COND_AE;
-    case X86::COND_BE: return X86::COND_A;
-    case X86::COND_A:  return X86::COND_BE;
-    case X86::COND_AE: return X86::COND_B;
-    case X86::COND_S:  return X86::COND_NS;
-    case X86::COND_NS: return X86::COND_S;
-    case X86::COND_P:  return X86::COND_NP;
-    case X86::COND_NP: return X86::COND_P;
-    case X86::COND_O:  return X86::COND_NO;
-    case X86::COND_NO: return X86::COND_O;
-    }
-  }
-
-  unsigned getCondCodesLogicalOr(unsigned CC1, unsigned CC2) const override {
-    enum DecodedCondCode : uint8_t {
-      DCC_EQUAL = 0x1,
-      DCC_GREATER = 0x2,
-      DCC_LESSER = 0x4,
-      DCC_GREATER_OR_LESSER = 0x6,
-      DCC_UNSIGNED = 0x8,
-      DCC_SIGNED = 0x10,
-      DCC_INVALID = 0x20,
-    };
-
-    auto decodeCondCode = [&](unsigned CC) -> uint8_t {
-      switch (CC) {
-      default: return DCC_INVALID;
-      case X86::COND_E: return DCC_EQUAL;
-      case X86::COND_NE: return DCC_GREATER | DCC_LESSER;
-      case X86::COND_L: return DCC_LESSER | DCC_SIGNED;
-      case X86::COND_LE: return DCC_EQUAL | DCC_LESSER | DCC_SIGNED;
-      case X86::COND_G: return DCC_GREATER | DCC_SIGNED;
-      case X86::COND_GE: return DCC_GREATER | DCC_EQUAL | DCC_SIGNED;
-      case X86::COND_B: return DCC_LESSER | DCC_UNSIGNED;
-      case X86::COND_BE: return DCC_EQUAL | DCC_LESSER | DCC_UNSIGNED;
-      case X86::COND_A: return DCC_GREATER | DCC_UNSIGNED;
-      case X86::COND_AE: return DCC_GREATER | DCC_EQUAL | DCC_UNSIGNED;
-      }
-    };
+  public:
+    X86MCPlusBuilder(const MCInstrAnalysis *Analysis, const MCInstrInfo *Info,
+                     const MCRegisterInfo *RegInfo)
+        : MCPlusBuilder(Analysis, Info, RegInfo) {}
+
+    std::unique_ptr<MCSymbolizer>
+    createTargetSymbolizer(BinaryFunction &Function) const override {
+        return std::make_unique<X86MCSymbolizer>(Function);
+    }
 
-    uint8_t DCC = decodeCondCode(CC1) | decodeCondCode(CC2);
-
-    if (DCC & DCC_INVALID)
-      return X86::COND_INVALID;
-
-    if (DCC & DCC_SIGNED && DCC & DCC_UNSIGNED)
-      return X86::COND_INVALID;
-
-    switch (DCC) {
-    default: return X86::COND_INVALID;
-    case DCC_EQUAL | DCC_LESSER | DCC_SIGNED: return X86::COND_LE;
-    case DCC_EQUAL | DCC_LESSER | DCC_UNSIGNED: return X86::COND_BE;
-    case DCC_EQUAL | DCC_GREATER | DCC_SIGNED: return X86::COND_GE;
-    case DCC_EQUAL | DCC_GREATER | DCC_UNSIGNED: return X86::COND_AE;
-    case DCC_GREATER | DCC_LESSER | DCC_SIGNED: return X86::COND_NE;
-    case DCC_GREATER | DCC_LESSER | DCC_UNSIGNED: return X86::COND_NE;
-    case DCC_GREATER | DCC_LESSER: return X86::COND_NE;
-    case DCC_EQUAL | DCC_SIGNED: return X86::COND_E;
-    case DCC_EQUAL | DCC_UNSIGNED: return X86::COND_E;
-    case DCC_EQUAL: return X86::COND_E;
-    case DCC_LESSER | DCC_SIGNED: return X86::COND_L;
-    case DCC_LESSER | DCC_UNSIGNED: return X86::COND_B;
-    case DCC_GREATER | DCC_SIGNED: return X86::COND_G;
-    case DCC_GREATER | DCC_UNSIGNED: return X86::COND_A;
-    }
-  }
-
-  bool isValidCondCode(unsigned CC) const override {
-    return (CC != X86::COND_INVALID);
-  }
-
-  bool isBreakpoint(const MCInst &Inst) const override {
-    return Inst.getOpcode() == X86::INT3;
-  }
-
-  bool isPrefix(const MCInst &Inst) const override {
-    const MCInstrDesc &Desc = Info->get(Inst.getOpcode());
-    return X86II::isPrefix(Desc.TSFlags);
-  }
-
-  bool isRep(const MCInst &Inst) const override {
-    return Inst.getFlags() == X86::IP_HAS_REPEAT;
-  }
-
-  bool deleteREPPrefix(MCInst &Inst) const override {
-    if (Inst.getFlags() == X86::IP_HAS_REPEAT) {
-      Inst.setFlags(0);
-      return true;
-    }
-    return false;
-  }
-
-  // FIXME: For compatibility with old LLVM only!
-  bool isTerminator(const MCInst &Inst) const override {
-    unsigned Opcode = Inst.getOpcode();
-    return Info->get(Opcode).isTerminator() || X86::isUD1(Opcode) ||
-           X86::isUD2(Opcode);
-  }
-
-  bool isIndirectCall(const MCInst &Inst) const override {
-    return isCall(Inst) &&
-           ((getMemoryOperandNo(Inst) != -1) || Inst.getOperand(0).isReg());
-  }
-
-  bool isPop(const MCInst &Inst) const override {
-    return getPopSize(Inst) == 0 ? false : true;
-  }
-
-  bool isTerminateBranch(const MCInst &Inst) const override {
-    return Inst.getOpcode() == X86::ENDBR32 || Inst.getOpcode() == X86::ENDBR64;
-  }
-
-  int getPopSize(const MCInst &Inst) const override {
-    switch (Inst.getOpcode()) {
-    case X86::POP16r:
-    case X86::POP16rmm:
-    case X86::POP16rmr:
-    case X86::POPF16:
-    case X86::POPA16:
-    case X86::POPDS16:
-    case X86::POPES16:
-    case X86::POPFS16:
-    case X86::POPGS16:
-    case X86::POPSS16:
-      return 2;
-    case X86::POP32r:
-    case X86::POP32rmm:
-    case X86::POP32rmr:
-    case X86::POPA32:
-    case X86::POPDS32:
-    case X86::POPES32:
-    case X86::POPF32:
-    case X86::POPFS32:
-    case X86::POPGS32:
-    case X86::POPSS32:
-      return 4;
-    case X86::POP64r:
-    case X86::POP64rmm:
-    case X86::POP64rmr:
-    case X86::POPF64:
-    case X86::POPFS64:
-    case X86::POPGS64:
-      return 8;
-    }
-    return 0;
-  }
-
-  bool isPush(const MCInst &Inst) const override {
-    return getPushSize(Inst) == 0 ? false : true;
-  }
-
-  int getPushSize(const MCInst &Inst) const override {
-    switch (Inst.getOpcode()) {
-    case X86::PUSH16i8:
-    case X86::PUSH16r:
-    case X86::PUSH16rmm:
-    case X86::PUSH16rmr:
-    case X86::PUSHA16:
-    case X86::PUSHCS16:
-    case X86::PUSHDS16:
-    case X86::PUSHES16:
-    case X86::PUSHF16:
-    case X86::PUSHFS16:
-    case X86::PUSHGS16:
-    case X86::PUSHSS16:
-    case X86::PUSHi16:
-      return 2;
-    case X86::PUSH32i8:
-    case X86::PUSH32r:
-    case X86::PUSH32rmm:
-    case X86::PUSH32rmr:
-    case X86::PUSHA32:
-    case X86::PUSHCS32:
-    case X86::PUSHDS32:
-    case X86::PUSHES32:
-    case X86::PUSHF32:
-    case X86::PUSHFS32:
-    case X86::PUSHGS32:
-    case X86::PUSHSS32:
-    case X86::PUSHi32:
-      return 4;
-    case X86::PUSH64i32:
-    case X86::PUSH64i8:
-    case X86::PUSH64r:
-    case X86::PUSH64rmm:
-    case X86::PUSH64rmr:
-    case X86::PUSHF64:
-    case X86::PUSHFS64:
-    case X86::PUSHGS64:
-      return 8;
-    }
-    return 0;
-  }
-
-  bool isSUB(const MCInst &Inst) const override {
-    return X86::isSUB(Inst.getOpcode());
-  }
-
-  bool isLEA64r(const MCInst &Inst) const override {
-    return Inst.getOpcode() == X86::LEA64r;
-  }
-
-  bool isLeave(const MCInst &Inst) const override {
-    return Inst.getOpcode() == X86::LEAVE || Inst.getOpcode() == X86::LEAVE64;
-  }
-
-  bool isMoveMem2Reg(const MCInst &Inst) const override {
-    switch (Inst.getOpcode()) {
-    case X86::MOV16rm:
-    case X86::MOV32rm:
-    case X86::MOV64rm:
-      return true;
-    }
-    return false;
-  }
-
-  bool isUnsupportedBranch(unsigned Opcode) const override {
-    switch (Opcode) {
-    default:
-      return false;
-    case X86::LOOP:
-    case X86::LOOPE:
-    case X86::LOOPNE:
-    case X86::JECXZ:
-    case X86::JRCXZ:
-      return true;
+    bool isBranch(const MCInst &Inst) const override {
+        return Analysis->isBranch(Inst) && !isTailCall(Inst);
     }
-  }
 
-  bool isLoad(const MCInst &Inst) const override {
-    if (isPop(Inst))
-      return true;
+    bool isNoop(const MCInst &Inst) const override {
+        return X86::isNOP(Inst.getOpcode());
+    }
 
-    int MemOpNo = getMemoryOperandNo(Inst);
-    const MCInstrDesc &MCII = Info->get(Inst.getOpcode());
+    unsigned getCondCode(const MCInst &Inst) const override {
+        unsigned Opcode = Inst.getOpcode();
+        if (X86::isJCC(Opcode))
+            return Inst.getOperand(Info->get(Opcode).NumOperands - 1).getImm();
+        return X86::COND_INVALID;
+    }
 
-    if (MemOpNo == -1)
-      return false;
+    unsigned getInvertedCondCode(unsigned CC) const override {
+        switch (CC) {
+        default:
+            return X86::COND_INVALID;
+        case X86::COND_E:
+            return X86::COND_NE;
+        case X86::COND_NE:
+            return X86::COND_E;
+        case X86::COND_L:
+            return X86::COND_GE;
+        case X86::COND_LE:
+            return X86::COND_G;
+        case X86::COND_G:
+            return X86::COND_LE;
+        case X86::COND_GE:
+            return X86::COND_L;
+        case X86::COND_B:
+            return X86::COND_AE;
+        case X86::COND_BE:
+            return X86::COND_A;
+        case X86::COND_A:
+            return X86::COND_BE;
+        case X86::COND_AE:
+            return X86::COND_B;
+        case X86::COND_S:
+            return X86::COND_NS;
+        case X86::COND_NS:
+            return X86::COND_S;
+        case X86::COND_P:
+            return X86::COND_NP;
+        case X86::COND_NP:
+            return X86::COND_P;
+        case X86::COND_O:
+            return X86::COND_NO;
+        case X86::COND_NO:
+            return X86::COND_O;
+        }
+    }
 
-    return MCII.mayLoad();
-  }
+    unsigned getCondCodesLogicalOr(unsigned CC1, unsigned CC2) const override {
+        enum DecodedCondCode : uint8_t {
+            DCC_EQUAL             = 0x1,
+            DCC_GREATER           = 0x2,
+            DCC_LESSER            = 0x4,
+            DCC_GREATER_OR_LESSER = 0x6,
+            DCC_UNSIGNED          = 0x8,
+            DCC_SIGNED            = 0x10,
+            DCC_INVALID           = 0x20,
+        };
+
+        auto decodeCondCode = [&](unsigned CC) -> uint8_t {
+            switch (CC) {
+            default:
+                return DCC_INVALID;
+            case X86::COND_E:
+                return DCC_EQUAL;
+            case X86::COND_NE:
+                return DCC_GREATER | DCC_LESSER;
+            case X86::COND_L:
+                return DCC_LESSER | DCC_SIGNED;
+            case X86::COND_LE:
+                return DCC_EQUAL | DCC_LESSER | DCC_SIGNED;
+            case X86::COND_G:
+                return DCC_GREATER | DCC_SIGNED;
+            case X86::COND_GE:
+                return DCC_GREATER | DCC_EQUAL | DCC_SIGNED;
+            case X86::COND_B:
+                return DCC_LESSER | DCC_UNSIGNED;
+            case X86::COND_BE:
+                return DCC_EQUAL | DCC_LESSER | DCC_UNSIGNED;
+            case X86::COND_A:
+                return DCC_GREATER | DCC_UNSIGNED;
+            case X86::COND_AE:
+                return DCC_GREATER | DCC_EQUAL | DCC_UNSIGNED;
+            }
+        };
+
+        uint8_t DCC = decodeCondCode(CC1) | decodeCondCode(CC2);
+
+        if (DCC & DCC_INVALID)
+            return X86::COND_INVALID;
+
+        if (DCC & DCC_SIGNED && DCC & DCC_UNSIGNED)
+            return X86::COND_INVALID;
+
+        switch (DCC) {
+        default:
+            return X86::COND_INVALID;
+        case DCC_EQUAL | DCC_LESSER | DCC_SIGNED:
+            return X86::COND_LE;
+        case DCC_EQUAL | DCC_LESSER | DCC_UNSIGNED:
+            return X86::COND_BE;
+        case DCC_EQUAL | DCC_GREATER | DCC_SIGNED:
+            return X86::COND_GE;
+        case DCC_EQUAL | DCC_GREATER | DCC_UNSIGNED:
+            return X86::COND_AE;
+        case DCC_GREATER | DCC_LESSER | DCC_SIGNED:
+            return X86::COND_NE;
+        case DCC_GREATER | DCC_LESSER | DCC_UNSIGNED:
+            return X86::COND_NE;
+        case DCC_GREATER | DCC_LESSER:
+            return X86::COND_NE;
+        case DCC_EQUAL | DCC_SIGNED:
+            return X86::COND_E;
+        case DCC_EQUAL | DCC_UNSIGNED:
+            return X86::COND_E;
+        case DCC_EQUAL:
+            return X86::COND_E;
+        case DCC_LESSER | DCC_SIGNED:
+            return X86::COND_L;
+        case DCC_LESSER | DCC_UNSIGNED:
+            return X86::COND_B;
+        case DCC_GREATER | DCC_SIGNED:
+            return X86::COND_G;
+        case DCC_GREATER | DCC_UNSIGNED:
+            return X86::COND_A;
+        }
+    }
 
-  bool isStore(const MCInst &Inst) const override {
-    if (isPush(Inst))
-      return true;
+    bool isValidCondCode(unsigned CC) const override {
+        return (CC != X86::COND_INVALID);
+    }
 
-    int MemOpNo = getMemoryOperandNo(Inst);
-    const MCInstrDesc &MCII = Info->get(Inst.getOpcode());
+    bool isBreakpoint(const MCInst &Inst) const override {
+        return Inst.getOpcode() == X86::INT3;
+    }
 
-    if (MemOpNo == -1)
-      return false;
+    bool isPrefix(const MCInst &Inst) const override {
+        const MCInstrDesc &Desc = Info->get(Inst.getOpcode());
+        return X86II::isPrefix(Desc.TSFlags);
+    }
 
-    return MCII.mayStore();
-  }
+    bool isRep(const MCInst &Inst) const override {
+        return Inst.getFlags() == X86::IP_HAS_REPEAT;
+    }
 
-  bool isCleanRegXOR(const MCInst &Inst) const override {
-    switch (Inst.getOpcode()) {
-    case X86::XOR16rr:
-    case X86::XOR32rr:
-    case X86::XOR64rr:
-      break;
-    default:
-      return false;
-    }
-    return (Inst.getOperand(0).getReg() == Inst.getOperand(2).getReg());
-  }
-
-  bool isPacked(const MCInst &Inst) const override {
-    const MCInstrDesc &Desc = Info->get(Inst.getOpcode());
-    return (Desc.TSFlags & X86II::OpPrefixMask) == X86II::PD;
-  }
-
-  bool shouldRecordCodeRelocation(uint64_t RelType) const override {
-    switch (RelType) {
-    case ELF::R_X86_64_8:
-    case ELF::R_X86_64_16:
-    case ELF::R_X86_64_32:
-    case ELF::R_X86_64_32S:
-    case ELF::R_X86_64_64:
-    case ELF::R_X86_64_PC8:
-    case ELF::R_X86_64_PC32:
-    case ELF::R_X86_64_PC64:
-    case ELF::R_X86_64_GOTPCRELX:
-    case ELF::R_X86_64_REX_GOTPCRELX:
-      return true;
-    case ELF::R_X86_64_PLT32:
-    case ELF::R_X86_64_GOTPCREL:
-    case ELF::R_X86_64_TPOFF32:
-    case ELF::R_X86_64_GOTTPOFF:
-      return false;
-    default:
-      llvm_unreachable("Unexpected x86 relocation type in code");
+    bool deleteREPPrefix(MCInst &Inst) const override {
+        if (Inst.getFlags() == X86::IP_HAS_REPEAT) {
+            Inst.setFlags(0);
+            return true;
+        }
+        return false;
     }
-  }
 
-  unsigned getTrapFillValue() const override { return 0xCC; }
+    // FIXME: For compatibility with old LLVM only!
+    bool isTerminator(const MCInst &Inst) const override {
+        unsigned Opcode = Inst.getOpcode();
+        return Info->get(Opcode).isTerminator() || X86::isUD1(Opcode) ||
+               X86::isUD2(Opcode);
+    }
 
-  struct IndJmpMatcherFrag1 : MCInstMatcher {
-    std::unique_ptr<MCInstMatcher> Base;
-    std::unique_ptr<MCInstMatcher> Scale;
-    std::unique_ptr<MCInstMatcher> Index;
-    std::unique_ptr<MCInstMatcher> Offset;
+    bool isIndirectCall(const MCInst &Inst) const override {
+        return isCall(Inst) &&
+               ((getMemoryOperandNo(Inst) != -1) || Inst.getOperand(0).isReg());
+    }
 
-    IndJmpMatcherFrag1(std::unique_ptr<MCInstMatcher> Base,
-                       std::unique_ptr<MCInstMatcher> Scale,
-                       std::unique_ptr<MCInstMatcher> Index,
-                       std::unique_ptr<MCInstMatcher> Offset)
-        : Base(std::move(Base)), Scale(std::move(Scale)),
-          Index(std::move(Index)), Offset(std::move(Offset)) {}
+    bool isPop(const MCInst &Inst) const override {
+        return getPopSize(Inst) == 0 ? false : true;
+    }
 
-    bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
-               MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
-      if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
-        return false;
+    bool isTerminateBranch(const MCInst &Inst) const override {
+        return Inst.getOpcode() == X86::ENDBR32 ||
+               Inst.getOpcode() == X86::ENDBR64;
+    }
 
-      if (CurInst->getOpcode() != X86::JMP64m)
-        return false;
+    int getPopSize(const MCInst &Inst) const override {
+        switch (Inst.getOpcode()) {
+        case X86::POP16r:
+        case X86::POP16rmm:
+        case X86::POP16rmr:
+        case X86::POPF16:
+        case X86::POPA16:
+        case X86::POPDS16:
+        case X86::POPES16:
+        case X86::POPFS16:
+        case X86::POPGS16:
+        case X86::POPSS16:
+            return 2;
+        case X86::POP32r:
+        case X86::POP32rmm:
+        case X86::POP32rmr:
+        case X86::POPA32:
+        case X86::POPDS32:
+        case X86::POPES32:
+        case X86::POPF32:
+        case X86::POPFS32:
+        case X86::POPGS32:
+        case X86::POPSS32:
+            return 4;
+        case X86::POP64r:
+        case X86::POP64rmm:
+        case X86::POP64rmr:
+        case X86::POPF64:
+        case X86::POPFS64:
+        case X86::POPGS64:
+            return 8;
+        }
+        return 0;
+    }
 
-      int MemOpNo = MIB.getMemoryOperandNo(*CurInst);
-      if (MemOpNo == -1)
-        return false;
+    bool isPush(const MCInst &Inst) const override {
+        return getPushSize(Inst) == 0 ? false : true;
+    }
 
-      if (!Base->match(MRI, MIB, this->InstrWindow, MemOpNo + X86::AddrBaseReg))
+    bool isPushRBP(const MCInst &Inst) const override {
+        if (isPush(Inst))
+            return Inst.getOperand(0).getReg() == X86::RBP;
         return false;
-      if (!Scale->match(MRI, MIB, this->InstrWindow,
-                        MemOpNo + X86::AddrScaleAmt))
-        return false;
-      if (!Index->match(MRI, MIB, this->InstrWindow,
-                        MemOpNo + X86::AddrIndexReg))
-        return false;
-      if (!Offset->match(MRI, MIB, this->InstrWindow, MemOpNo + X86::AddrDisp))
-        return false;
-      return true;
     }
 
-    void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
-      MIB.addAnnotation(*CurInst, Annotation, true);
-      Base->annotate(MIB, Annotation);
-      Scale->annotate(MIB, Annotation);
-      Index->annotate(MIB, Annotation);
-      Offset->annotate(MIB, Annotation);
+    int getPushSize(const MCInst &Inst) const override {
+        switch (Inst.getOpcode()) {
+        case X86::PUSH16i8:
+        case X86::PUSH16r:
+        case X86::PUSH16rmm:
+        case X86::PUSH16rmr:
+        case X86::PUSHA16:
+        case X86::PUSHCS16:
+        case X86::PUSHDS16:
+        case X86::PUSHES16:
+        case X86::PUSHF16:
+        case X86::PUSHFS16:
+        case X86::PUSHGS16:
+        case X86::PUSHSS16:
+        case X86::PUSHi16:
+            return 2;
+        case X86::PUSH32i8:
+        case X86::PUSH32r:
+        case X86::PUSH32rmm:
+        case X86::PUSH32rmr:
+        case X86::PUSHA32:
+        case X86::PUSHCS32:
+        case X86::PUSHDS32:
+        case X86::PUSHES32:
+        case X86::PUSHF32:
+        case X86::PUSHFS32:
+        case X86::PUSHGS32:
+        case X86::PUSHSS32:
+        case X86::PUSHi32:
+            return 4;
+        case X86::PUSH64i32:
+        case X86::PUSH64i8:
+        case X86::PUSH64r:
+        case X86::PUSH64rmm:
+        case X86::PUSH64rmr:
+        case X86::PUSHF64:
+        case X86::PUSHFS64:
+        case X86::PUSHGS64:
+            return 8;
+        }
+        return 0;
     }
-  };
 
-  std::unique_ptr<MCInstMatcher>
-  matchIndJmp(std::unique_ptr<MCInstMatcher> Base,
-              std::unique_ptr<MCInstMatcher> Scale,
-              std::unique_ptr<MCInstMatcher> Index,
-              std::unique_ptr<MCInstMatcher> Offset) const override {
-    return std::unique_ptr<MCInstMatcher>(
-        new IndJmpMatcherFrag1(std::move(Base), std::move(Scale),
-                               std::move(Index), std::move(Offset)));
-  }
-
-  struct IndJmpMatcherFrag2 : MCInstMatcher {
-    std::unique_ptr<MCInstMatcher> Reg;
+    bool isSUB(const MCInst &Inst) const override {
+        return X86::isSUB(Inst.getOpcode());
+    }
 
-    IndJmpMatcherFrag2(std::unique_ptr<MCInstMatcher> Reg)
-        : Reg(std::move(Reg)) {}
+    bool isLEA64r(const MCInst &Inst) const override {
+        return Inst.getOpcode() == X86::LEA64r;
+    }
 
-    bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
-               MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
-      if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
-        return false;
+    bool isLeave(const MCInst &Inst) const override {
+        return Inst.getOpcode() == X86::LEAVE ||
+               Inst.getOpcode() == X86::LEAVE64;
+    }
 
-      if (CurInst->getOpcode() != X86::JMP64r)
+    bool isMoveMem2Reg(const MCInst &Inst) const override {
+        switch (Inst.getOpcode()) {
+        case X86::MOV16rm:
+        case X86::MOV32rm:
+        case X86::MOV64rm:
+            return true;
+        }
         return false;
-
-      return Reg->match(MRI, MIB, this->InstrWindow, 0);
     }
 
-    void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
-      MIB.addAnnotation(*CurInst, Annotation, true);
-      Reg->annotate(MIB, Annotation);
+    bool isUnsupportedBranch(unsigned Opcode) const override {
+        switch (Opcode) {
+        default:
+            return false;
+        case X86::LOOP:
+        case X86::LOOPE:
+        case X86::LOOPNE:
+        case X86::JECXZ:
+        case X86::JRCXZ:
+            return true;
+        }
     }
-  };
 
-  std::unique_ptr<MCInstMatcher>
-  matchIndJmp(std::unique_ptr<MCInstMatcher> Target) const override {
-    return std::unique_ptr<MCInstMatcher>(
-        new IndJmpMatcherFrag2(std::move(Target)));
-  }
+    bool isLoad(const MCInst &Inst) const override {
+        if (isPop(Inst))
+            return true;
 
-  struct LoadMatcherFrag1 : MCInstMatcher {
-    std::unique_ptr<MCInstMatcher> Base;
-    std::unique_ptr<MCInstMatcher> Scale;
-    std::unique_ptr<MCInstMatcher> Index;
-    std::unique_ptr<MCInstMatcher> Offset;
+        int                MemOpNo = getMemoryOperandNo(Inst);
+        const MCInstrDesc &MCII    = Info->get(Inst.getOpcode());
 
-    LoadMatcherFrag1(std::unique_ptr<MCInstMatcher> Base,
-                     std::unique_ptr<MCInstMatcher> Scale,
-                     std::unique_ptr<MCInstMatcher> Index,
-                     std::unique_ptr<MCInstMatcher> Offset)
-        : Base(std::move(Base)), Scale(std::move(Scale)),
-          Index(std::move(Index)), Offset(std::move(Offset)) {}
+        if (MemOpNo == -1)
+            return false;
 
-    bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
-               MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
-      if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
-        return false;
+        return MCII.mayLoad();
+    }
 
-      if (CurInst->getOpcode() != X86::MOV64rm &&
-          CurInst->getOpcode() != X86::MOVSX64rm32)
+    bool isRetInteger(const MCInst &Inst) const override {
+        if (Inst.getNumOperands() >= 2) {
+            MCOperand op = Inst.getOperand(0);
+            if (op.isReg()) {
+                if (op.getReg() == X86::RAX || op.getReg() == X86::EAX) {
+                    return true;
+                }
+            }
+        }
         return false;
+    }
 
-      int MemOpNo = MIB.getMemoryOperandNo(*CurInst);
-      if (MemOpNo == -1)
+    bool isRetFloat(const MCInst &Inst) const override {
+        if (Inst.getNumOperands() >= 2) {
+            MCOperand op = Inst.getOperand(0);
+            if (op.isReg()) {
+                if (op.getReg() == X86::XMM0
+                    || op.getReg() == X86::XMM1
+                    || op.getReg() == X86::XMM2
+                    || op.getReg() == X86::XMM3) {
+                    return true;
+                }
+            }
+        }
         return false;
+    }
 
-      if (!Base->match(MRI, MIB, this->InstrWindow, MemOpNo + X86::AddrBaseReg))
-        return false;
-      if (!Scale->match(MRI, MIB, this->InstrWindow,
-                        MemOpNo + X86::AddrScaleAmt))
-        return false;
-      if (!Index->match(MRI, MIB, this->InstrWindow,
-                        MemOpNo + X86::AddrIndexReg))
-        return false;
-      if (!Offset->match(MRI, MIB, this->InstrWindow, MemOpNo + X86::AddrDisp))
-        return false;
-      return true;
-    }
-
-    void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
-      MIB.addAnnotation(*CurInst, Annotation, true);
-      Base->annotate(MIB, Annotation);
-      Scale->annotate(MIB, Annotation);
-      Index->annotate(MIB, Annotation);
-      Offset->annotate(MIB, Annotation);
-    }
-  };
-
-  std::unique_ptr<MCInstMatcher>
-  matchLoad(std::unique_ptr<MCInstMatcher> Base,
-            std::unique_ptr<MCInstMatcher> Scale,
-            std::unique_ptr<MCInstMatcher> Index,
-            std::unique_ptr<MCInstMatcher> Offset) const override {
-    return std::unique_ptr<MCInstMatcher>(
-        new LoadMatcherFrag1(std::move(Base), std::move(Scale),
-                             std::move(Index), std::move(Offset)));
-  }
-
-  struct AddMatcher : MCInstMatcher {
-    std::unique_ptr<MCInstMatcher> A;
-    std::unique_ptr<MCInstMatcher> B;
-
-    AddMatcher(std::unique_ptr<MCInstMatcher> A,
-               std::unique_ptr<MCInstMatcher> B)
-        : A(std::move(A)), B(std::move(B)) {}
-
-    bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
-               MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
-      if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
-        return false;
+    bool isPrefetchable(const MCInst &inst, bool &IsWide) const override {
+        // 1. Does operand has memory operand?
+        // I do not assume that we support certain types of operands type (e.g.
+        // rm) because some instructions do not have concept of src/dst operand.
+        // (e.g. CMP [mem], imm) This simplifies the check a lot. I believe that
+        // our profiler filters out instruction well so the memory operand here
+        // always causes load, not store. Note that each instruction can have at
+        // most one memory operand in x86.
+        IsWide = false;
+
+        int MemOpNo = getMemoryOperandNo(inst);
+        if (MemOpNo == -1)
+            return false;
+
+        // 2. is dest operand wide registers?
+        // Those cases are handled individually
+        if (inst.getOperand(0).isReg()) {
+            unsigned reg = inst.getOperand(0).getReg();
+            if (reg == X86::XMM0 || reg == X86::XMM1 || reg == X86::XMM2 ||
+                reg == X86::XMM3 || reg == X86::XMM4 || reg == X86::XMM5 ||
+                reg == X86::XMM6 || reg == X86::XMM7) {
+                IsWide = true;
+                /*
+                switch (inst.getOpcode()) {
+                case X86::MOVSDrm:
+                    break;
+                default:
+                    return false;
+                }*/
+            }
+        }
+        return true;
+    }
+
+    bool isStackBaseSet(const MCInst &inst) const override {
+        // mov %rsp, %rbp
+        switch (inst.getOpcode()) {
+        case X86::MOV64rr:
+            if (inst.getOperand(0).getReg() == X86::RBP &&
+                inst.getOperand(1).getReg() == X86::RSP) {
+                return true;
+            }
+            return false;
 
-      if (CurInst->getOpcode() == X86::ADD64rr ||
-          CurInst->getOpcode() == X86::ADD64rr_DB ||
-          CurInst->getOpcode() == X86::ADD64rr_REV) {
-        if (!A->match(MRI, MIB, this->InstrWindow, 1)) {
-          if (!B->match(MRI, MIB, this->InstrWindow, 1))
+        default:
             return false;
-          return A->match(MRI, MIB, this->InstrWindow, 2);
         }
+    }
 
-        if (B->match(MRI, MIB, this->InstrWindow, 2))
-          return true;
+    bool isStore(const MCInst &Inst) const override {
+        if (isPush(Inst))
+            return true;
 
-        if (!B->match(MRI, MIB, this->InstrWindow, 1))
-          return false;
-        return A->match(MRI, MIB, this->InstrWindow, 2);
-      }
+        int                MemOpNo = getMemoryOperandNo(Inst);
+        const MCInstrDesc &MCII    = Info->get(Inst.getOpcode());
+
+        if (MemOpNo == -1)
+            return false;
 
-      return false;
+        return MCII.mayStore();
     }
 
-    void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
-      MIB.addAnnotation(*CurInst, Annotation, true);
-      A->annotate(MIB, Annotation);
-      B->annotate(MIB, Annotation);
+    bool isCleanRegXOR(const MCInst &Inst) const override {
+        switch (Inst.getOpcode()) {
+        case X86::XOR16rr:
+        case X86::XOR32rr:
+        case X86::XOR64rr:
+            break;
+        default:
+            return false;
+        }
+        return (Inst.getOperand(0).getReg() == Inst.getOperand(2).getReg());
     }
-  };
 
-  std::unique_ptr<MCInstMatcher>
-  matchAdd(std::unique_ptr<MCInstMatcher> A,
-           std::unique_ptr<MCInstMatcher> B) const override {
-    return std::unique_ptr<MCInstMatcher>(
-        new AddMatcher(std::move(A), std::move(B)));
-  }
+    bool isPacked(const MCInst &Inst) const override {
+        const MCInstrDesc &Desc = Info->get(Inst.getOpcode());
+        return (Desc.TSFlags & X86II::OpPrefixMask) == X86II::PD;
+    }
 
-  struct LEAMatcher : MCInstMatcher {
-    std::unique_ptr<MCInstMatcher> Target;
+    bool shouldRecordCodeRelocation(uint64_t RelType) const override {
+        switch (RelType) {
+        case ELF::R_X86_64_8:
+        case ELF::R_X86_64_16:
+        case ELF::R_X86_64_32:
+        case ELF::R_X86_64_32S:
+        case ELF::R_X86_64_64:
+        case ELF::R_X86_64_PC8:
+        case ELF::R_X86_64_PC32:
+        case ELF::R_X86_64_PC64:
+        case ELF::R_X86_64_GOTPCRELX:
+        case ELF::R_X86_64_REX_GOTPCRELX:
+            return true;
+        case ELF::R_X86_64_PLT32:
+        case ELF::R_X86_64_GOTPCREL:
+        case ELF::R_X86_64_TPOFF32:
+        case ELF::R_X86_64_GOTTPOFF:
+            return false;
+        default:
+            llvm_unreachable("Unexpected x86 relocation type in code");
+        }
+    }
 
-    LEAMatcher(std::unique_ptr<MCInstMatcher> Target)
-        : Target(std::move(Target)) {}
+    unsigned getTrapFillValue() const override { return 0xCC; }
+
+    struct IndJmpMatcherFrag1 : MCInstMatcher {
+        std::unique_ptr<MCInstMatcher> Base;
+        std::unique_ptr<MCInstMatcher> Scale;
+        std::unique_ptr<MCInstMatcher> Index;
+        std::unique_ptr<MCInstMatcher> Offset;
+
+        IndJmpMatcherFrag1(std::unique_ptr<MCInstMatcher> Base,
+                           std::unique_ptr<MCInstMatcher> Scale,
+                           std::unique_ptr<MCInstMatcher> Index,
+                           std::unique_ptr<MCInstMatcher> Offset)
+            : Base(std::move(Base)), Scale(std::move(Scale)),
+              Index(std::move(Index)), Offset(std::move(Offset)) {}
+
+        bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
+                   MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
+            if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
+                return false;
+
+            if (CurInst->getOpcode() != X86::JMP64m)
+                return false;
+
+            int MemOpNo = MIB.getMemoryOperandNo(*CurInst);
+            if (MemOpNo == -1)
+                return false;
+
+            if (!Base->match(MRI, MIB, this->InstrWindow,
+                             MemOpNo + X86::AddrBaseReg))
+                return false;
+            if (!Scale->match(MRI, MIB, this->InstrWindow,
+                              MemOpNo + X86::AddrScaleAmt))
+                return false;
+            if (!Index->match(MRI, MIB, this->InstrWindow,
+                              MemOpNo + X86::AddrIndexReg))
+                return false;
+            if (!Offset->match(MRI, MIB, this->InstrWindow,
+                               MemOpNo + X86::AddrDisp))
+                return false;
+            return true;
+        }
 
-    bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
-               MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
-      if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
-        return false;
+        void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
+            MIB.addAnnotation(*CurInst, Annotation, true);
+            Base->annotate(MIB, Annotation);
+            Scale->annotate(MIB, Annotation);
+            Index->annotate(MIB, Annotation);
+            Offset->annotate(MIB, Annotation);
+        }
+    };
 
-      if (CurInst->getOpcode() != X86::LEA64r)
-        return false;
+    std::unique_ptr<MCInstMatcher>
+    matchIndJmp(std::unique_ptr<MCInstMatcher> Base,
+                std::unique_ptr<MCInstMatcher> Scale,
+                std::unique_ptr<MCInstMatcher> Index,
+                std::unique_ptr<MCInstMatcher> Offset) const override {
+        return std::unique_ptr<MCInstMatcher>(
+            new IndJmpMatcherFrag1(std::move(Base), std::move(Scale),
+                                   std::move(Index), std::move(Offset)));
+    }
 
-      if (CurInst->getOperand(1 + X86::AddrScaleAmt).getImm() != 1 ||
-          CurInst->getOperand(1 + X86::AddrIndexReg).getReg() !=
-              X86::NoRegister ||
-          (CurInst->getOperand(1 + X86::AddrBaseReg).getReg() !=
-               X86::NoRegister &&
-           CurInst->getOperand(1 + X86::AddrBaseReg).getReg() != X86::RIP))
-        return false;
+    struct IndJmpMatcherFrag2 : MCInstMatcher {
+        std::unique_ptr<MCInstMatcher> Reg;
 
-      return Target->match(MRI, MIB, this->InstrWindow, 1 + X86::AddrDisp);
-    }
+        IndJmpMatcherFrag2(std::unique_ptr<MCInstMatcher> Reg)
+            : Reg(std::move(Reg)) {}
 
-    void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
-      MIB.addAnnotation(*CurInst, Annotation, true);
-      Target->annotate(MIB, Annotation);
-    }
-  };
+        bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
+                   MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
+            if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
+                return false;
 
-  std::unique_ptr<MCInstMatcher>
-  matchLoadAddr(std::unique_ptr<MCInstMatcher> Target) const override {
-    return std::unique_ptr<MCInstMatcher>(new LEAMatcher(std::move(Target)));
-  }
+            if (CurInst->getOpcode() != X86::JMP64r)
+                return false;
 
-  bool hasPCRelOperand(const MCInst &Inst) const override {
-    for (const MCOperand &Operand : Inst)
-      if (Operand.isReg() && Operand.getReg() == X86::RIP)
-        return true;
-    return false;
-  }
-
-  int getMemoryOperandNo(const MCInst &Inst) const override {
-    unsigned Opcode = Inst.getOpcode();
-    const MCInstrDesc &Desc = Info->get(Opcode);
-    int MemOpNo = X86II::getMemoryOperandNo(Desc.TSFlags);
-    if (MemOpNo >= 0)
-      MemOpNo += X86II::getOperandBias(Desc);
-    return MemOpNo;
-  }
-
-  bool hasEVEXEncoding(const MCInst &Inst) const override {
-    const MCInstrDesc &Desc = Info->get(Inst.getOpcode());
-    return (Desc.TSFlags & X86II::EncodingMask) == X86II::EVEX;
-  }
-
-  bool isMacroOpFusionPair(ArrayRef<MCInst> Insts) const override {
-    const auto *I = Insts.begin();
-    while (I != Insts.end() && isPrefix(*I))
-      ++I;
-    if (I == Insts.end())
-      return false;
-
-    const MCInst &FirstInst = *I;
-    ++I;
-    while (I != Insts.end() && isPrefix(*I))
-      ++I;
-    if (I == Insts.end())
-      return false;
-    const MCInst &SecondInst = *I;
-
-    if (!isConditionalBranch(SecondInst))
-      return false;
-    // Cannot fuse if the first instruction uses RIP-relative memory.
-    if (hasPCRelOperand(FirstInst))
-      return false;
-
-    const X86::FirstMacroFusionInstKind CmpKind =
-        X86::classifyFirstOpcodeInMacroFusion(FirstInst.getOpcode());
-    if (CmpKind == X86::FirstMacroFusionInstKind::Invalid)
-      return false;
-
-    X86::CondCode CC = static_cast<X86::CondCode>(getCondCode(SecondInst));
-    X86::SecondMacroFusionInstKind BranchKind =
-        X86::classifySecondCondCodeInMacroFusion(CC);
-    if (BranchKind == X86::SecondMacroFusionInstKind::Invalid)
-      return false;
-    return X86::isMacroFused(CmpKind, BranchKind);
-  }
-
-  std::optional<X86MemOperand>
-  evaluateX86MemoryOperand(const MCInst &Inst) const override {
-    int MemOpNo = getMemoryOperandNo(Inst);
-    if (MemOpNo < 0)
-      return std::nullopt;
-    unsigned MemOpOffset = static_cast<unsigned>(MemOpNo);
-
-    if (MemOpOffset + X86::AddrSegmentReg >= MCPlus::getNumPrimeOperands(Inst))
-      return std::nullopt;
-
-    const MCOperand &Base = Inst.getOperand(MemOpOffset + X86::AddrBaseReg);
-    const MCOperand &Scale = Inst.getOperand(MemOpOffset + X86::AddrScaleAmt);
-    const MCOperand &Index = Inst.getOperand(MemOpOffset + X86::AddrIndexReg);
-    const MCOperand &Disp = Inst.getOperand(MemOpOffset + X86::AddrDisp);
-    const MCOperand &Segment =
-        Inst.getOperand(MemOpOffset + X86::AddrSegmentReg);
-
-    // Make sure it is a well-formed memory operand.
-    if (!Base.isReg() || !Scale.isImm() || !Index.isReg() ||
-        (!Disp.isImm() && !Disp.isExpr()) || !Segment.isReg())
-      return std::nullopt;
-
-    X86MemOperand MO;
-    MO.BaseRegNum = Base.getReg();
-    MO.ScaleImm = Scale.getImm();
-    MO.IndexRegNum = Index.getReg();
-    MO.DispImm = Disp.isImm() ? Disp.getImm() : 0;
-    MO.DispExpr = Disp.isExpr() ? Disp.getExpr() : nullptr;
-    MO.SegRegNum = Segment.getReg();
-    return MO;
-  }
-
-  bool evaluateMemOperandTarget(const MCInst &Inst, uint64_t &Target,
-                                uint64_t Address,
-                                uint64_t Size) const override {
-    std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Inst);
-    if (!MO)
-      return false;
-
-    // Make sure it's a well-formed addressing we can statically evaluate.
-    if ((MO->BaseRegNum != X86::RIP && MO->BaseRegNum != X86::NoRegister) ||
-        MO->IndexRegNum != X86::NoRegister ||
-        MO->SegRegNum != X86::NoRegister || MO->DispExpr)
-      return false;
-
-    Target = MO->DispImm;
-    if (MO->BaseRegNum == X86::RIP) {
-      assert(Size != 0 && "instruction size required in order to statically "
-                          "evaluate RIP-relative address");
-      Target += Address + Size;
-    }
-    return true;
-  }
-
-  MCInst::iterator getMemOperandDisp(MCInst &Inst) const override {
-    int MemOpNo = getMemoryOperandNo(Inst);
-    if (MemOpNo < 0)
-      return Inst.end();
-    return Inst.begin() + (MemOpNo + X86::AddrDisp);
-  }
-
-  bool replaceMemOperandDisp(MCInst &Inst, MCOperand Operand) const override {
-    MCOperand *OI = getMemOperandDisp(Inst);
-    if (OI == Inst.end())
-      return false;
-    *OI = Operand;
-    return true;
-  }
-
-  /// Get the registers used as function parameters.
-  /// This function is specific to the x86_64 abi on Linux.
-  BitVector getRegsUsedAsParams() const override {
-    BitVector Regs = BitVector(RegInfo->getNumRegs(), false);
-    Regs |= getAliases(X86::RSI);
-    Regs |= getAliases(X86::RDI);
-    Regs |= getAliases(X86::RDX);
-    Regs |= getAliases(X86::RCX);
-    Regs |= getAliases(X86::R8);
-    Regs |= getAliases(X86::R9);
-    return Regs;
-  }
-
-  void getCalleeSavedRegs(BitVector &Regs) const override {
-    Regs |= getAliases(X86::RBX);
-    Regs |= getAliases(X86::RBP);
-    Regs |= getAliases(X86::R12);
-    Regs |= getAliases(X86::R13);
-    Regs |= getAliases(X86::R14);
-    Regs |= getAliases(X86::R15);
-  }
-
-  void getDefaultDefIn(BitVector &Regs) const override {
-    assert(Regs.size() >= RegInfo->getNumRegs() &&
-           "The size of BitVector is less than RegInfo->getNumRegs().");
-    Regs.set(X86::RAX);
-    Regs.set(X86::RCX);
-    Regs.set(X86::RDX);
-    Regs.set(X86::RSI);
-    Regs.set(X86::RDI);
-    Regs.set(X86::R8);
-    Regs.set(X86::R9);
-    Regs.set(X86::XMM0);
-    Regs.set(X86::XMM1);
-    Regs.set(X86::XMM2);
-    Regs.set(X86::XMM3);
-    Regs.set(X86::XMM4);
-    Regs.set(X86::XMM5);
-    Regs.set(X86::XMM6);
-    Regs.set(X86::XMM7);
-  }
-
-  void getDefaultLiveOut(BitVector &Regs) const override {
-    assert(Regs.size() >= RegInfo->getNumRegs() &&
-           "The size of BitVector is less than RegInfo->getNumRegs().");
-    Regs |= getAliases(X86::RAX);
-    Regs |= getAliases(X86::RDX);
-    Regs |= getAliases(X86::RCX);
-    Regs |= getAliases(X86::XMM0);
-    Regs |= getAliases(X86::XMM1);
-  }
-
-  void getGPRegs(BitVector &Regs, bool IncludeAlias) const override {
-    if (IncludeAlias) {
-      Regs |= getAliases(X86::RAX);
-      Regs |= getAliases(X86::RBX);
-      Regs |= getAliases(X86::RBP);
-      Regs |= getAliases(X86::RSI);
-      Regs |= getAliases(X86::RDI);
-      Regs |= getAliases(X86::RDX);
-      Regs |= getAliases(X86::RCX);
-      Regs |= getAliases(X86::R8);
-      Regs |= getAliases(X86::R9);
-      Regs |= getAliases(X86::R10);
-      Regs |= getAliases(X86::R11);
-      Regs |= getAliases(X86::R12);
-      Regs |= getAliases(X86::R13);
-      Regs |= getAliases(X86::R14);
-      Regs |= getAliases(X86::R15);
-      return;
-    }
-    Regs.set(X86::RAX);
-    Regs.set(X86::RBX);
-    Regs.set(X86::RBP);
-    Regs.set(X86::RSI);
-    Regs.set(X86::RDI);
-    Regs.set(X86::RDX);
-    Regs.set(X86::RCX);
-    Regs.set(X86::R8);
-    Regs.set(X86::R9);
-    Regs.set(X86::R10);
-    Regs.set(X86::R11);
-    Regs.set(X86::R12);
-    Regs.set(X86::R13);
-    Regs.set(X86::R14);
-    Regs.set(X86::R15);
-  }
-
-  void getClassicGPRegs(BitVector &Regs) const override {
-    Regs |= getAliases(X86::RAX);
-    Regs |= getAliases(X86::RBX);
-    Regs |= getAliases(X86::RBP);
-    Regs |= getAliases(X86::RSI);
-    Regs |= getAliases(X86::RDI);
-    Regs |= getAliases(X86::RDX);
-    Regs |= getAliases(X86::RCX);
-  }
-
-  void getRepRegs(BitVector &Regs) const override {
-    Regs |= getAliases(X86::RCX);
-  }
-
-  MCPhysReg getAliasSized(MCPhysReg Reg, uint8_t Size) const override {
-    Reg = getX86SubSuperRegister(Reg, Size * 8);
-    assert((Reg != X86::NoRegister) && "Invalid register");
-    return Reg;
-  }
-
-  bool isUpper8BitReg(MCPhysReg Reg) const override {
-    switch (Reg) {
-    case X86::AH:
-    case X86::BH:
-    case X86::CH:
-    case X86::DH:
-      return true;
-    default:
-      return false;
-    }
-  }
-
-  bool cannotUseREX(const MCInst &Inst) const override {
-    switch (Inst.getOpcode()) {
-    case X86::MOV8mr_NOREX:
-    case X86::MOV8rm_NOREX:
-    case X86::MOV8rr_NOREX:
-    case X86::MOVSX32rm8_NOREX:
-    case X86::MOVSX32rr8_NOREX:
-    case X86::MOVZX32rm8_NOREX:
-    case X86::MOVZX32rr8_NOREX:
-    case X86::MOV8mr:
-    case X86::MOV8rm:
-    case X86::MOV8rr:
-    case X86::MOVSX32rm8:
-    case X86::MOVSX32rr8:
-    case X86::MOVZX32rm8:
-    case X86::MOVZX32rr8:
-    case X86::TEST8ri:
-      for (const MCOperand &Operand : MCPlus::primeOperands(Inst)) {
-        if (!Operand.isReg())
-          continue;
-        if (isUpper8BitReg(Operand.getReg()))
-          return true;
-      }
-      [[fallthrough]];
-    default:
-      return false;
-    }
-  }
-
-  static uint8_t getMemDataSize(const MCInst &Inst, int MemOpNo) {
-    using namespace llvm::X86;
-    int OpType = getOperandType(Inst.getOpcode(), MemOpNo);
-    return getMemOperandSize(OpType) / 8;
-  }
-
-  /// Classifying a stack access as *not* "SIMPLE" here means we don't know how
-  /// to change this instruction memory access. It will disable any changes to
-  /// the stack layout, so we can't do the most aggressive form of shrink
-  /// wrapping. We must do so in a way that keeps the original stack layout.
-  /// Otherwise you need to adjust the offset of all instructions accessing the
-  /// stack: we can't do that anymore because there is one instruction that is
-  /// not simple. There are other implications as well. We have heuristics to
-  /// detect when a register is callee-saved and thus eligible for shrink
-  /// wrapping. If you are restoring a register using a non-simple stack access,
-  /// then it is classified as NOT callee-saved, and it disables shrink wrapping
-  /// for *that* register (but not for others).
-  ///
-  /// Classifying a stack access as "size 0" or detecting an indexed memory
-  /// access (to address a vector, for example) here means we know there is a
-  /// stack access, but we can't quite understand how wide is the access in
-  /// bytes. This is very serious because we can't understand how memory
-  /// accesses alias with each other for this function. This will essentially
-  /// disable not only shrink wrapping but all frame analysis, it will fail it
-  /// as "we don't understand this function and we give up on it".
-  bool isStackAccess(const MCInst &Inst, bool &IsLoad, bool &IsStore,
-                     bool &IsStoreFromReg, MCPhysReg &Reg, int32_t &SrcImm,
-                     uint16_t &StackPtrReg, int64_t &StackOffset, uint8_t &Size,
-                     bool &IsSimple, bool &IsIndexed) const override {
-    // Detect simple push/pop cases first
-    if (int Sz = getPushSize(Inst)) {
-      IsLoad = false;
-      IsStore = true;
-      IsStoreFromReg = true;
-      StackPtrReg = X86::RSP;
-      StackOffset = -Sz;
-      Size = Sz;
-      IsSimple = true;
-      if (Inst.getOperand(0).isImm())
-        SrcImm = Inst.getOperand(0).getImm();
-      else if (Inst.getOperand(0).isReg())
-        Reg = Inst.getOperand(0).getReg();
-      else
-        IsSimple = false;
-
-      return true;
-    }
-    if (int Sz = getPopSize(Inst)) {
-      IsLoad = true;
-      IsStore = false;
-      if (Inst.getNumOperands() == 0 || !Inst.getOperand(0).isReg()) {
-        IsSimple = false;
-      } else {
-        Reg = Inst.getOperand(0).getReg();
-        IsSimple = true;
-      }
-      StackPtrReg = X86::RSP;
-      StackOffset = 0;
-      Size = Sz;
-      return true;
-    }
-
-    struct InstInfo {
-      // Size in bytes that Inst loads from memory.
-      uint8_t DataSize;
-      bool IsLoad;
-      bool IsStore;
-      bool StoreFromReg;
-      bool Simple;
-    };
+            return Reg->match(MRI, MIB, this->InstrWindow, 0);
+        }
 
-    InstInfo I;
-    int MemOpNo = getMemoryOperandNo(Inst);
-    const MCInstrDesc &MCII = Info->get(Inst.getOpcode());
-    // If it is not dealing with a memory operand, we discard it
-    if (MemOpNo == -1 || MCII.isCall())
-      return false;
-
-    switch (Inst.getOpcode()) {
-    default: {
-      bool IsLoad = MCII.mayLoad();
-      bool IsStore = MCII.mayStore();
-      // Is it LEA? (deals with memory but is not loading nor storing)
-      if (!IsLoad && !IsStore) {
-        I = {0, IsLoad, IsStore, false, false};
-        break;
-      }
-      uint8_t Sz = getMemDataSize(Inst, MemOpNo);
-      I = {Sz, IsLoad, IsStore, false, false};
-      break;
-    }
-    // Report simple stack accesses
-    case X86::MOV8rm: I = {1, true, false, false, true}; break;
-    case X86::MOV16rm: I = {2, true, false, false, true}; break;
-    case X86::MOV32rm: I = {4, true, false, false, true}; break;
-    case X86::MOV64rm: I = {8, true, false, false, true}; break;
-    case X86::MOV8mr: I = {1, false, true, true, true};  break;
-    case X86::MOV16mr: I = {2, false, true, true, true};  break;
-    case X86::MOV32mr: I = {4, false, true, true, true};  break;
-    case X86::MOV64mr: I = {8, false, true, true, true};  break;
-    case X86::MOV8mi: I = {1, false, true, false, true}; break;
-    case X86::MOV16mi: I = {2, false, true, false, true}; break;
-    case X86::MOV32mi: I = {4, false, true, false, true}; break;
-    } // end switch (Inst.getOpcode())
-
-    std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Inst);
-    if (!MO) {
-      LLVM_DEBUG(dbgs() << "Evaluate failed on ");
-      LLVM_DEBUG(Inst.dump());
-      return false;
-    }
-
-    // Make sure it's a stack access
-    if (MO->BaseRegNum != X86::RBP && MO->BaseRegNum != X86::RSP)
-      return false;
-
-    IsLoad = I.IsLoad;
-    IsStore = I.IsStore;
-    IsStoreFromReg = I.StoreFromReg;
-    Size = I.DataSize;
-    IsSimple = I.Simple;
-    StackPtrReg = MO->BaseRegNum;
-    StackOffset = MO->DispImm;
-    IsIndexed =
-        MO->IndexRegNum != X86::NoRegister || MO->SegRegNum != X86::NoRegister;
-
-    if (!I.Simple)
-      return true;
-
-    // Retrieve related register in simple MOV from/to stack operations.
-    unsigned MemOpOffset = static_cast<unsigned>(MemOpNo);
-    if (I.IsLoad) {
-      MCOperand RegOpnd = Inst.getOperand(0);
-      assert(RegOpnd.isReg() && "unexpected destination operand");
-      Reg = RegOpnd.getReg();
-    } else if (I.IsStore) {
-      MCOperand SrcOpnd =
-          Inst.getOperand(MemOpOffset + X86::AddrSegmentReg + 1);
-      if (I.StoreFromReg) {
-        assert(SrcOpnd.isReg() && "unexpected source operand");
-        Reg = SrcOpnd.getReg();
-      } else {
-        assert(SrcOpnd.isImm() && "unexpected source operand");
-        SrcImm = SrcOpnd.getImm();
-      }
-    }
-
-    return true;
-  }
-
-  void changeToPushOrPop(MCInst &Inst) const override {
-    assert(!isPush(Inst) && !isPop(Inst));
-
-    struct InstInfo {
-      // Size in bytes that Inst loads from memory.
-      uint8_t DataSize;
-      bool IsLoad;
-      bool StoreFromReg;
+        void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
+            MIB.addAnnotation(*CurInst, Annotation, true);
+            Reg->annotate(MIB, Annotation);
+        }
     };
 
-    InstInfo I;
-    switch (Inst.getOpcode()) {
-    default: {
-      llvm_unreachable("Unhandled opcode");
-      return;
-    }
-    case X86::MOV16rm: I = {2, true, false}; break;
-    case X86::MOV32rm: I = {4, true, false}; break;
-    case X86::MOV64rm: I = {8, true, false}; break;
-    case X86::MOV16mr: I = {2, false, true};  break;
-    case X86::MOV32mr: I = {4, false, true};  break;
-    case X86::MOV64mr: I = {8, false, true};  break;
-    case X86::MOV16mi: I = {2, false, false}; break;
-    case X86::MOV32mi: I = {4, false, false}; break;
-    } // end switch (Inst.getOpcode())
-
-    std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Inst);
-    if (!MO) {
-      llvm_unreachable("Evaluate failed");
-      return;
-    }
-    // Make sure it's a stack access
-    if (MO->BaseRegNum != X86::RBP && MO->BaseRegNum != X86::RSP) {
-      llvm_unreachable("Not a stack access");
-      return;
-    }
-
-    unsigned MemOpOffset = getMemoryOperandNo(Inst);
-    unsigned NewOpcode = 0;
-    if (I.IsLoad) {
-      switch (I.DataSize) {
-      case 2: NewOpcode = X86::POP16r; break;
-      case 4: NewOpcode = X86::POP32r; break;
-      case 8: NewOpcode = X86::POP64r; break;
-      default:
-        llvm_unreachable("Unexpected size");
-      }
-      unsigned RegOpndNum = Inst.getOperand(0).getReg();
-      Inst.clear();
-      Inst.setOpcode(NewOpcode);
-      Inst.addOperand(MCOperand::createReg(RegOpndNum));
-    } else {
-      MCOperand SrcOpnd =
-          Inst.getOperand(MemOpOffset + X86::AddrSegmentReg + 1);
-      if (I.StoreFromReg) {
-        switch (I.DataSize) {
-        case 2: NewOpcode = X86::PUSH16r; break;
-        case 4: NewOpcode = X86::PUSH32r; break;
-        case 8: NewOpcode = X86::PUSH64r; break;
-        default:
-          llvm_unreachable("Unexpected size");
+    std::unique_ptr<MCInstMatcher>
+    matchIndJmp(std::unique_ptr<MCInstMatcher> Target) const override {
+        return std::unique_ptr<MCInstMatcher>(
+            new IndJmpMatcherFrag2(std::move(Target)));
+    }
+
+    struct LoadMatcherFrag1 : MCInstMatcher {
+        std::unique_ptr<MCInstMatcher> Base;
+        std::unique_ptr<MCInstMatcher> Scale;
+        std::unique_ptr<MCInstMatcher> Index;
+        std::unique_ptr<MCInstMatcher> Offset;
+
+        LoadMatcherFrag1(std::unique_ptr<MCInstMatcher> Base,
+                         std::unique_ptr<MCInstMatcher> Scale,
+                         std::unique_ptr<MCInstMatcher> Index,
+                         std::unique_ptr<MCInstMatcher> Offset)
+            : Base(std::move(Base)), Scale(std::move(Scale)),
+              Index(std::move(Index)), Offset(std::move(Offset)) {}
+
+        bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
+                   MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
+            if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
+                return false;
+
+            if (CurInst->getOpcode() != X86::MOV64rm &&
+                CurInst->getOpcode() != X86::MOVSX64rm32)
+                return false;
+
+            int MemOpNo = MIB.getMemoryOperandNo(*CurInst);
+            if (MemOpNo == -1)
+                return false;
+
+            if (!Base->match(MRI, MIB, this->InstrWindow,
+                             MemOpNo + X86::AddrBaseReg))
+                return false;
+            if (!Scale->match(MRI, MIB, this->InstrWindow,
+                              MemOpNo + X86::AddrScaleAmt))
+                return false;
+            if (!Index->match(MRI, MIB, this->InstrWindow,
+                              MemOpNo + X86::AddrIndexReg))
+                return false;
+            if (!Offset->match(MRI, MIB, this->InstrWindow,
+                               MemOpNo + X86::AddrDisp))
+                return false;
+            return true;
         }
-        assert(SrcOpnd.isReg() && "Unexpected source operand");
-        unsigned RegOpndNum = SrcOpnd.getReg();
-        Inst.clear();
-        Inst.setOpcode(NewOpcode);
-        Inst.addOperand(MCOperand::createReg(RegOpndNum));
-      } else {
-        switch (I.DataSize) {
-        case 2: NewOpcode = X86::PUSH16i8; break;
-        case 4: NewOpcode = X86::PUSH32i8; break;
-        case 8: NewOpcode = X86::PUSH64i32; break;
-        default:
-          llvm_unreachable("Unexpected size");
+
+        void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
+            MIB.addAnnotation(*CurInst, Annotation, true);
+            Base->annotate(MIB, Annotation);
+            Scale->annotate(MIB, Annotation);
+            Index->annotate(MIB, Annotation);
+            Offset->annotate(MIB, Annotation);
         }
-        assert(SrcOpnd.isImm() && "Unexpected source operand");
-        int64_t SrcImm = SrcOpnd.getImm();
-        Inst.clear();
-        Inst.setOpcode(NewOpcode);
-        Inst.addOperand(MCOperand::createImm(SrcImm));
-      }
-    }
-  }
+    };
 
-  bool isStackAdjustment(const MCInst &Inst) const override {
-    switch (Inst.getOpcode()) {
-    default:
-      return false;
-    case X86::SUB64ri32:
-    case X86::SUB64ri8:
-    case X86::ADD64ri32:
-    case X86::ADD64ri8:
-    case X86::LEA64r:
-      break;
-    }
-
-    const MCInstrDesc &MCII = Info->get(Inst.getOpcode());
-    for (int I = 0, E = MCII.getNumDefs(); I != E; ++I) {
-      const MCOperand &Operand = Inst.getOperand(I);
-      if (Operand.isReg() && Operand.getReg() == X86::RSP)
-        return true;
+    std::unique_ptr<MCInstMatcher>
+    matchLoad(std::unique_ptr<MCInstMatcher> Base,
+              std::unique_ptr<MCInstMatcher> Scale,
+              std::unique_ptr<MCInstMatcher> Index,
+              std::unique_ptr<MCInstMatcher> Offset) const override {
+        return std::unique_ptr<MCInstMatcher>(
+            new LoadMatcherFrag1(std::move(Base), std::move(Scale),
+                                 std::move(Index), std::move(Offset)));
     }
-    return false;
-  }
 
-  bool
-  evaluateStackOffsetExpr(const MCInst &Inst, int64_t &Output,
-                          std::pair<MCPhysReg, int64_t> Input1,
-                          std::pair<MCPhysReg, int64_t> Input2) const override {
+    struct AddMatcher : MCInstMatcher {
+        std::unique_ptr<MCInstMatcher> A;
+        std::unique_ptr<MCInstMatcher> B;
 
-    auto getOperandVal = [&](MCPhysReg Reg) -> ErrorOr<int64_t> {
-      if (Reg == Input1.first)
-        return Input1.second;
-      if (Reg == Input2.first)
-        return Input2.second;
-      return make_error_code(errc::result_out_of_range);
-    };
+        AddMatcher(std::unique_ptr<MCInstMatcher> A,
+                   std::unique_ptr<MCInstMatcher> B)
+            : A(std::move(A)), B(std::move(B)) {}
 
-    switch (Inst.getOpcode()) {
-    default:
-      return false;
+        bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
+                   MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
+            if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
+                return false;
 
-    case X86::SUB64ri32:
-    case X86::SUB64ri8:
-      if (!Inst.getOperand(2).isImm())
-        return false;
-      if (ErrorOr<int64_t> InputVal =
-              getOperandVal(Inst.getOperand(1).getReg()))
-        Output = *InputVal - Inst.getOperand(2).getImm();
-      else
-        return false;
-      break;
-    case X86::ADD64ri32:
-    case X86::ADD64ri8:
-      if (!Inst.getOperand(2).isImm())
-        return false;
-      if (ErrorOr<int64_t> InputVal =
-              getOperandVal(Inst.getOperand(1).getReg()))
-        Output = *InputVal + Inst.getOperand(2).getImm();
-      else
-        return false;
-      break;
-    case X86::ADD64i32:
-      if (!Inst.getOperand(0).isImm())
-        return false;
-      if (ErrorOr<int64_t> InputVal = getOperandVal(X86::RAX))
-        Output = *InputVal + Inst.getOperand(0).getImm();
-      else
-        return false;
-      break;
+            if (CurInst->getOpcode() == X86::ADD64rr ||
+                CurInst->getOpcode() == X86::ADD64rr_DB ||
+                CurInst->getOpcode() == X86::ADD64rr_REV) {
+                if (!A->match(MRI, MIB, this->InstrWindow, 1)) {
+                    if (!B->match(MRI, MIB, this->InstrWindow, 1))
+                        return false;
+                    return A->match(MRI, MIB, this->InstrWindow, 2);
+                }
 
-    case X86::LEA64r: {
-      std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Inst);
-      if (!MO)
-        return false;
+                if (B->match(MRI, MIB, this->InstrWindow, 2))
+                    return true;
 
-      if (MO->BaseRegNum == X86::NoRegister ||
-          MO->IndexRegNum != X86::NoRegister ||
-          MO->SegRegNum != X86::NoRegister || MO->DispExpr)
-        return false;
+                if (!B->match(MRI, MIB, this->InstrWindow, 1))
+                    return false;
+                return A->match(MRI, MIB, this->InstrWindow, 2);
+            }
 
-      if (ErrorOr<int64_t> InputVal = getOperandVal(MO->BaseRegNum))
-        Output = *InputVal + MO->DispImm;
-      else
-        return false;
+            return false;
+        }
 
-      break;
-    }
+        void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
+            MIB.addAnnotation(*CurInst, Annotation, true);
+            A->annotate(MIB, Annotation);
+            B->annotate(MIB, Annotation);
+        }
+    };
+
+    std::unique_ptr<MCInstMatcher>
+    matchAdd(std::unique_ptr<MCInstMatcher> A,
+             std::unique_ptr<MCInstMatcher> B) const override {
+        return std::unique_ptr<MCInstMatcher>(
+            new AddMatcher(std::move(A), std::move(B)));
     }
-    return true;
-  }
 
-  bool isRegToRegMove(const MCInst &Inst, MCPhysReg &From,
-                      MCPhysReg &To) const override {
-    switch (Inst.getOpcode()) {
-    default:
-      return false;
-    case X86::LEAVE:
-    case X86::LEAVE64:
-      To = getStackPointer();
-      From = getFramePointer();
-      return true;
-    case X86::MOV64rr:
-      To = Inst.getOperand(0).getReg();
-      From = Inst.getOperand(1).getReg();
-      return true;
-    }
-  }
-
-  MCPhysReg getStackPointer() const override { return X86::RSP; }
-  MCPhysReg getFramePointer() const override { return X86::RBP; }
-  MCPhysReg getFlagsReg() const override { return X86::EFLAGS; }
-
-  bool escapesVariable(const MCInst &Inst,
-                       bool HasFramePointer) const override {
-    int MemOpNo = getMemoryOperandNo(Inst);
-    const MCInstrDesc &MCII = Info->get(Inst.getOpcode());
-    const unsigned NumDefs = MCII.getNumDefs();
-    static BitVector SPBPAliases(BitVector(getAliases(X86::RSP)) |=
-                                 getAliases(X86::RBP));
-    static BitVector SPAliases(getAliases(X86::RSP));
-
-    // FIXME: PUSH can be technically a leak, but let's ignore this for now
-    // because a lot of harmless prologue code will spill SP to the stack.
-    // Unless push is clearly pushing an object address to the stack as
-    // demonstrated by having a MemOp.
-    bool IsPush = isPush(Inst);
-    if (IsPush && MemOpNo == -1)
-      return false;
-
-    // We use this to detect LEA (has memop but does not access mem)
-    bool AccessMem = MCII.mayLoad() || MCII.mayStore();
-    bool DoesLeak = false;
-    for (int I = 0, E = MCPlus::getNumPrimeOperands(Inst); I != E; ++I) {
-      // Ignore if SP/BP is used to dereference memory -- that's fine
-      if (MemOpNo != -1 && !IsPush && AccessMem && I >= MemOpNo &&
-          I <= MemOpNo + 5)
-        continue;
-      // Ignore if someone is writing to SP/BP
-      if (I < static_cast<int>(NumDefs))
-        continue;
-
-      const MCOperand &Operand = Inst.getOperand(I);
-      if (HasFramePointer && Operand.isReg() && SPBPAliases[Operand.getReg()]) {
-        DoesLeak = true;
-        break;
-      }
-      if (!HasFramePointer && Operand.isReg() && SPAliases[Operand.getReg()]) {
-        DoesLeak = true;
-        break;
-      }
-    }
-
-    // If potential leak, check if it is not just writing to itself/sp/bp
-    if (DoesLeak) {
-      for (int I = 0, E = NumDefs; I != E; ++I) {
-        const MCOperand &Operand = Inst.getOperand(I);
-        if (HasFramePointer && Operand.isReg() &&
-            SPBPAliases[Operand.getReg()]) {
-          DoesLeak = false;
-          break;
-        }
-        if (!HasFramePointer && Operand.isReg() &&
-            SPAliases[Operand.getReg()]) {
-          DoesLeak = false;
-          break;
-        }
-      }
-    }
-    return DoesLeak;
-  }
-
-  bool addToImm(MCInst &Inst, int64_t &Amt, MCContext *Ctx) const override {
-    unsigned ImmOpNo = -1U;
-    int MemOpNo = getMemoryOperandNo(Inst);
-    if (MemOpNo != -1)
-      ImmOpNo = MemOpNo + X86::AddrDisp;
-    else
-      for (unsigned Index = 0; Index < MCPlus::getNumPrimeOperands(Inst);
-           ++Index)
-        if (Inst.getOperand(Index).isImm())
-          ImmOpNo = Index;
-    if (ImmOpNo == -1U)
-      return false;
-
-    MCOperand &Operand = Inst.getOperand(ImmOpNo);
-    Amt += Operand.getImm();
-    Operand.setImm(Amt);
-    // Check for the need for relaxation
-    if (int64_t(Amt) == int64_t(int8_t(Amt)))
-      return true;
-
-    // Relax instruction
-    switch (Inst.getOpcode()) {
-    case X86::SUB64ri8:
-      Inst.setOpcode(X86::SUB64ri32);
-      break;
-    case X86::ADD64ri8:
-      Inst.setOpcode(X86::ADD64ri32);
-      break;
-    default:
-      // No need for relaxation
-      break;
-    }
-    return true;
-  }
-
-  /// TODO: this implementation currently works for the most common opcodes that
-  /// load from memory. It can be extended to work with memory store opcodes as
-  /// well as more memory load opcodes.
-  bool replaceMemOperandWithImm(MCInst &Inst, StringRef ConstantData,
-                                uint64_t Offset) const override {
-    enum CheckSignExt : uint8_t {
-      NOCHECK = 0,
-      CHECK8,
-      CHECK32,
-    };
+    struct LEAMatcher : MCInstMatcher {
+        std::unique_ptr<MCInstMatcher> Target;
 
-    using CheckList = std::vector<std::pair<CheckSignExt, unsigned>>;
-    struct InstInfo {
-      // Size in bytes that Inst loads from memory.
-      uint8_t DataSize;
+        LEAMatcher(std::unique_ptr<MCInstMatcher> Target)
+            : Target(std::move(Target)) {}
 
-      // True when the target operand has to be duplicated because the opcode
-      // expects a LHS operand.
-      bool HasLHS;
+        bool match(const MCRegisterInfo &MRI, MCPlusBuilder &MIB,
+                   MutableArrayRef<MCInst> InInstrWindow, int OpNum) override {
+            if (!MCInstMatcher::match(MRI, MIB, InInstrWindow, OpNum))
+                return false;
 
-      // List of checks and corresponding opcodes to be used. We try to use the
-      // smallest possible immediate value when various sizes are available,
-      // hence we may need to check whether a larger constant fits in a smaller
-      // immediate.
-      CheckList Checks;
+            if (CurInst->getOpcode() != X86::LEA64r)
+                return false;
+
+            if (CurInst->getOperand(1 + X86::AddrScaleAmt).getImm() != 1 ||
+                CurInst->getOperand(1 + X86::AddrIndexReg).getReg() !=
+                    X86::NoRegister ||
+                (CurInst->getOperand(1 + X86::AddrBaseReg).getReg() !=
+                     X86::NoRegister &&
+                 CurInst->getOperand(1 + X86::AddrBaseReg).getReg() !=
+                     X86::RIP))
+                return false;
+
+            return Target->match(MRI, MIB, this->InstrWindow,
+                                 1 + X86::AddrDisp);
+        }
+
+        void annotate(MCPlusBuilder &MIB, StringRef Annotation) override {
+            MIB.addAnnotation(*CurInst, Annotation, true);
+            Target->annotate(MIB, Annotation);
+        }
     };
 
-    InstInfo I;
-
-    switch (Inst.getOpcode()) {
-    default: {
-      switch (getPopSize(Inst)) {
-      case 2:            I = {2, false, {{NOCHECK, X86::MOV16ri}}};  break;
-      case 4:            I = {4, false, {{NOCHECK, X86::MOV32ri}}};  break;
-      case 8:            I = {8, false, {{CHECK32, X86::MOV64ri32},
-                                         {NOCHECK, X86::MOV64rm}}};  break;
-      default:           return false;
-      }
-      break;
-    }
-
-    // MOV
-    case X86::MOV8rm:      I = {1, false, {{NOCHECK, X86::MOV8ri}}};   break;
-    case X86::MOV16rm:     I = {2, false, {{NOCHECK, X86::MOV16ri}}};  break;
-    case X86::MOV32rm:     I = {4, false, {{NOCHECK, X86::MOV32ri}}};  break;
-    case X86::MOV64rm:     I = {8, false, {{CHECK32, X86::MOV64ri32},
-                                           {NOCHECK, X86::MOV64rm}}};  break;
-
-    // MOVZX
-    case X86::MOVZX16rm8:  I = {1, false, {{NOCHECK, X86::MOV16ri}}};  break;
-    case X86::MOVZX32rm8:  I = {1, false, {{NOCHECK, X86::MOV32ri}}};  break;
-    case X86::MOVZX32rm16: I = {2, false, {{NOCHECK, X86::MOV32ri}}};  break;
-
-    // CMP
-    case X86::CMP8rm:      I = {1, false, {{NOCHECK, X86::CMP8ri}}};   break;
-    case X86::CMP16rm:     I = {2, false, {{CHECK8,  X86::CMP16ri8},
-                                           {NOCHECK, X86::CMP16ri}}};  break;
-    case X86::CMP32rm:     I = {4, false, {{CHECK8,  X86::CMP32ri8},
-                                           {NOCHECK, X86::CMP32ri}}};  break;
-    case X86::CMP64rm:     I = {8, false, {{CHECK8,  X86::CMP64ri8},
-                                           {CHECK32, X86::CMP64ri32},
-                                           {NOCHECK, X86::CMP64rm}}};  break;
-
-    // TEST
-    case X86::TEST8mr:     I = {1, false, {{NOCHECK, X86::TEST8ri}}};  break;
-    case X86::TEST16mr:    I = {2, false, {{NOCHECK, X86::TEST16ri}}}; break;
-    case X86::TEST32mr:    I = {4, false, {{NOCHECK, X86::TEST32ri}}}; break;
-    case X86::TEST64mr:    I = {8, false, {{CHECK32, X86::TEST64ri32},
-                                           {NOCHECK, X86::TEST64mr}}}; break;
-
-    // ADD
-    case X86::ADD8rm:      I = {1, true,  {{NOCHECK, X86::ADD8ri}}};   break;
-    case X86::ADD16rm:     I = {2, true,  {{CHECK8,  X86::ADD16ri8},
-                                           {NOCHECK, X86::ADD16ri}}};  break;
-    case X86::ADD32rm:     I = {4, true,  {{CHECK8,  X86::ADD32ri8},
-                                           {NOCHECK, X86::ADD32ri}}};  break;
-    case X86::ADD64rm:     I = {8, true,  {{CHECK8,  X86::ADD64ri8},
-                                           {CHECK32, X86::ADD64ri32},
-                                           {NOCHECK, X86::ADD64rm}}};  break;
-
-    // SUB
-    case X86::SUB8rm:      I = {1, true,  {{NOCHECK, X86::SUB8ri}}};   break;
-    case X86::SUB16rm:     I = {2, true,  {{CHECK8,  X86::SUB16ri8},
-                                           {NOCHECK, X86::SUB16ri}}};  break;
-    case X86::SUB32rm:     I = {4, true,  {{CHECK8,  X86::SUB32ri8},
-                                           {NOCHECK, X86::SUB32ri}}};  break;
-    case X86::SUB64rm:     I = {8, true,  {{CHECK8,  X86::SUB64ri8},
-                                           {CHECK32, X86::SUB64ri32},
-                                           {NOCHECK, X86::SUB64rm}}};  break;
-
-    // AND
-    case X86::AND8rm:      I = {1, true,  {{NOCHECK, X86::AND8ri}}};   break;
-    case X86::AND16rm:     I = {2, true,  {{CHECK8,  X86::AND16ri8},
-                                           {NOCHECK, X86::AND16ri}}};  break;
-    case X86::AND32rm:     I = {4, true,  {{CHECK8,  X86::AND32ri8},
-                                           {NOCHECK, X86::AND32ri}}};  break;
-    case X86::AND64rm:     I = {8, true,  {{CHECK8,  X86::AND64ri8},
-                                           {CHECK32, X86::AND64ri32},
-                                           {NOCHECK, X86::AND64rm}}};  break;
-
-    // OR
-    case X86::OR8rm:       I = {1, true,  {{NOCHECK, X86::OR8ri}}};    break;
-    case X86::OR16rm:      I = {2, true,  {{CHECK8,  X86::OR16ri8},
-                                           {NOCHECK, X86::OR16ri}}};   break;
-    case X86::OR32rm:      I = {4, true,  {{CHECK8,  X86::OR32ri8},
-                                           {NOCHECK, X86::OR32ri}}};   break;
-    case X86::OR64rm:      I = {8, true,  {{CHECK8,  X86::OR64ri8},
-                                           {CHECK32, X86::OR64ri32},
-                                           {NOCHECK, X86::OR64rm}}};   break;
-
-    // XOR
-    case X86::XOR8rm:      I = {1, true,  {{NOCHECK, X86::XOR8ri}}};   break;
-    case X86::XOR16rm:     I = {2, true,  {{CHECK8,  X86::XOR16ri8},
-                                           {NOCHECK, X86::XOR16ri}}};  break;
-    case X86::XOR32rm:     I = {4, true,  {{CHECK8,  X86::XOR32ri8},
-                                           {NOCHECK, X86::XOR32ri}}};  break;
-    case X86::XOR64rm:     I = {8, true,  {{CHECK8,  X86::XOR64ri8},
-                                           {CHECK32, X86::XOR64ri32},
-                                           {NOCHECK, X86::XOR64rm}}};  break;
-    }
-
-    // Compute the immediate value.
-    assert(Offset + I.DataSize <= ConstantData.size() &&
-           "invalid offset for given constant data");
-    int64_t ImmVal =
-        DataExtractor(ConstantData, true, 8).getSigned(&Offset, I.DataSize);
-
-    // Compute the new opcode.
-    unsigned NewOpcode = 0;
-    for (const std::pair<CheckSignExt, unsigned> &Check : I.Checks) {
-      NewOpcode = Check.second;
-      if (Check.first == NOCHECK)
-        break;
-      if (Check.first == CHECK8 && isInt<8>(ImmVal))
-        break;
-      if (Check.first == CHECK32 && isInt<32>(ImmVal))
-        break;
-    }
-    if (NewOpcode == Inst.getOpcode())
-      return false;
-
-    // Modify the instruction.
-    MCOperand ImmOp = MCOperand::createImm(ImmVal);
-    uint32_t TargetOpNum = 0;
-    // Test instruction does not follow the regular pattern of putting the
-    // memory reference of a load (5 MCOperands) last in the list of operands.
-    // Since it is not modifying the register operand, it is not treated as
-    // a destination operand and it is not the first operand as it is in the
-    // other instructions we treat here.
-    if (NewOpcode == X86::TEST8ri || NewOpcode == X86::TEST16ri ||
-        NewOpcode == X86::TEST32ri || NewOpcode == X86::TEST64ri32)
-      TargetOpNum = getMemoryOperandNo(Inst) + X86::AddrNumOperands;
-
-    MCOperand TargetOp = Inst.getOperand(TargetOpNum);
-    Inst.clear();
-    Inst.setOpcode(NewOpcode);
-    Inst.addOperand(TargetOp);
-    if (I.HasLHS)
-      Inst.addOperand(TargetOp);
-    Inst.addOperand(ImmOp);
-
-    return true;
-  }
-
-  /// TODO: this implementation currently works for the most common opcodes that
-  /// load from memory. It can be extended to work with memory store opcodes as
-  /// well as more memory load opcodes.
-  bool replaceMemOperandWithReg(MCInst &Inst, MCPhysReg RegNum) const override {
-    unsigned NewOpcode;
-
-    switch (Inst.getOpcode()) {
-    default: {
-      switch (getPopSize(Inst)) {
-      case 2:            NewOpcode = X86::MOV16rr; break;
-      case 4:            NewOpcode = X86::MOV32rr; break;
-      case 8:            NewOpcode = X86::MOV64rr; break;
-      default:           return false;
-      }
-      break;
-    }
-
-    // MOV
-    case X86::MOV8rm:      NewOpcode = X86::MOV8rr;   break;
-    case X86::MOV16rm:     NewOpcode = X86::MOV16rr;  break;
-    case X86::MOV32rm:     NewOpcode = X86::MOV32rr;  break;
-    case X86::MOV64rm:     NewOpcode = X86::MOV64rr;  break;
-    }
-
-    // Modify the instruction.
-    MCOperand RegOp = MCOperand::createReg(RegNum);
-    MCOperand TargetOp = Inst.getOperand(0);
-    Inst.clear();
-    Inst.setOpcode(NewOpcode);
-    Inst.addOperand(TargetOp);
-    Inst.addOperand(RegOp);
-
-    return true;
-  }
-
-  bool isRedundantMove(const MCInst &Inst) const override {
-    switch (Inst.getOpcode()) {
-    default:
-      return false;
-
-    // MOV
-    case X86::MOV8rr:
-    case X86::MOV16rr:
-    case X86::MOV32rr:
-    case X86::MOV64rr:
-      break;
-    }
-
-    assert(Inst.getOperand(0).isReg() && Inst.getOperand(1).isReg());
-    return Inst.getOperand(0).getReg() == Inst.getOperand(1).getReg();
-  }
-
-  bool requiresAlignedAddress(const MCInst &Inst) const override {
-    const MCInstrDesc &Desc = Info->get(Inst.getOpcode());
-    for (unsigned int I = 0; I < Desc.getNumOperands(); ++I) {
-      const MCOperandInfo &Op = Desc.operands()[I];
-      if (Op.OperandType != MCOI::OPERAND_REGISTER)
-        continue;
-      if (Op.RegClass == X86::VR128RegClassID)
-        return true;
+    std::unique_ptr<MCInstMatcher>
+    matchLoadAddr(std::unique_ptr<MCInstMatcher> Target) const override {
+        return std::unique_ptr<MCInstMatcher>(
+            new LEAMatcher(std::move(Target)));
     }
-    return false;
-  }
-
-  bool convertJmpToTailCall(MCInst &Inst) override {
-    if (isTailCall(Inst))
-      return false;
 
-    int NewOpcode;
-    switch (Inst.getOpcode()) {
-    default:
-      return false;
-    case X86::JMP_1:
-    case X86::JMP_2:
-    case X86::JMP_4:
-      NewOpcode = X86::JMP_4;
-      break;
-    case X86::JMP16m:
-    case X86::JMP32m:
-    case X86::JMP64m:
-      NewOpcode = X86::JMP32m;
-      break;
-    case X86::JMP16r:
-    case X86::JMP32r:
-    case X86::JMP64r:
-      NewOpcode = X86::JMP32r;
-      break;
-    }
-
-    Inst.setOpcode(NewOpcode);
-    setTailCall(Inst);
-    return true;
-  }
-
-  bool convertTailCallToJmp(MCInst &Inst) override {
-    int NewOpcode;
-    switch (Inst.getOpcode()) {
-    default:
-      return false;
-    case X86::JMP_4:
-      NewOpcode = X86::JMP_1;
-      break;
-    case X86::JMP32m:
-      NewOpcode = X86::JMP64m;
-      break;
-    case X86::JMP32r:
-      NewOpcode = X86::JMP64r;
-      break;
-    }
-
-    Inst.setOpcode(NewOpcode);
-    removeAnnotation(Inst, MCPlus::MCAnnotation::kTailCall);
-    clearOffset(Inst);
-    return true;
-  }
-
-  bool convertTailCallToCall(MCInst &Inst) override {
-    int NewOpcode;
-    switch (Inst.getOpcode()) {
-    default:
-      return false;
-    case X86::JMP_4:
-      NewOpcode = X86::CALL64pcrel32;
-      break;
-    case X86::JMP32m:
-      NewOpcode = X86::CALL64m;
-      break;
-    case X86::JMP32r:
-      NewOpcode = X86::CALL64r;
-      break;
-    }
-
-    Inst.setOpcode(NewOpcode);
-    removeAnnotation(Inst, MCPlus::MCAnnotation::kTailCall);
-    return true;
-  }
-
-  bool convertCallToIndirectCall(MCInst &Inst, const MCSymbol *TargetLocation,
-                                 MCContext *Ctx) override {
-    assert((Inst.getOpcode() == X86::CALL64pcrel32 ||
-            (Inst.getOpcode() == X86::JMP_4 && isTailCall(Inst))) &&
-           "64-bit direct (tail) call instruction expected");
-    const auto NewOpcode =
-        (Inst.getOpcode() == X86::CALL64pcrel32) ? X86::CALL64m : X86::JMP32m;
-    Inst.setOpcode(NewOpcode);
-
-    // Replace the first operand and preserve auxiliary operands of
-    // the instruction.
-    Inst.erase(Inst.begin());
-    Inst.insert(Inst.begin(),
-                MCOperand::createReg(X86::NoRegister)); // AddrSegmentReg
-    Inst.insert(Inst.begin(),
-                MCOperand::createExpr(                  // Displacement
-                    MCSymbolRefExpr::create(TargetLocation,
-                                            MCSymbolRefExpr::VK_None, *Ctx)));
-    Inst.insert(Inst.begin(),
-                MCOperand::createReg(X86::NoRegister)); // IndexReg
-    Inst.insert(Inst.begin(),
-                MCOperand::createImm(1));               // ScaleAmt
-    Inst.insert(Inst.begin(),
-                MCOperand::createReg(X86::RIP));        // BaseReg
-
-    return true;
-  }
-
-  void convertIndirectCallToLoad(MCInst &Inst, MCPhysReg Reg) override {
-    bool IsTailCall = isTailCall(Inst);
-    if (IsTailCall)
-      removeAnnotation(Inst, MCPlus::MCAnnotation::kTailCall);
-    if (Inst.getOpcode() == X86::CALL64m ||
-        (Inst.getOpcode() == X86::JMP32m && IsTailCall)) {
-      Inst.setOpcode(X86::MOV64rm);
-      Inst.insert(Inst.begin(), MCOperand::createReg(Reg));
-      return;
-    }
-    if (Inst.getOpcode() == X86::CALL64r ||
-        (Inst.getOpcode() == X86::JMP32r && IsTailCall)) {
-      Inst.setOpcode(X86::MOV64rr);
-      Inst.insert(Inst.begin(), MCOperand::createReg(Reg));
-      return;
-    }
-    LLVM_DEBUG(Inst.dump());
-    llvm_unreachable("not implemented");
-  }
-
-  bool shortenInstruction(MCInst &Inst,
-                          const MCSubtargetInfo &STI) const override {
-    unsigned OldOpcode = Inst.getOpcode();
-    unsigned NewOpcode = OldOpcode;
-
-    int MemOpNo = getMemoryOperandNo(Inst);
-
-    // Check and remove redundant Address-Size override prefix.
-    if (opts::X86StripRedundantAddressSize) {
-      uint64_t TSFlags = Info->get(OldOpcode).TSFlags;
-      unsigned Flags = Inst.getFlags();
-
-      if (!X86_MC::needsAddressSizeOverride(Inst, STI, MemOpNo, TSFlags) &&
-          Flags & X86::IP_HAS_AD_SIZE)
-        Inst.setFlags(Flags ^ X86::IP_HAS_AD_SIZE);
-    }
-
-    // Check and remove EIZ/RIZ. These cases represent ambiguous cases where
-    // SIB byte is present, but no index is used and modrm alone should have
-    // been enough. Converting to NoRegister effectively removes the SIB byte.
-    if (MemOpNo >= 0) {
-      MCOperand &IndexOp =
-          Inst.getOperand(static_cast<unsigned>(MemOpNo) + X86::AddrIndexReg);
-      if (IndexOp.getReg() == X86::EIZ || IndexOp.getReg() == X86::RIZ)
-        IndexOp = MCOperand::createReg(X86::NoRegister);
-    }
-
-    if (isBranch(Inst)) {
-      NewOpcode = getShortBranchOpcode(OldOpcode);
-    } else if (OldOpcode == X86::MOV64ri) {
-      if (Inst.getOperand(MCPlus::getNumPrimeOperands(Inst) - 1).isImm()) {
-        const int64_t Imm =
-            Inst.getOperand(MCPlus::getNumPrimeOperands(Inst) - 1).getImm();
-        if (int64_t(Imm) == int64_t(int32_t(Imm)))
-          NewOpcode = X86::MOV64ri32;
-      }
-    } else {
-      // If it's arithmetic instruction check if signed operand fits in 1 byte.
-      const unsigned ShortOpcode = getShortArithOpcode(OldOpcode);
-      if (ShortOpcode != OldOpcode &&
-          Inst.getOperand(MCPlus::getNumPrimeOperands(Inst) - 1).isImm()) {
-        int64_t Imm =
-            Inst.getOperand(MCPlus::getNumPrimeOperands(Inst) - 1).getImm();
-        if (int64_t(Imm) == int64_t(int8_t(Imm)))
-          NewOpcode = ShortOpcode;
-      }
-    }
-
-    if (NewOpcode == OldOpcode)
-      return false;
-
-    Inst.setOpcode(NewOpcode);
-    return true;
-  }
-
-  bool
-  convertMoveToConditionalMove(MCInst &Inst, unsigned CC, bool AllowStackMemOp,
-                               bool AllowBasePtrStackMemOp) const override {
-    // - Register-register moves are OK
-    // - Stores are filtered out by opcode (no store CMOV)
-    // - Non-stack loads are prohibited (generally unsafe)
-    // - Stack loads are OK if AllowStackMemOp is true
-    // - Stack loads with RBP are OK if AllowBasePtrStackMemOp is true
-    if (isLoad(Inst)) {
-      // If stack memory operands are not allowed, no loads are allowed
-      if (!AllowStackMemOp)
+    bool hasPCRelOperand(const MCInst &Inst) const override {
+        for (const MCOperand &Operand : Inst)
+            if (Operand.isReg() && Operand.getReg() == X86::RIP)
+                return true;
         return false;
+    }
 
-      // If stack memory operands are allowed, check if it's a load from stack
-      bool IsLoad, IsStore, IsStoreFromReg, IsSimple, IsIndexed;
-      MCPhysReg Reg;
-      int32_t SrcImm;
-      uint16_t StackPtrReg;
-      int64_t StackOffset;
-      uint8_t Size;
-      bool IsStackAccess =
-          isStackAccess(Inst, IsLoad, IsStore, IsStoreFromReg, Reg, SrcImm,
-                        StackPtrReg, StackOffset, Size, IsSimple, IsIndexed);
-      // Prohibit non-stack-based loads
-      if (!IsStackAccess)
-        return false;
-      // If stack memory operands are allowed, check if it's RBP-based
-      if (!AllowBasePtrStackMemOp &&
-          RegInfo->isSubRegisterEq(X86::RBP, StackPtrReg))
-        return false;
+    int getMemoryOperandNo(const MCInst &Inst) const override {
+        unsigned           Opcode  = Inst.getOpcode();
+        const MCInstrDesc &Desc    = Info->get(Opcode);
+        int                MemOpNo = X86II::getMemoryOperandNo(Desc.TSFlags);
+        if (MemOpNo >= 0)
+            MemOpNo += X86II::getOperandBias(Desc);
+        return MemOpNo;
     }
 
-    unsigned NewOpcode = 0;
-    switch (Inst.getOpcode()) {
-    case X86::MOV16rr:
-      NewOpcode = X86::CMOV16rr;
-      break;
-    case X86::MOV16rm:
-      NewOpcode = X86::CMOV16rm;
-      break;
-    case X86::MOV32rr:
-      NewOpcode = X86::CMOV32rr;
-      break;
-    case X86::MOV32rm:
-      NewOpcode = X86::CMOV32rm;
-      break;
-    case X86::MOV64rr:
-      NewOpcode = X86::CMOV64rr;
-      break;
-    case X86::MOV64rm:
-      NewOpcode = X86::CMOV64rm;
-      break;
-    default:
-      return false;
-    }
-    Inst.setOpcode(NewOpcode);
-    // Insert CC at the end of prime operands, before annotations
-    Inst.insert(Inst.begin() + MCPlus::getNumPrimeOperands(Inst),
-                MCOperand::createImm(CC));
-    // CMOV is a 3-operand MCInst, so duplicate the destination as src1
-    Inst.insert(Inst.begin(), Inst.getOperand(0));
-    return true;
-  }
-
-  bool lowerTailCall(MCInst &Inst) override {
-    if (Inst.getOpcode() == X86::JMP_4 && isTailCall(Inst)) {
-      Inst.setOpcode(X86::JMP_1);
-      removeAnnotation(Inst, MCPlus::MCAnnotation::kTailCall);
-      return true;
-    }
-    return false;
-  }
-
-  const MCSymbol *getTargetSymbol(const MCInst &Inst,
-                                  unsigned OpNum = 0) const override {
-    if (OpNum >= MCPlus::getNumPrimeOperands(Inst))
-      return nullptr;
-
-    const MCOperand &Op = Inst.getOperand(OpNum);
-    if (!Op.isExpr())
-      return nullptr;
-
-    auto *SymExpr = dyn_cast<MCSymbolRefExpr>(Op.getExpr());
-    if (!SymExpr || SymExpr->getKind() != MCSymbolRefExpr::VK_None)
-      return nullptr;
-
-    return &SymExpr->getSymbol();
-  }
-
-  // This is the same as the base class, but since we are overriding one of
-  // getTargetSymbol's signatures above, we need to override all of them.
-  const MCSymbol *getTargetSymbol(const MCExpr *Expr) const override {
-    return &cast<const MCSymbolRefExpr>(Expr)->getSymbol();
-  }
-
-  bool analyzeBranch(InstructionIterator Begin, InstructionIterator End,
-                     const MCSymbol *&TBB, const MCSymbol *&FBB,
-                     MCInst *&CondBranch,
-                     MCInst *&UncondBranch) const override {
-    auto I = End;
-
-    // Bottom-up analysis
-    while (I != Begin) {
-      --I;
-
-      // Ignore nops and CFIs
-      if (isPseudo(*I))
-        continue;
-
-      // Stop when we find the first non-terminator
-      if (!isTerminator(*I))
-        break;
-
-      if (!isBranch(*I))
-        break;
-
-      // Handle unconditional branches.
-      if ((I->getOpcode() == X86::JMP_1 || I->getOpcode() == X86::JMP_2 ||
-           I->getOpcode() == X86::JMP_4) &&
-          !isTailCall(*I)) {
-        // If any code was seen after this unconditional branch, we've seen
-        // unreachable code. Ignore them.
-        CondBranch = nullptr;
-        UncondBranch = &*I;
-        const MCSymbol *Sym = getTargetSymbol(*I);
-        assert(Sym != nullptr &&
-               "Couldn't extract BB symbol from jump operand");
-        TBB = Sym;
-        continue;
-      }
-
-      // Handle conditional branches and ignore indirect branches
-      if (!isUnsupportedBranch(I->getOpcode()) &&
-          getCondCode(*I) == X86::COND_INVALID) {
-        // Indirect branch
-        return false;
-      }
-
-      if (CondBranch == nullptr) {
-        const MCSymbol *TargetBB = getTargetSymbol(*I);
-        if (TargetBB == nullptr) {
-          // Unrecognized branch target
-          return false;
-        }
-        FBB = TBB;
-        TBB = TargetBB;
-        CondBranch = &*I;
-        continue;
-      }
-
-      llvm_unreachable("multiple conditional branches in one BB");
-    }
-    return true;
-  }
-
-  template <typename Itr>
-  std::pair<IndirectBranchType, MCInst *>
-  analyzePICJumpTable(Itr II, Itr IE, MCPhysReg R1, MCPhysReg R2) const {
-    // Analyze PIC-style jump table code template:
-    //
-    //    lea PIC_JUMP_TABLE(%rip), {%r1|%r2}     <- MemLocInstr
-    //    mov ({%r1|%r2}, %index, 4), {%r2|%r1}
-    //    add %r2, %r1
-    //    jmp *%r1
-    //
-    // (with any irrelevant instructions in-between)
-    //
-    // When we call this helper we've already determined %r1 and %r2, and
-    // reverse instruction iterator \p II is pointing to the ADD instruction.
-    //
-    // PIC jump table looks like following:
-    //
-    //   JT:  ----------
-    //    E1:| L1 - JT  |
-    //       |----------|
-    //    E2:| L2 - JT  |
-    //       |----------|
-    //       |          |
-    //          ......
-    //    En:| Ln - JT  |
-    //        ----------
-    //
-    // Where L1, L2, ..., Ln represent labels in the function.
-    //
-    // The actual relocations in the table will be of the form:
-    //
-    //   Ln - JT
-    //    = (Ln - En) + (En - JT)
-    //    = R_X86_64_PC32(Ln) + En - JT
-    //    = R_X86_64_PC32(Ln + offsetof(En))
-    //
-    LLVM_DEBUG(dbgs() << "Checking for PIC jump table\n");
-    MCInst *MemLocInstr = nullptr;
-    const MCInst *MovInstr = nullptr;
-    while (++II != IE) {
-      MCInst &Instr = *II;
-      const MCInstrDesc &InstrDesc = Info->get(Instr.getOpcode());
-      if (!InstrDesc.hasDefOfPhysReg(Instr, R1, *RegInfo) &&
-          !InstrDesc.hasDefOfPhysReg(Instr, R2, *RegInfo)) {
-        // Ignore instructions that don't affect R1, R2 registers.
-        continue;
-      }
-      if (!MovInstr) {
-        // Expect to see MOV instruction.
-        if (!isMOVSX64rm32(Instr)) {
-          LLVM_DEBUG(dbgs() << "MOV instruction expected.\n");
-          break;
-        }
-
-        // Check if it's setting %r1 or %r2. In canonical form it sets %r2.
-        // If it sets %r1 - rename the registers so we have to only check
-        // a single form.
-        unsigned MovDestReg = Instr.getOperand(0).getReg();
-        if (MovDestReg != R2)
-          std::swap(R1, R2);
-        if (MovDestReg != R2) {
-          LLVM_DEBUG(dbgs() << "MOV instruction expected to set %r2\n");
-          break;
-        }
-
-        // Verify operands for MOV.
-        std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Instr);
-        if (!MO)
-          break;
-        if (MO->BaseRegNum != R1 || MO->ScaleImm != 4 ||
-            MO->IndexRegNum == X86::NoRegister || MO->DispImm != 0 ||
-            MO->SegRegNum != X86::NoRegister)
-          break;
-        MovInstr = &Instr;
-      } else {
-        if (!InstrDesc.hasDefOfPhysReg(Instr, R1, *RegInfo))
-          continue;
-        if (!isLEA64r(Instr)) {
-          LLVM_DEBUG(dbgs() << "LEA instruction expected\n");
-          break;
-        }
-        if (Instr.getOperand(0).getReg() != R1) {
-          LLVM_DEBUG(dbgs() << "LEA instruction expected to set %r1\n");
-          break;
-        }
-
-        // Verify operands for LEA.
-        std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Instr);
+    bool hasEVEXEncoding(const MCInst &Inst) const override {
+        const MCInstrDesc &Desc = Info->get(Inst.getOpcode());
+        return (Desc.TSFlags & X86II::EncodingMask) == X86II::EVEX;
+    }
+
+    bool isMacroOpFusionPair(ArrayRef<MCInst> Insts) const override {
+        const auto *I = Insts.begin();
+        while (I != Insts.end() && isPrefix(*I))
+            ++I;
+        if (I == Insts.end())
+            return false;
+
+        const MCInst &FirstInst = *I;
+        ++I;
+        while (I != Insts.end() && isPrefix(*I))
+            ++I;
+        if (I == Insts.end())
+            return false;
+        const MCInst &SecondInst = *I;
+
+        if (!isConditionalBranch(SecondInst))
+            return false;
+        // Cannot fuse if the first instruction uses RIP-relative memory.
+        if (hasPCRelOperand(FirstInst))
+            return false;
+
+        const X86::FirstMacroFusionInstKind CmpKind =
+            X86::classifyFirstOpcodeInMacroFusion(FirstInst.getOpcode());
+        if (CmpKind == X86::FirstMacroFusionInstKind::Invalid)
+            return false;
+
+        X86::CondCode CC = static_cast<X86::CondCode>(getCondCode(SecondInst));
+        X86::SecondMacroFusionInstKind BranchKind =
+            X86::classifySecondCondCodeInMacroFusion(CC);
+        if (BranchKind == X86::SecondMacroFusionInstKind::Invalid)
+            return false;
+        return X86::isMacroFused(CmpKind, BranchKind);
+    }
+
+    std::optional<X86MemOperand>
+    evaluateX86MemoryOperand(const MCInst &Inst) const override {
+        int MemOpNo = getMemoryOperandNo(Inst);
+        if (MemOpNo < 0)
+            return std::nullopt;
+        unsigned MemOpOffset = static_cast<unsigned>(MemOpNo);
+
+        if (MemOpOffset + X86::AddrSegmentReg >=
+            MCPlus::getNumPrimeOperands(Inst))
+            return std::nullopt;
+
+        const MCOperand &Base = Inst.getOperand(MemOpOffset + X86::AddrBaseReg);
+        const MCOperand &Scale =
+            Inst.getOperand(MemOpOffset + X86::AddrScaleAmt);
+        const MCOperand &Index =
+            Inst.getOperand(MemOpOffset + X86::AddrIndexReg);
+        const MCOperand &Disp = Inst.getOperand(MemOpOffset + X86::AddrDisp);
+        const MCOperand &Segment =
+            Inst.getOperand(MemOpOffset + X86::AddrSegmentReg);
+
+        // Make sure it is a well-formed memory operand.
+        if (!Base.isReg() || !Scale.isImm() || !Index.isReg() ||
+            (!Disp.isImm() && !Disp.isExpr()) || !Segment.isReg())
+            return std::nullopt;
+
+        X86MemOperand MO;
+        MO.BaseRegNum  = Base.getReg();
+        MO.ScaleImm    = Scale.getImm();
+        MO.IndexRegNum = Index.getReg();
+        MO.DispImm     = Disp.isImm() ? Disp.getImm() : 0;
+        MO.DispExpr    = Disp.isExpr() ? Disp.getExpr() : nullptr;
+        MO.SegRegNum   = Segment.getReg();
+        return MO;
+    }
+
+    bool evaluateMemOperandTarget(const MCInst &Inst, uint64_t &Target,
+                                  uint64_t Address,
+                                  uint64_t Size) const override {
+        std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Inst);
         if (!MO)
-          break;
-        if (MO->BaseRegNum != RegInfo->getProgramCounter() ||
+            return false;
+
+        // Make sure it's a well-formed addressing we can statically evaluate.
+        if ((MO->BaseRegNum != X86::RIP && MO->BaseRegNum != X86::NoRegister) ||
             MO->IndexRegNum != X86::NoRegister ||
-            MO->SegRegNum != X86::NoRegister || MO->DispExpr == nullptr)
-          break;
-        MemLocInstr = &Instr;
-        break;
-      }
-    }
-
-    if (!MemLocInstr)
-      return std::make_pair(IndirectBranchType::UNKNOWN, nullptr);
-
-    LLVM_DEBUG(dbgs() << "checking potential PIC jump table\n");
-    return std::make_pair(IndirectBranchType::POSSIBLE_PIC_JUMP_TABLE,
-                          MemLocInstr);
-  }
-
-  IndirectBranchType analyzeIndirectBranch(
-      MCInst &Instruction, InstructionIterator Begin, InstructionIterator End,
-      const unsigned PtrSize, MCInst *&MemLocInstrOut, unsigned &BaseRegNumOut,
-      unsigned &IndexRegNumOut, int64_t &DispValueOut,
-      const MCExpr *&DispExprOut, MCInst *&PCRelBaseOut) const override {
-    // Try to find a (base) memory location from where the address for
-    // the indirect branch is loaded. For X86-64 the memory will be specified
-    // in the following format:
-    //
-    //   {%rip}/{%basereg} + Imm + IndexReg * Scale
-    //
-    // We are interested in the cases where Scale == sizeof(uintptr_t) and
-    // the contents of the memory are presumably an array of pointers to code.
-    //
-    // Normal jump table:
-    //
-    //    jmp *(JUMP_TABLE, %index, Scale)        <- MemLocInstr
-    //
-    //    or
-    //
-    //    mov (JUMP_TABLE, %index, Scale), %r1    <- MemLocInstr
-    //    ...
-    //    jmp %r1
-    //
-    // We handle PIC-style jump tables separately.
-    //
-    MemLocInstrOut = nullptr;
-    BaseRegNumOut = X86::NoRegister;
-    IndexRegNumOut = X86::NoRegister;
-    DispValueOut = 0;
-    DispExprOut = nullptr;
-
-    std::reverse_iterator<InstructionIterator> II(End);
-    std::reverse_iterator<InstructionIterator> IE(Begin);
-
-    IndirectBranchType Type = IndirectBranchType::UNKNOWN;
-
-    // An instruction referencing memory used by jump instruction (directly or
-    // via register). This location could be an array of function pointers
-    // in case of indirect tail call, or a jump table.
-    MCInst *MemLocInstr = nullptr;
-
-    if (MCPlus::getNumPrimeOperands(Instruction) == 1) {
-      // If the indirect jump is on register - try to detect if the
-      // register value is loaded from a memory location.
-      assert(Instruction.getOperand(0).isReg() && "register operand expected");
-      const unsigned R1 = Instruction.getOperand(0).getReg();
-      // Check if one of the previous instructions defines the jump-on register.
-      for (auto PrevII = II; PrevII != IE; ++PrevII) {
-        MCInst &PrevInstr = *PrevII;
-        const MCInstrDesc &PrevInstrDesc = Info->get(PrevInstr.getOpcode());
-
-        if (!PrevInstrDesc.hasDefOfPhysReg(PrevInstr, R1, *RegInfo))
-          continue;
-
-        if (isMoveMem2Reg(PrevInstr)) {
-          MemLocInstr = &PrevInstr;
-          break;
-        }
-        if (isADD64rr(PrevInstr)) {
-          unsigned R2 = PrevInstr.getOperand(2).getReg();
-          if (R1 == R2)
-            return IndirectBranchType::UNKNOWN;
-          std::tie(Type, MemLocInstr) = analyzePICJumpTable(PrevII, IE, R1, R2);
-          break;
-        }
-        return IndirectBranchType::UNKNOWN;
-      }
-      if (!MemLocInstr) {
-        // No definition seen for the register in this function so far. Could be
-        // an input parameter - which means it is an external code reference.
-        // It also could be that the definition happens to be in the code that
-        // we haven't processed yet. Since we have to be conservative, return
-        // as UNKNOWN case.
-        return IndirectBranchType::UNKNOWN;
-      }
-    } else {
-      MemLocInstr = &Instruction;
-    }
-
-    const MCRegister RIPRegister = RegInfo->getProgramCounter();
-
-    // Analyze the memory location.
-    std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(*MemLocInstr);
-    if (!MO)
-      return IndirectBranchType::UNKNOWN;
-
-    BaseRegNumOut = MO->BaseRegNum;
-    IndexRegNumOut = MO->IndexRegNum;
-    DispValueOut = MO->DispImm;
-    DispExprOut = MO->DispExpr;
-
-    if ((MO->BaseRegNum != X86::NoRegister && MO->BaseRegNum != RIPRegister) ||
-        MO->SegRegNum != X86::NoRegister)
-      return IndirectBranchType::UNKNOWN;
-
-    if (MemLocInstr == &Instruction &&
-        (!MO->ScaleImm || MO->IndexRegNum == X86::NoRegister)) {
-      MemLocInstrOut = MemLocInstr;
-      return IndirectBranchType::POSSIBLE_FIXED_BRANCH;
-    }
-
-    if (Type == IndirectBranchType::POSSIBLE_PIC_JUMP_TABLE &&
-        (MO->ScaleImm != 1 || MO->BaseRegNum != RIPRegister))
-      return IndirectBranchType::UNKNOWN;
-
-    if (Type != IndirectBranchType::POSSIBLE_PIC_JUMP_TABLE &&
-        MO->ScaleImm != PtrSize)
-      return IndirectBranchType::UNKNOWN;
-
-    MemLocInstrOut = MemLocInstr;
-
-    return Type;
-  }
-
-  /// Analyze a callsite to see if it could be a virtual method call.  This only
-  /// checks to see if the overall pattern is satisfied, it does not guarantee
-  /// that the callsite is a true virtual method call.
-  /// The format of virtual method calls that are recognized is one of the
-  /// following:
-  ///
-  ///  Form 1: (found in debug code)
-  ///    add METHOD_OFFSET, %VtableReg
-  ///    mov (%VtableReg), %MethodReg
-  ///    ...
-  ///    call or jmp *%MethodReg
-  ///
-  ///  Form 2:
-  ///    mov METHOD_OFFSET(%VtableReg), %MethodReg
-  ///    ...
-  ///    call or jmp *%MethodReg
-  ///
-  ///  Form 3:
-  ///    ...
-  ///    call or jmp *METHOD_OFFSET(%VtableReg)
-  ///
-  bool analyzeVirtualMethodCall(InstructionIterator ForwardBegin,
-                                InstructionIterator ForwardEnd,
-                                std::vector<MCInst *> &MethodFetchInsns,
-                                unsigned &VtableRegNum, unsigned &MethodRegNum,
-                                uint64_t &MethodOffset) const override {
-    VtableRegNum = X86::NoRegister;
-    MethodRegNum = X86::NoRegister;
-    MethodOffset = 0;
-
-    std::reverse_iterator<InstructionIterator> Itr(ForwardEnd);
-    std::reverse_iterator<InstructionIterator> End(ForwardBegin);
-
-    MCInst &CallInst = *Itr++;
-    assert(isIndirectBranch(CallInst) || isCall(CallInst));
-
-    // The call can just be jmp offset(reg)
-    if (std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(CallInst)) {
-      if (!MO->DispExpr && MO->BaseRegNum != X86::RIP &&
-          MO->BaseRegNum != X86::RBP && MO->BaseRegNum != X86::NoRegister) {
-        MethodRegNum = MO->BaseRegNum;
-        if (MO->ScaleImm == 1 && MO->IndexRegNum == X86::NoRegister &&
-            MO->SegRegNum == X86::NoRegister) {
-          VtableRegNum = MethodRegNum;
-          MethodOffset = MO->DispImm;
-          MethodFetchInsns.push_back(&CallInst);
-          return true;
-        }
-      }
-      return false;
-    }
-    if (CallInst.getOperand(0).isReg())
-      MethodRegNum = CallInst.getOperand(0).getReg();
-    else
-      return false;
-
-    if (MethodRegNum == X86::RIP || MethodRegNum == X86::RBP) {
-      VtableRegNum = X86::NoRegister;
-      MethodRegNum = X86::NoRegister;
-      return false;
-    }
-
-    // find load from vtable, this may or may not include the method offset
-    while (Itr != End) {
-      MCInst &CurInst = *Itr++;
-      const MCInstrDesc &Desc = Info->get(CurInst.getOpcode());
-      if (Desc.hasDefOfPhysReg(CurInst, MethodRegNum, *RegInfo)) {
-        if (!isLoad(CurInst))
-          return false;
-        if (std::optional<X86MemOperand> MO =
-                evaluateX86MemoryOperand(CurInst)) {
-          if (!MO->DispExpr && MO->ScaleImm == 1 &&
-              MO->BaseRegNum != X86::RIP && MO->BaseRegNum != X86::RBP &&
-              MO->BaseRegNum != X86::NoRegister &&
-              MO->IndexRegNum == X86::NoRegister &&
-              MO->SegRegNum == X86::NoRegister && MO->BaseRegNum != X86::RIP) {
-            VtableRegNum = MO->BaseRegNum;
-            MethodOffset = MO->DispImm;
-            MethodFetchInsns.push_back(&CurInst);
-            if (MethodOffset != 0)
-              return true;
-            break;
-          }
-        }
-        return false;
-      }
-    }
-
-    if (!VtableRegNum)
-      return false;
-
-    // look for any adds affecting the method register.
-    while (Itr != End) {
-      MCInst &CurInst = *Itr++;
-      const MCInstrDesc &Desc = Info->get(CurInst.getOpcode());
-      if (Desc.hasDefOfPhysReg(CurInst, VtableRegNum, *RegInfo)) {
-        if (isADDri(CurInst)) {
-          assert(!MethodOffset);
-          MethodOffset = CurInst.getOperand(2).getImm();
-          MethodFetchInsns.insert(MethodFetchInsns.begin(), &CurInst);
-          break;
-        }
-      }
-    }
-
-    return true;
-  }
-
-  bool createStackPointerIncrement(MCInst &Inst, int Size,
-                                   bool NoFlagsClobber) const override {
-    if (NoFlagsClobber) {
-      Inst.setOpcode(X86::LEA64r);
-      Inst.clear();
-      Inst.addOperand(MCOperand::createReg(X86::RSP));
-      Inst.addOperand(MCOperand::createReg(X86::RSP));        // BaseReg
-      Inst.addOperand(MCOperand::createImm(1));               // ScaleAmt
-      Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // IndexReg
-      Inst.addOperand(MCOperand::createImm(-Size));           // Displacement
-      Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // AddrSegmentReg
-      return true;
-    }
-    Inst.setOpcode(X86::SUB64ri8);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createReg(X86::RSP));
-    Inst.addOperand(MCOperand::createReg(X86::RSP));
-    Inst.addOperand(MCOperand::createImm(Size));
-    return true;
-  }
-
-  bool createStackPointerDecrement(MCInst &Inst, int Size,
-                                   bool NoFlagsClobber) const override {
-    if (NoFlagsClobber) {
-      Inst.setOpcode(X86::LEA64r);
-      Inst.clear();
-      Inst.addOperand(MCOperand::createReg(X86::RSP));
-      Inst.addOperand(MCOperand::createReg(X86::RSP));        // BaseReg
-      Inst.addOperand(MCOperand::createImm(1));               // ScaleAmt
-      Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // IndexReg
-      Inst.addOperand(MCOperand::createImm(Size));            // Displacement
-      Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // AddrSegmentReg
-      return true;
-    }
-    Inst.setOpcode(X86::ADD64ri8);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createReg(X86::RSP));
-    Inst.addOperand(MCOperand::createReg(X86::RSP));
-    Inst.addOperand(MCOperand::createImm(Size));
-    return true;
-  }
-
-  bool createSaveToStack(MCInst &Inst, const MCPhysReg &StackReg, int Offset,
-                         const MCPhysReg &SrcReg, int Size) const override {
-    unsigned NewOpcode;
-    switch (Size) {
-    default:
-      return false;
-    case 2:      NewOpcode = X86::MOV16mr; break;
-    case 4:      NewOpcode = X86::MOV32mr; break;
-    case 8:      NewOpcode = X86::MOV64mr; break;
-    }
-    Inst.setOpcode(NewOpcode);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createReg(StackReg));        // BaseReg
-    Inst.addOperand(MCOperand::createImm(1));               // ScaleAmt
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // IndexReg
-    Inst.addOperand(MCOperand::createImm(Offset));          // Displacement
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // AddrSegmentReg
-    Inst.addOperand(MCOperand::createReg(SrcReg));
-    return true;
-  }
-
-  bool createRestoreFromStack(MCInst &Inst, const MCPhysReg &StackReg,
-                              int Offset, const MCPhysReg &DstReg,
-                              int Size) const override {
-    return createLoad(Inst, StackReg, /*Scale=*/1, /*IndexReg=*/X86::NoRegister,
-                      Offset, nullptr, /*AddrSegmentReg=*/X86::NoRegister,
-                      DstReg, Size);
-  }
-
-  bool createLoad(MCInst &Inst, const MCPhysReg &BaseReg, int64_t Scale,
-                  const MCPhysReg &IndexReg, int64_t Offset,
-                  const MCExpr *OffsetExpr, const MCPhysReg &AddrSegmentReg,
-                  const MCPhysReg &DstReg, int Size) const override {
-    unsigned NewOpcode;
-    switch (Size) {
-    default:
-      return false;
-    case 2:      NewOpcode = X86::MOV16rm; break;
-    case 4:      NewOpcode = X86::MOV32rm; break;
-    case 8:      NewOpcode = X86::MOV64rm; break;
-    }
-    Inst.setOpcode(NewOpcode);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createReg(DstReg));
-    Inst.addOperand(MCOperand::createReg(BaseReg));
-    Inst.addOperand(MCOperand::createImm(Scale));
-    Inst.addOperand(MCOperand::createReg(IndexReg));
-    if (OffsetExpr)
-      Inst.addOperand(MCOperand::createExpr(OffsetExpr)); // Displacement
-    else
-      Inst.addOperand(MCOperand::createImm(Offset)); // Displacement
-    Inst.addOperand(MCOperand::createReg(AddrSegmentReg)); // AddrSegmentReg
-    return true;
-  }
-
-  void createLoadImmediate(MCInst &Inst, const MCPhysReg Dest,
-                           uint32_t Imm) const override {
-    Inst.setOpcode(X86::MOV64ri32);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createReg(Dest));
-    Inst.addOperand(MCOperand::createImm(Imm));
-  }
-
-  bool createIncMemory(MCInst &Inst, const MCSymbol *Target,
-                       MCContext *Ctx) const override {
-
-    Inst.setOpcode(X86::LOCK_INC64m);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createReg(X86::RIP));        // BaseReg
-    Inst.addOperand(MCOperand::createImm(1));               // ScaleAmt
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // IndexReg
-
-    Inst.addOperand(MCOperand::createExpr(
-        MCSymbolRefExpr::create(Target, MCSymbolRefExpr::VK_None,
-                                *Ctx)));                    // Displacement
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // AddrSegmentReg
-    return true;
-  }
-
-  bool createIJmp32Frag(SmallVectorImpl<MCInst> &Insts,
-                        const MCOperand &BaseReg, const MCOperand &Scale,
-                        const MCOperand &IndexReg, const MCOperand &Offset,
-                        const MCOperand &TmpReg) const override {
-    // The code fragment we emit here is:
-    //
-    //  mov32 (%base, %index, scale), %tmpreg
-    //  ijmp *(%tmpreg)
-    //
-    MCInst IJmp;
-    IJmp.setOpcode(X86::JMP64r);
-    IJmp.addOperand(TmpReg);
-
-    MCInst Load;
-    Load.setOpcode(X86::MOV32rm);
-    Load.addOperand(TmpReg);
-    Load.addOperand(BaseReg);
-    Load.addOperand(Scale);
-    Load.addOperand(IndexReg);
-    Load.addOperand(Offset);
-    Load.addOperand(MCOperand::createReg(X86::NoRegister));
-
-    Insts.push_back(Load);
-    Insts.push_back(IJmp);
-    return true;
-  }
-
-  bool createNoop(MCInst &Inst) const override {
-    Inst.setOpcode(X86::NOOP);
-    return true;
-  }
-
-  bool createReturn(MCInst &Inst) const override {
-    Inst.setOpcode(X86::RET64);
-    return true;
-  }
-
-  InstructionListType createInlineMemcpy(bool ReturnEnd) const override {
-    InstructionListType Code;
-    if (ReturnEnd)
-      Code.emplace_back(MCInstBuilder(X86::LEA64r)
-                            .addReg(X86::RAX)
-                            .addReg(X86::RDI)
-                            .addImm(1)
-                            .addReg(X86::RDX)
-                            .addImm(0)
-                            .addReg(X86::NoRegister));
-    else
-      Code.emplace_back(MCInstBuilder(X86::MOV64rr)
-                            .addReg(X86::RAX)
-                            .addReg(X86::RDI));
-
-    Code.emplace_back(MCInstBuilder(X86::MOV32rr)
-                          .addReg(X86::ECX)
-                          .addReg(X86::EDX));
-    Code.emplace_back(MCInstBuilder(X86::REP_MOVSB_64));
-
-    return Code;
-  }
-
-  InstructionListType createOneByteMemcpy() const override {
-    InstructionListType Code;
-    Code.emplace_back(MCInstBuilder(X86::MOV8rm)
-                          .addReg(X86::CL)
-                          .addReg(X86::RSI)
-                          .addImm(0)
-                          .addReg(X86::NoRegister)
-                          .addImm(0)
-                          .addReg(X86::NoRegister));
-    Code.emplace_back(MCInstBuilder(X86::MOV8mr)
-                          .addReg(X86::RDI)
-                          .addImm(0)
-                          .addReg(X86::NoRegister)
-                          .addImm(0)
-                          .addReg(X86::NoRegister)
-                          .addReg(X86::CL));
-    Code.emplace_back(MCInstBuilder(X86::MOV64rr)
-                          .addReg(X86::RAX)
-                          .addReg(X86::RDI));
-    return Code;
-  }
-
-  InstructionListType createCmpJE(MCPhysReg RegNo, int64_t Imm,
-                                  const MCSymbol *Target,
-                                  MCContext *Ctx) const override {
-    InstructionListType Code;
-    Code.emplace_back(MCInstBuilder(X86::CMP64ri8)
-                          .addReg(RegNo)
-                          .addImm(Imm));
-    Code.emplace_back(MCInstBuilder(X86::JCC_1)
-                          .addExpr(MCSymbolRefExpr::create(
-                              Target, MCSymbolRefExpr::VK_None, *Ctx))
-                          .addImm(X86::COND_E));
-    return Code;
-  }
-
-  std::optional<Relocation>
-  createRelocation(const MCFixup &Fixup,
-                   const MCAsmBackend &MAB) const override {
-    const MCFixupKindInfo &FKI = MAB.getFixupKindInfo(Fixup.getKind());
-
-    assert(FKI.TargetOffset == 0 && "0-bit relocation offset expected");
-    const uint64_t RelOffset = Fixup.getOffset();
-
-    uint64_t RelType;
-    if (FKI.Flags & MCFixupKindInfo::FKF_IsPCRel) {
-      switch (FKI.TargetSize) {
-      default:
-        return std::nullopt;
-      case  8: RelType = ELF::R_X86_64_PC8; break;
-      case 16: RelType = ELF::R_X86_64_PC16; break;
-      case 32: RelType = ELF::R_X86_64_PC32; break;
-      case 64: RelType = ELF::R_X86_64_PC64; break;
-      }
-    } else {
-      switch (FKI.TargetSize) {
-      default:
-        return std::nullopt;
-      case  8: RelType = ELF::R_X86_64_8; break;
-      case 16: RelType = ELF::R_X86_64_16; break;
-      case 32: RelType = ELF::R_X86_64_32; break;
-      case 64: RelType = ELF::R_X86_64_64; break;
-      }
-    }
-
-    // Extract a symbol and an addend out of the fixup value expression.
-    //
-    // Only the following limited expression types are supported:
-    //   Symbol + Addend
-    //   Symbol
-    uint64_t Addend = 0;
-    MCSymbol *Symbol = nullptr;
-    const MCExpr *ValueExpr = Fixup.getValue();
-    if (ValueExpr->getKind() == MCExpr::Binary) {
-      const auto *BinaryExpr = cast<MCBinaryExpr>(ValueExpr);
-      assert(BinaryExpr->getOpcode() == MCBinaryExpr::Add &&
-             "unexpected binary expression");
-      const MCExpr *LHS = BinaryExpr->getLHS();
-      assert(LHS->getKind() == MCExpr::SymbolRef && "unexpected LHS");
-      Symbol = const_cast<MCSymbol *>(this->getTargetSymbol(LHS));
-      const MCExpr *RHS = BinaryExpr->getRHS();
-      assert(RHS->getKind() == MCExpr::Constant && "unexpected RHS");
-      Addend = cast<MCConstantExpr>(RHS)->getValue();
-    } else {
-      assert(ValueExpr->getKind() == MCExpr::SymbolRef && "unexpected value");
-      Symbol = const_cast<MCSymbol *>(this->getTargetSymbol(ValueExpr));
-    }
-
-    return Relocation({RelOffset, Symbol, RelType, Addend, 0});
-  }
-
-  bool replaceImmWithSymbolRef(MCInst &Inst, const MCSymbol *Symbol,
-                               int64_t Addend, MCContext *Ctx, int64_t &Value,
-                               uint64_t RelType) const override {
-    unsigned ImmOpNo = -1U;
-
-    for (unsigned Index = 0; Index < MCPlus::getNumPrimeOperands(Inst);
-         ++Index) {
-      if (Inst.getOperand(Index).isImm()) {
-        ImmOpNo = Index;
-        // TODO: this is a bit hacky.  It finds the correct operand by
-        // searching for a specific immediate value.  If no value is
-        // provided it defaults to the last immediate operand found.
-        // This could lead to unexpected results if the instruction
-        // has more than one immediate with the same value.
-        if (Inst.getOperand(ImmOpNo).getImm() == Value)
-          break;
-      }
-    }
-
-    if (ImmOpNo == -1U)
-      return false;
-
-    Value = Inst.getOperand(ImmOpNo).getImm();
-
-    setOperandToSymbolRef(Inst, ImmOpNo, Symbol, Addend, Ctx, RelType);
-
-    return true;
-  }
-
-  bool replaceRegWithImm(MCInst &Inst, unsigned Register,
-                         int64_t Imm) const override {
-
-    enum CheckSignExt : uint8_t {
-      NOCHECK = 0,
-      CHECK8,
-      CHECK32,
-    };
+            MO->SegRegNum != X86::NoRegister || MO->DispExpr)
+            return false;
 
-    using CheckList = std::vector<std::pair<CheckSignExt, unsigned>>;
-    struct InstInfo {
-      // Size in bytes that Inst loads from memory.
-      uint8_t DataSize;
+        Target = MO->DispImm;
+        if (MO->BaseRegNum == X86::RIP) {
+            assert(Size != 0 &&
+                   "instruction size required in order to statically "
+                   "evaluate RIP-relative address");
+            Target += Address + Size;
+        }
+        return true;
+    }
 
-      // True when the target operand has to be duplicated because the opcode
-      // expects a LHS operand.
-      bool HasLHS;
+    MCInst::iterator getMemOperandDisp(MCInst &Inst) const override {
+        int MemOpNo = getMemoryOperandNo(Inst);
+        if (MemOpNo < 0)
+            return Inst.end();
+        return Inst.begin() + (MemOpNo + X86::AddrDisp);
+    }
 
-      // List of checks and corresponding opcodes to be used. We try to use the
-      // smallest possible immediate value when various sizes are available,
-      // hence we may need to check whether a larger constant fits in a smaller
-      // immediate.
-      CheckList Checks;
-    };
+    bool replaceMemOperandDisp(MCInst &Inst, MCOperand Operand) const override {
+        MCOperand *OI = getMemOperandDisp(Inst);
+        if (OI == Inst.end())
+            return false;
+        *OI = Operand;
+        return true;
+    }
 
-    InstInfo I;
-
-    switch (Inst.getOpcode()) {
-    default: {
-      switch (getPushSize(Inst)) {
-
-      case 2: I = {2, false, {{CHECK8, X86::PUSH16i8}, {NOCHECK, X86::PUSHi16}}}; break;
-      case 4: I = {4, false, {{CHECK8, X86::PUSH32i8}, {NOCHECK, X86::PUSHi32}}}; break;
-      case 8: I = {8, false, {{CHECK8, X86::PUSH64i8},
-                              {CHECK32, X86::PUSH64i32},
-                              {NOCHECK, Inst.getOpcode()}}}; break;
-      default: return false;
-      }
-      break;
-    }
-
-    // MOV
-    case X86::MOV8rr:       I = {1, false, {{NOCHECK, X86::MOV8ri}}}; break;
-    case X86::MOV16rr:      I = {2, false, {{NOCHECK, X86::MOV16ri}}}; break;
-    case X86::MOV32rr:      I = {4, false, {{NOCHECK, X86::MOV32ri}}}; break;
-    case X86::MOV64rr:      I = {8, false, {{CHECK32, X86::MOV64ri32},
-                                            {NOCHECK, X86::MOV64ri}}}; break;
-
-    case X86::MOV8mr:       I = {1, false, {{NOCHECK, X86::MOV8mi}}}; break;
-    case X86::MOV16mr:      I = {2, false, {{NOCHECK, X86::MOV16mi}}}; break;
-    case X86::MOV32mr:      I = {4, false, {{NOCHECK, X86::MOV32mi}}}; break;
-    case X86::MOV64mr:      I = {8, false, {{CHECK32, X86::MOV64mi32},
-                                            {NOCHECK, X86::MOV64mr}}}; break;
-
-    // MOVZX
-    case X86::MOVZX16rr8:   I = {1, false, {{NOCHECK, X86::MOV16ri}}}; break;
-    case X86::MOVZX32rr8:   I = {1, false, {{NOCHECK, X86::MOV32ri}}}; break;
-    case X86::MOVZX32rr16:  I = {2, false, {{NOCHECK, X86::MOV32ri}}}; break;
-
-    // CMP
-    case X86::CMP8rr:       I = {1, false, {{NOCHECK, X86::CMP8ri}}}; break;
-    case X86::CMP16rr:      I = {2, false, {{CHECK8, X86::CMP16ri8},
-                                            {NOCHECK, X86::CMP16ri}}}; break;
-    case X86::CMP32rr:      I = {4, false, {{CHECK8, X86::CMP32ri8},
-                                            {NOCHECK, X86::CMP32ri}}}; break;
-    case X86::CMP64rr:      I = {8, false, {{CHECK8, X86::CMP64ri8},
-                                            {CHECK32, X86::CMP64ri32},
-                                            {NOCHECK, X86::CMP64rr}}}; break;
-
-    // TEST
-    case X86::TEST8rr:      I = {1, false, {{NOCHECK, X86::TEST8ri}}}; break;
-    case X86::TEST16rr:     I = {2, false, {{NOCHECK, X86::TEST16ri}}}; break;
-    case X86::TEST32rr:     I = {4, false, {{NOCHECK, X86::TEST32ri}}}; break;
-    case X86::TEST64rr:     I = {8, false, {{CHECK32, X86::TEST64ri32},
-                                            {NOCHECK, X86::TEST64rr}}}; break;
-
-    // ADD
-    case X86::ADD8rr:       I = {1, true, {{NOCHECK, X86::ADD8ri}}}; break;
-    case X86::ADD16rr:      I = {2, true, {{CHECK8, X86::ADD16ri8},
-                                           {NOCHECK, X86::ADD16ri}}}; break;
-    case X86::ADD32rr:      I = {4, true, {{CHECK8, X86::ADD32ri8},
-                                           {NOCHECK, X86::ADD32ri}}}; break;
-    case X86::ADD64rr:      I = {8, true, {{CHECK8, X86::ADD64ri8},
-                                           {CHECK32, X86::ADD64ri32},
-                                           {NOCHECK, X86::ADD64rr}}}; break;
-
-    // SUB
-    case X86::SUB8rr:       I = {1, true, {{NOCHECK, X86::SUB8ri}}}; break;
-    case X86::SUB16rr:      I = {2, true, {{CHECK8, X86::SUB16ri8},
-                                           {NOCHECK, X86::SUB16ri}}}; break;
-    case X86::SUB32rr:      I = {4, true, {{CHECK8, X86::SUB32ri8},
-                                           {NOCHECK, X86::SUB32ri}}}; break;
-    case X86::SUB64rr:      I = {8, true, {{CHECK8, X86::SUB64ri8},
-                                           {CHECK32, X86::SUB64ri32},
-                                           {NOCHECK, X86::SUB64rr}}}; break;
-
-    // AND
-    case X86::AND8rr:       I = {1, true, {{NOCHECK, X86::AND8ri}}}; break;
-    case X86::AND16rr:      I = {2, true, {{CHECK8, X86::AND16ri8},
-                                           {NOCHECK, X86::AND16ri}}}; break;
-    case X86::AND32rr:      I = {4, true, {{CHECK8, X86::AND32ri8},
-                                           {NOCHECK, X86::AND32ri}}}; break;
-    case X86::AND64rr:      I = {8, true, {{CHECK8, X86::AND64ri8},
-                                           {CHECK32, X86::AND64ri32},
-                                           {NOCHECK, X86::AND64rr}}}; break;
-
-    // OR
-    case X86::OR8rr:        I = {1, true, {{NOCHECK, X86::OR8ri}}}; break;
-    case X86::OR16rr:       I = {2, true, {{CHECK8, X86::OR16ri8},
-                                           {NOCHECK, X86::OR16ri}}}; break;
-    case X86::OR32rr:       I = {4, true, {{CHECK8, X86::OR32ri8},
-                                           {NOCHECK, X86::OR32ri}}}; break;
-    case X86::OR64rr:       I = {8, true, {{CHECK8, X86::OR64ri8},
-                                           {CHECK32, X86::OR64ri32},
-                                           {NOCHECK, X86::OR64rr}}}; break;
-
-    // XOR
-    case X86::XOR8rr:       I = {1, true, {{NOCHECK, X86::XOR8ri}}}; break;
-    case X86::XOR16rr:      I = {2, true, {{CHECK8, X86::XOR16ri8},
-                                           {NOCHECK, X86::XOR16ri}}}; break;
-    case X86::XOR32rr:      I = {4, true, {{CHECK8, X86::XOR32ri8},
-                                           {NOCHECK, X86::XOR32ri}}}; break;
-    case X86::XOR64rr:      I = {8, true, {{CHECK8, X86::XOR64ri8},
-                                           {CHECK32, X86::XOR64ri32},
-                                           {NOCHECK, X86::XOR64rr}}}; break;
-    }
-
-    // Compute the new opcode.
-    unsigned NewOpcode = 0;
-    for (const std::pair<CheckSignExt, unsigned> &Check : I.Checks) {
-      NewOpcode = Check.second;
-      if (Check.first == NOCHECK)
-        break;
-      if (Check.first == CHECK8 && isInt<8>(Imm))
-        break;
-      if (Check.first == CHECK32 && isInt<32>(Imm))
-        break;
-    }
-    if (NewOpcode == Inst.getOpcode())
-      return false;
-
-    const MCInstrDesc &InstDesc = Info->get(Inst.getOpcode());
-
-    unsigned NumFound = 0;
-    for (unsigned Index = InstDesc.getNumDefs() + (I.HasLHS ? 1 : 0),
-                  E = InstDesc.getNumOperands();
-         Index != E; ++Index)
-      if (Inst.getOperand(Index).isReg() &&
-          Inst.getOperand(Index).getReg() == Register)
-        NumFound++;
-
-    if (NumFound != 1)
-      return false;
-
-    MCOperand TargetOp = Inst.getOperand(0);
-    Inst.clear();
-    Inst.setOpcode(NewOpcode);
-    Inst.addOperand(TargetOp);
-    if (I.HasLHS)
-      Inst.addOperand(TargetOp);
-    Inst.addOperand(MCOperand::createImm(Imm));
-
-    return true;
-  }
-
-  bool replaceRegWithReg(MCInst &Inst, unsigned ToReplace,
-                         unsigned ReplaceWith) const override {
-
-    // Get the HasLHS value so that iteration can be done
-    bool HasLHS;
-    if (X86::isAND(Inst.getOpcode()) || X86::isADD(Inst.getOpcode()) ||
-        X86::isSUB(Inst.getOpcode())) {
-      HasLHS = true;
-    } else if (isPop(Inst) || isPush(Inst) || X86::isCMP(Inst.getOpcode()) ||
-               X86::isTEST(Inst.getOpcode())) {
-      HasLHS = false;
-    } else {
-      switch (Inst.getOpcode()) {
-      case X86::MOV8rr:
-      case X86::MOV8rm:
-      case X86::MOV8mr:
-      case X86::MOV8ri:
-      case X86::MOV16rr:
-      case X86::MOV16rm:
-      case X86::MOV16mr:
-      case X86::MOV16ri:
-      case X86::MOV32rr:
-      case X86::MOV32rm:
-      case X86::MOV32mr:
-      case X86::MOV32ri:
-      case X86::MOV64rr:
-      case X86::MOV64rm:
-      case X86::MOV64mr:
-      case X86::MOV64ri:
-      case X86::MOVZX16rr8:
-      case X86::MOVZX32rr8:
-      case X86::MOVZX32rr16:
-      case X86::MOVSX32rm8:
-      case X86::MOVSX32rr8:
-      case X86::MOVSX64rm32:
-      case X86::LEA64r:
-        HasLHS = false;
-        break;
-      default:
-        return false;
-      }
-    }
-
-    const MCInstrDesc &InstDesc = Info->get(Inst.getOpcode());
-
-    bool FoundOne = false;
-
-    // Iterate only through src operands that arent also dest operands
-    for (unsigned Index = InstDesc.getNumDefs() + (HasLHS ? 1 : 0),
-                  E = InstDesc.getNumOperands();
-         Index != E; ++Index) {
-      BitVector RegAliases = getAliases(ToReplace, true);
-      if (!Inst.getOperand(Index).isReg() ||
-          !RegAliases.test(Inst.getOperand(Index).getReg()))
-        continue;
-      // Resize register if needed
-      unsigned SizedReplaceWith = getAliasSized(
-          ReplaceWith, getRegSize(Inst.getOperand(Index).getReg()));
-      MCOperand NewOperand = MCOperand::createReg(SizedReplaceWith);
-      Inst.getOperand(Index) = NewOperand;
-      FoundOne = true;
-    }
-
-    // Return true if at least one operand was replaced
-    return FoundOne;
-  }
-
-  bool createUncondBranch(MCInst &Inst, const MCSymbol *TBB,
-                          MCContext *Ctx) const override {
-    Inst.setOpcode(X86::JMP_1);
-    Inst.addOperand(MCOperand::createExpr(
-        MCSymbolRefExpr::create(TBB, MCSymbolRefExpr::VK_None, *Ctx)));
-    return true;
-  }
-
-  bool createCall(MCInst &Inst, const MCSymbol *Target,
-                  MCContext *Ctx) override {
-    Inst.setOpcode(X86::CALL64pcrel32);
-    Inst.addOperand(MCOperand::createExpr(
-        MCSymbolRefExpr::create(Target, MCSymbolRefExpr::VK_None, *Ctx)));
-    return true;
-  }
-
-  bool createTailCall(MCInst &Inst, const MCSymbol *Target,
-                      MCContext *Ctx) override {
-    return createDirectCall(Inst, Target, Ctx, /*IsTailCall*/ true);
-  }
-
-  void createLongTailCall(InstructionListType &Seq, const MCSymbol *Target,
-                          MCContext *Ctx) override {
-    Seq.clear();
-    Seq.emplace_back();
-    createDirectCall(Seq.back(), Target, Ctx, /*IsTailCall*/ true);
-  }
-
-  bool createTrap(MCInst &Inst) const override {
-    Inst.clear();
-    Inst.setOpcode(X86::TRAP);
-    return true;
-  }
-
-  bool reverseBranchCondition(MCInst &Inst, const MCSymbol *TBB,
-                              MCContext *Ctx) const override {
-    unsigned InvCC = getInvertedCondCode(getCondCode(Inst));
-    assert(InvCC != X86::COND_INVALID && "invalid branch instruction");
-    Inst.getOperand(Info->get(Inst.getOpcode()).NumOperands - 1).setImm(InvCC);
-    Inst.getOperand(0) = MCOperand::createExpr(
-        MCSymbolRefExpr::create(TBB, MCSymbolRefExpr::VK_None, *Ctx));
-    return true;
-  }
-
-  bool replaceBranchCondition(MCInst &Inst, const MCSymbol *TBB, MCContext *Ctx,
-                              unsigned CC) const override {
-    if (CC == X86::COND_INVALID)
-      return false;
-    Inst.getOperand(Info->get(Inst.getOpcode()).NumOperands - 1).setImm(CC);
-    Inst.getOperand(0) = MCOperand::createExpr(
-        MCSymbolRefExpr::create(TBB, MCSymbolRefExpr::VK_None, *Ctx));
-    return true;
-  }
-
-  unsigned getCanonicalBranchCondCode(unsigned CC) const override {
-    switch (CC) {
-    default:           return X86::COND_INVALID;
-
-    case X86::COND_E:  return X86::COND_E;
-    case X86::COND_NE: return X86::COND_E;
-
-    case X86::COND_L:  return X86::COND_L;
-    case X86::COND_GE: return X86::COND_L;
-
-    case X86::COND_LE: return X86::COND_G;
-    case X86::COND_G:  return X86::COND_G;
-
-    case X86::COND_B:  return X86::COND_B;
-    case X86::COND_AE: return X86::COND_B;
-
-    case X86::COND_BE: return X86::COND_A;
-    case X86::COND_A:  return X86::COND_A;
-
-    case X86::COND_S:  return X86::COND_S;
-    case X86::COND_NS: return X86::COND_S;
-
-    case X86::COND_P:  return X86::COND_P;
-    case X86::COND_NP: return X86::COND_P;
-
-    case X86::COND_O:  return X86::COND_O;
-    case X86::COND_NO: return X86::COND_O;
-    }
-  }
-
-  bool replaceBranchTarget(MCInst &Inst, const MCSymbol *TBB,
-                           MCContext *Ctx) const override {
-    assert((isCall(Inst) || isBranch(Inst)) && !isIndirectBranch(Inst) &&
-           "Invalid instruction");
-    Inst.getOperand(0) = MCOperand::createExpr(
-        MCSymbolRefExpr::create(TBB, MCSymbolRefExpr::VK_None, *Ctx));
-    return true;
-  }
-
-  MCPhysReg getX86R11() const override { return X86::R11; }
-
-  MCPhysReg getIntArgRegister(unsigned ArgNo) const override {
-    // FIXME: this should depend on the calling convention.
-    switch (ArgNo) {
-    case 0:   return X86::RDI;
-    case 1:   return X86::RSI;
-    case 2:   return X86::RDX;
-    case 3:   return X86::RCX;
-    case 4:   return X86::R8;
-    case 5:   return X86::R9;
-    default:  return getNoRegister();
-    }
-  }
-
-  void createPause(MCInst &Inst) const override {
-    Inst.clear();
-    Inst.setOpcode(X86::PAUSE);
-  }
-
-  void createLfence(MCInst &Inst) const override {
-    Inst.clear();
-    Inst.setOpcode(X86::LFENCE);
-  }
-
-  bool createDirectCall(MCInst &Inst, const MCSymbol *Target, MCContext *Ctx,
-                        bool IsTailCall) override {
-    Inst.clear();
-    Inst.setOpcode(IsTailCall ? X86::JMP_4 : X86::CALL64pcrel32);
-    Inst.addOperand(MCOperand::createExpr(
-        MCSymbolRefExpr::create(Target, MCSymbolRefExpr::VK_None, *Ctx)));
-    if (IsTailCall)
-      setTailCall(Inst);
-    return true;
-  }
-
-  void createShortJmp(InstructionListType &Seq, const MCSymbol *Target,
-                      MCContext *Ctx, bool IsTailCall) override {
-    Seq.clear();
-    MCInst Inst;
-    Inst.setOpcode(X86::JMP_1);
-    Inst.addOperand(MCOperand::createExpr(
-        MCSymbolRefExpr::create(Target, MCSymbolRefExpr::VK_None, *Ctx)));
-    if (IsTailCall)
-      setTailCall(Inst);
-    Seq.emplace_back(Inst);
-  }
-
-  bool isConditionalMove(const MCInst &Inst) const override {
-    unsigned OpCode = Inst.getOpcode();
-    return (OpCode == X86::CMOV16rr || OpCode == X86::CMOV32rr ||
-            OpCode == X86::CMOV64rr);
-  }
-
-  bool isBranchOnMem(const MCInst &Inst) const override {
-    unsigned OpCode = Inst.getOpcode();
-    if (OpCode == X86::CALL64m || (OpCode == X86::JMP32m && isTailCall(Inst)) ||
-        OpCode == X86::JMP64m)
-      return true;
-
-    return false;
-  }
-
-  bool isBranchOnReg(const MCInst &Inst) const override {
-    unsigned OpCode = Inst.getOpcode();
-    if (OpCode == X86::CALL64r || (OpCode == X86::JMP32r && isTailCall(Inst)) ||
-        OpCode == X86::JMP64r)
-      return true;
-
-    return false;
-  }
-
-  void createPushRegister(MCInst &Inst, MCPhysReg Reg,
-                          unsigned Size) const override {
-    Inst.clear();
-    unsigned NewOpcode = 0;
-    if (Reg == X86::EFLAGS) {
-      switch (Size) {
-      case 2: NewOpcode = X86::PUSHF16;  break;
-      case 4: NewOpcode = X86::PUSHF32;  break;
-      case 8: NewOpcode = X86::PUSHF64;  break;
-      default:
-        llvm_unreachable("Unexpected size");
-      }
-      Inst.setOpcode(NewOpcode);
-      return;
-    }
-    switch (Size) {
-    case 2: NewOpcode = X86::PUSH16r;  break;
-    case 4: NewOpcode = X86::PUSH32r;  break;
-    case 8: NewOpcode = X86::PUSH64r;  break;
-    default:
-      llvm_unreachable("Unexpected size");
+    /// Get the registers used as function parameters.
+    /// This function is specific to the x86_64 abi on Linux.
+    BitVector getRegsUsedAsParams() const override {
+        BitVector Regs = BitVector(RegInfo->getNumRegs(), false);
+        Regs |= getAliases(X86::RSI);
+        Regs |= getAliases(X86::RDI);
+        Regs |= getAliases(X86::RDX);
+        Regs |= getAliases(X86::RCX);
+        Regs |= getAliases(X86::R8);
+        Regs |= getAliases(X86::R9);
+        return Regs;
     }
-    Inst.setOpcode(NewOpcode);
-    Inst.addOperand(MCOperand::createReg(Reg));
-  }
 
-  void createPopRegister(MCInst &Inst, MCPhysReg Reg,
-                         unsigned Size) const override {
-    Inst.clear();
-    unsigned NewOpcode = 0;
-    if (Reg == X86::EFLAGS) {
-      switch (Size) {
-      case 2: NewOpcode = X86::POPF16;  break;
-      case 4: NewOpcode = X86::POPF32;  break;
-      case 8: NewOpcode = X86::POPF64;  break;
-      default:
-        llvm_unreachable("Unexpected size");
-      }
-      Inst.setOpcode(NewOpcode);
-      return;
-    }
-    switch (Size) {
-    case 2: NewOpcode = X86::POP16r;  break;
-    case 4: NewOpcode = X86::POP32r;  break;
-    case 8: NewOpcode = X86::POP64r;  break;
-    default:
-      llvm_unreachable("Unexpected size");
-    }
-    Inst.setOpcode(NewOpcode);
-    Inst.addOperand(MCOperand::createReg(Reg));
-  }
-
-  void createPushFlags(MCInst &Inst, unsigned Size) const override {
-    return createPushRegister(Inst, X86::EFLAGS, Size);
-  }
-
-  void createPopFlags(MCInst &Inst, unsigned Size) const override {
-    return createPopRegister(Inst, X86::EFLAGS, Size);
-  }
-
-  void createAddRegImm(MCInst &Inst, MCPhysReg Reg, int64_t Value,
-                       unsigned Size) const {
-    unsigned int Opcode;
-    switch (Size) {
-    case 1: Opcode = X86::ADD8ri; break;
-    case 2: Opcode = X86::ADD16ri; break;
-    case 4: Opcode = X86::ADD32ri; break;
-    default:
-      llvm_unreachable("Unexpected size");
-    }
-    Inst.setOpcode(Opcode);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createReg(Reg));
-    Inst.addOperand(MCOperand::createReg(Reg));
-    Inst.addOperand(MCOperand::createImm(Value));
-  }
-
-  void createClearRegWithNoEFlagsUpdate(MCInst &Inst, MCPhysReg Reg,
-                                        unsigned Size) const {
-    unsigned int Opcode;
-    switch (Size) {
-    case 1: Opcode = X86::MOV8ri; break;
-    case 2: Opcode = X86::MOV16ri; break;
-    case 4: Opcode = X86::MOV32ri; break;
-    // Writing to a 32-bit register always zeros the upper 32 bits of the
-    // full-width register
-    case 8:
-      Opcode = X86::MOV32ri;
-      Reg = getAliasSized(Reg, 4);
-      break;
-    default:
-      llvm_unreachable("Unexpected size");
-    }
-    Inst.setOpcode(Opcode);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createReg(Reg));
-    Inst.addOperand(MCOperand::createImm(0));
-  }
-
-  void createX86SaveOVFlagToRegister(MCInst &Inst, MCPhysReg Reg) const {
-    Inst.setOpcode(X86::SETCCr);
-    Inst.clear();
-    Inst.addOperand(MCOperand::createReg(Reg));
-    Inst.addOperand(MCOperand::createImm(X86::COND_O));
-  }
-
-  void createX86Lahf(MCInst &Inst) const {
-    Inst.setOpcode(X86::LAHF);
-    Inst.clear();
-  }
-
-  void createX86Sahf(MCInst &Inst) const {
-    Inst.setOpcode(X86::SAHF);
-    Inst.clear();
-  }
-
-  InstructionListType createInstrIncMemory(const MCSymbol *Target,
-                                           MCContext *Ctx,
-                                           bool IsLeaf) const override {
-    InstructionListType Instrs(IsLeaf ? 13 : 11);
-    unsigned int I = 0;
-
-    // Don't clobber application red zone (ABI dependent)
-    if (IsLeaf)
-      createStackPointerIncrement(Instrs[I++], 128,
-                                  /*NoFlagsClobber=*/true);
-
-    // Performance improvements based on the optimization discussed at
-    // https://reviews.llvm.org/D6629
-    // LAHF/SAHF are used instead of PUSHF/POPF
-    // PUSHF
-    createPushRegister(Instrs[I++], X86::RAX, 8);
-    createClearRegWithNoEFlagsUpdate(Instrs[I++], X86::RAX, 8);
-    createX86Lahf(Instrs[I++]);
-    createPushRegister(Instrs[I++], X86::RAX, 8);
-    createClearRegWithNoEFlagsUpdate(Instrs[I++], X86::RAX, 8);
-    createX86SaveOVFlagToRegister(Instrs[I++], X86::AL);
-    // LOCK INC
-    createIncMemory(Instrs[I++], Target, Ctx);
-    // POPF
-    createAddRegImm(Instrs[I++], X86::AL, 127, 1);
-    createPopRegister(Instrs[I++], X86::RAX, 8);
-    createX86Sahf(Instrs[I++]);
-    createPopRegister(Instrs[I++], X86::RAX, 8);
-
-    if (IsLeaf)
-      createStackPointerDecrement(Instrs[I], 128,
-                                  /*NoFlagsClobber=*/true);
-    return Instrs;
-  }
-
-  void createSwap(MCInst &Inst, MCPhysReg Source, MCPhysReg MemBaseReg,
-                  int64_t Disp) const {
-    Inst.setOpcode(X86::XCHG64rm);
-    Inst.addOperand(MCOperand::createReg(Source));
-    Inst.addOperand(MCOperand::createReg(Source));
-    Inst.addOperand(MCOperand::createReg(MemBaseReg));      // BaseReg
-    Inst.addOperand(MCOperand::createImm(1));               // ScaleAmt
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // IndexReg
-    Inst.addOperand(MCOperand::createImm(Disp));            // Displacement
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // AddrSegmentReg
-  }
-
-  void createIndirectBranch(MCInst &Inst, MCPhysReg MemBaseReg,
-                            int64_t Disp) const {
-    Inst.setOpcode(X86::JMP64m);
-    Inst.addOperand(MCOperand::createReg(MemBaseReg));      // BaseReg
-    Inst.addOperand(MCOperand::createImm(1));               // ScaleAmt
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // IndexReg
-    Inst.addOperand(MCOperand::createImm(Disp));            // Displacement
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // AddrSegmentReg
-  }
-
-  InstructionListType createInstrumentedIndirectCall(MCInst &&CallInst,
-                                                     MCSymbol *HandlerFuncAddr,
-                                                     int CallSiteID,
-                                                     MCContext *Ctx) override {
-    // Check if the target address expression used in the original indirect call
-    // uses the stack pointer, which we are going to clobber.
-    static BitVector SPAliases(getAliases(X86::RSP));
-    bool UsesSP = false;
-    // Skip defs.
-    for (unsigned I = Info->get(CallInst.getOpcode()).getNumDefs(),
-                  E = MCPlus::getNumPrimeOperands(CallInst);
-         I != E; ++I) {
-      const MCOperand &Operand = CallInst.getOperand(I);
-      if (Operand.isReg() && SPAliases[Operand.getReg()]) {
-        UsesSP = true;
-        break;
-      }
-    }
-
-    InstructionListType Insts;
-    MCPhysReg TempReg = getIntArgRegister(0);
-    // Code sequence used to enter indirect call instrumentation helper:
-    //   push %rdi
-    //   add $8, %rsp       ;; $rsp may be used in target, so fix it to prev val
-    //   movq target, %rdi  ;; via convertIndirectCallTargetToLoad
-    //   sub $8, %rsp       ;; restore correct stack value
-    //   push %rdi
-    //   movq $CallSiteID, %rdi
-    //   push %rdi
-    //   callq/jmp HandlerFuncAddr
-    Insts.emplace_back();
-    createPushRegister(Insts.back(), TempReg, 8);
-    if (UsesSP) { // Only adjust SP if we really need to
-      Insts.emplace_back();
-      createStackPointerDecrement(Insts.back(), 8, /*NoFlagsClobber=*/false);
-    }
-    Insts.emplace_back(CallInst);
-    // Insts.back() and CallInst now share the same annotation instruction.
-    // Strip it from Insts.back(), only preserving tail call annotation.
-    stripAnnotations(Insts.back(), /*KeepTC=*/true);
-    convertIndirectCallToLoad(Insts.back(), TempReg);
-    if (UsesSP) {
-      Insts.emplace_back();
-      createStackPointerIncrement(Insts.back(), 8, /*NoFlagsClobber=*/false);
-    }
-    Insts.emplace_back();
-    createPushRegister(Insts.back(), TempReg, 8);
-    Insts.emplace_back();
-    createLoadImmediate(Insts.back(), TempReg, CallSiteID);
-    Insts.emplace_back();
-    createPushRegister(Insts.back(), TempReg, 8);
-
-    MCInst &NewCallInst = Insts.emplace_back();
-    createDirectCall(NewCallInst, HandlerFuncAddr, Ctx, isTailCall(CallInst));
-
-    // Carry over metadata including tail call marker if present.
-    stripAnnotations(NewCallInst);
-    moveAnnotations(std::move(CallInst), NewCallInst);
-
-    return Insts;
-  }
-
-  InstructionListType createInstrumentedIndCallHandlerExitBB() const override {
-    const MCPhysReg TempReg = getIntArgRegister(0);
-    // We just need to undo the sequence created for every ind call in
-    // instrumentIndirectTarget(), which can be accomplished minimally with:
-    //   popfq
-    //   pop %rdi
-    //   add $16, %rsp
-    //   xchg (%rsp), %rdi
-    //   jmp *-8(%rsp)
-    InstructionListType Insts(5);
-    createPopFlags(Insts[0], 8);
-    createPopRegister(Insts[1], TempReg, 8);
-    createStackPointerDecrement(Insts[2], 16, /*NoFlagsClobber=*/false);
-    createSwap(Insts[3], TempReg, X86::RSP, 0);
-    createIndirectBranch(Insts[4], X86::RSP, -8);
-    return Insts;
-  }
-
-  InstructionListType
-  createInstrumentedIndTailCallHandlerExitBB() const override {
-    const MCPhysReg TempReg = getIntArgRegister(0);
-    // Same thing as above, but for tail calls
-    //   popfq
-    //   add $16, %rsp
-    //   pop %rdi
-    //   jmp *-16(%rsp)
-    InstructionListType Insts(4);
-    createPopFlags(Insts[0], 8);
-    createStackPointerDecrement(Insts[1], 16, /*NoFlagsClobber=*/false);
-    createPopRegister(Insts[2], TempReg, 8);
-    createIndirectBranch(Insts[3], X86::RSP, -16);
-    return Insts;
-  }
-
-  InstructionListType
-  createInstrumentedIndCallHandlerEntryBB(const MCSymbol *InstrTrampoline,
-                                          const MCSymbol *IndCallHandler,
-                                          MCContext *Ctx) override {
-    const MCPhysReg TempReg = getIntArgRegister(0);
-    // Code sequence used to check whether InstrTampoline was initialized
-    // and call it if so, returns via IndCallHandler.
-    //   pushfq
-    //   mov    InstrTrampoline,%rdi
-    //   cmp    $0x0,%rdi
-    //   je     IndCallHandler
-    //   callq  *%rdi
-    //   jmpq   IndCallHandler
-    InstructionListType Insts;
-    Insts.emplace_back();
-    createPushFlags(Insts.back(), 8);
-    Insts.emplace_back();
-    createMove(Insts.back(), InstrTrampoline, TempReg, Ctx);
-    InstructionListType cmpJmp = createCmpJE(TempReg, 0, IndCallHandler, Ctx);
-    Insts.insert(Insts.end(), cmpJmp.begin(), cmpJmp.end());
-    Insts.emplace_back();
-    Insts.back().setOpcode(X86::CALL64r);
-    Insts.back().addOperand(MCOperand::createReg(TempReg));
-    Insts.emplace_back();
-    createDirectCall(Insts.back(), IndCallHandler, Ctx, /*IsTailCall*/ true);
-    return Insts;
-  }
-
-  InstructionListType createNumCountersGetter(MCContext *Ctx) const override {
-    InstructionListType Insts(2);
-    MCSymbol *NumLocs = Ctx->getOrCreateSymbol("__bolt_num_counters");
-    createMove(Insts[0], NumLocs, X86::EAX, Ctx);
-    createReturn(Insts[1]);
-    return Insts;
-  }
-
-  InstructionListType
-  createInstrLocationsGetter(MCContext *Ctx) const override {
-    InstructionListType Insts(2);
-    MCSymbol *Locs = Ctx->getOrCreateSymbol("__bolt_instr_locations");
-    createLea(Insts[0], Locs, X86::EAX, Ctx);
-    createReturn(Insts[1]);
-    return Insts;
-  }
-
-  InstructionListType createInstrTablesGetter(MCContext *Ctx) const override {
-    InstructionListType Insts(2);
-    MCSymbol *Locs = Ctx->getOrCreateSymbol("__bolt_instr_tables");
-    createLea(Insts[0], Locs, X86::EAX, Ctx);
-    createReturn(Insts[1]);
-    return Insts;
-  }
-
-  InstructionListType createInstrNumFuncsGetter(MCContext *Ctx) const override {
-    InstructionListType Insts(2);
-    MCSymbol *NumFuncs = Ctx->getOrCreateSymbol("__bolt_instr_num_funcs");
-    createMove(Insts[0], NumFuncs, X86::EAX, Ctx);
-    createReturn(Insts[1]);
-    return Insts;
-  }
-
-  InstructionListType createSymbolTrampoline(const MCSymbol *TgtSym,
-                                             MCContext *Ctx) const override {
-    InstructionListType Insts(1);
-    createUncondBranch(Insts[0], TgtSym, Ctx);
-    return Insts;
-  }
-
-  InstructionListType createDummyReturnFunction(MCContext *Ctx) const override {
-    InstructionListType Insts(1);
-    createReturn(Insts[0]);
-    return Insts;
-  }
-
-  BlocksVectorTy indirectCallPromotion(
-      const MCInst &CallInst,
-      const std::vector<std::pair<MCSymbol *, uint64_t>> &Targets,
-      const std::vector<std::pair<MCSymbol *, uint64_t>> &VtableSyms,
-      const std::vector<MCInst *> &MethodFetchInsns,
-      const bool MinimizeCodeSize, MCContext *Ctx) override {
-    const bool IsTailCall = isTailCall(CallInst);
-    const bool IsJumpTable = getJumpTable(CallInst) != 0;
-    BlocksVectorTy Results;
-
-    // Label for the current code block.
-    MCSymbol *NextTarget = nullptr;
-
-    // The join block which contains all the instructions following CallInst.
-    // MergeBlock remains null if CallInst is a tail call.
-    MCSymbol *MergeBlock = nullptr;
-
-    unsigned FuncAddrReg = X86::R10;
-
-    const bool LoadElim = !VtableSyms.empty();
-    assert((!LoadElim || VtableSyms.size() == Targets.size()) &&
-           "There must be a vtable entry for every method "
-           "in the targets vector.");
-
-    if (MinimizeCodeSize && !LoadElim) {
-      std::set<unsigned> UsedRegs;
-
-      for (unsigned int I = 0; I < MCPlus::getNumPrimeOperands(CallInst); ++I) {
-        const MCOperand &Op = CallInst.getOperand(I);
-        if (Op.isReg())
-          UsedRegs.insert(Op.getReg());
-      }
-
-      if (UsedRegs.count(X86::R10) == 0)
-        FuncAddrReg = X86::R10;
-      else if (UsedRegs.count(X86::R11) == 0)
-        FuncAddrReg = X86::R11;
-      else
-        return Results;
+    void getCalleeSavedRegs(BitVector &Regs) const override {
+        Regs |= getAliases(X86::RBX);
+        Regs |= getAliases(X86::RBP);
+        Regs |= getAliases(X86::R12);
+        Regs |= getAliases(X86::R13);
+        Regs |= getAliases(X86::R14);
+        Regs |= getAliases(X86::R15);
     }
 
-    const auto jumpToMergeBlock = [&](InstructionListType &NewCall) {
-      assert(MergeBlock);
-      NewCall.push_back(CallInst);
-      MCInst &Merge = NewCall.back();
-      Merge.clear();
-      createUncondBranch(Merge, MergeBlock, Ctx);
-    };
+    void getDefaultDefIn(BitVector &Regs) const override {
+        assert(Regs.size() >= RegInfo->getNumRegs() &&
+               "The size of BitVector is less than RegInfo->getNumRegs().");
+        Regs.set(X86::RAX);
+        Regs.set(X86::RCX);
+        Regs.set(X86::RDX);
+        Regs.set(X86::RSI);
+        Regs.set(X86::RDI);
+        Regs.set(X86::R8);
+        Regs.set(X86::R9);
+        Regs.set(X86::XMM0);
+        Regs.set(X86::XMM1);
+        Regs.set(X86::XMM2);
+        Regs.set(X86::XMM3);
+        Regs.set(X86::XMM4);
+        Regs.set(X86::XMM5);
+        Regs.set(X86::XMM6);
+        Regs.set(X86::XMM7);
+    }
 
-    for (unsigned int i = 0; i < Targets.size(); ++i) {
-      Results.emplace_back(NextTarget, InstructionListType());
-      InstructionListType *NewCall = &Results.back().second;
-
-      if (MinimizeCodeSize && !LoadElim) {
-        // Load the call target into FuncAddrReg.
-        NewCall->push_back(CallInst); // Copy CallInst in order to get SMLoc
-        MCInst &Target = NewCall->back();
-        Target.clear();
-        Target.setOpcode(X86::MOV64ri32);
-        Target.addOperand(MCOperand::createReg(FuncAddrReg));
-        if (Targets[i].first) {
-          // Is this OK?
-          Target.addOperand(MCOperand::createExpr(MCSymbolRefExpr::create(
-              Targets[i].first, MCSymbolRefExpr::VK_None, *Ctx)));
-        } else {
-          const uint64_t Addr = Targets[i].second;
-          // Immediate address is out of sign extended 32 bit range.
-          if (int64_t(Addr) != int64_t(int32_t(Addr)))
-            return BlocksVectorTy();
+    void getDefaultLiveOut(BitVector &Regs) const override {
+        assert(Regs.size() >= RegInfo->getNumRegs() &&
+               "The size of BitVector is less than RegInfo->getNumRegs().");
+        Regs |= getAliases(X86::RAX);
+        Regs |= getAliases(X86::RDX);
+        Regs |= getAliases(X86::RCX);
+        Regs |= getAliases(X86::XMM0);
+        Regs |= getAliases(X86::XMM1);
+    }
 
-          Target.addOperand(MCOperand::createImm(Addr));
+    void getGPRegs(BitVector &Regs, bool IncludeAlias) const override {
+        if (IncludeAlias) {
+            Regs |= getAliases(X86::RAX);
+            Regs |= getAliases(X86::RBX);
+            Regs |= getAliases(X86::RBP);
+            Regs |= getAliases(X86::RSI);
+            Regs |= getAliases(X86::RDI);
+            Regs |= getAliases(X86::RDX);
+            Regs |= getAliases(X86::RCX);
+            Regs |= getAliases(X86::R8);
+            Regs |= getAliases(X86::R9);
+            Regs |= getAliases(X86::R10);
+            Regs |= getAliases(X86::R11);
+            Regs |= getAliases(X86::R12);
+            Regs |= getAliases(X86::R13);
+            Regs |= getAliases(X86::R14);
+            Regs |= getAliases(X86::R15);
+            return;
         }
+        Regs.set(X86::RAX);
+        Regs.set(X86::RBX);
+        Regs.set(X86::RBP);
+        Regs.set(X86::RSI);
+        Regs.set(X86::RDI);
+        Regs.set(X86::RDX);
+        Regs.set(X86::RCX);
+        Regs.set(X86::R8);
+        Regs.set(X86::R9);
+        Regs.set(X86::R10);
+        Regs.set(X86::R11);
+        Regs.set(X86::R12);
+        Regs.set(X86::R13);
+        Regs.set(X86::R14);
+        Regs.set(X86::R15);
+    }
 
-        // Compare current call target to a specific address.
-        NewCall->push_back(CallInst);
-        MCInst &Compare = NewCall->back();
-        Compare.clear();
-        if (isBranchOnReg(CallInst))
-          Compare.setOpcode(X86::CMP64rr);
-        else if (CallInst.getOpcode() == X86::CALL64pcrel32)
-          Compare.setOpcode(X86::CMP64ri32);
-        else
-          Compare.setOpcode(X86::CMP64rm);
-
-        Compare.addOperand(MCOperand::createReg(FuncAddrReg));
-
-        // TODO: Would be preferable to only load this value once.
-        for (unsigned i = 0;
-             i < Info->get(CallInst.getOpcode()).getNumOperands(); ++i)
-          if (!CallInst.getOperand(i).isInst())
-            Compare.addOperand(CallInst.getOperand(i));
-      } else {
-        // Compare current call target to a specific address.
-        NewCall->push_back(CallInst);
-        MCInst &Compare = NewCall->back();
-        Compare.clear();
-        if (isBranchOnReg(CallInst))
-          Compare.setOpcode(X86::CMP64ri32);
-        else
-          Compare.setOpcode(X86::CMP64mi32);
-
-        // Original call address.
-        for (unsigned i = 0;
-             i < Info->get(CallInst.getOpcode()).getNumOperands(); ++i)
-          if (!CallInst.getOperand(i).isInst())
-            Compare.addOperand(CallInst.getOperand(i));
-
-        // Target address.
-        if (Targets[i].first || LoadElim) {
-          const MCSymbol *Sym =
-              LoadElim ? VtableSyms[i].first : Targets[i].first;
-          const uint64_t Addend = LoadElim ? VtableSyms[i].second : 0;
-          const MCExpr *Expr = MCSymbolRefExpr::create(Sym, *Ctx);
-          if (Addend)
-            Expr = MCBinaryExpr::createAdd(
-                Expr, MCConstantExpr::create(Addend, *Ctx), *Ctx);
-          Compare.addOperand(MCOperand::createExpr(Expr));
-        } else {
-          const uint64_t Addr = Targets[i].second;
-          // Immediate address is out of sign extended 32 bit range.
-          if (int64_t(Addr) != int64_t(int32_t(Addr)))
-            return BlocksVectorTy();
+    void getClassicGPRegs(BitVector &Regs) const override {
+        Regs |= getAliases(X86::RAX);
+        Regs |= getAliases(X86::RBX);
+        Regs |= getAliases(X86::RBP);
+        Regs |= getAliases(X86::RSI);
+        Regs |= getAliases(X86::RDI);
+        Regs |= getAliases(X86::RDX);
+        Regs |= getAliases(X86::RCX);
+    }
 
-          Compare.addOperand(MCOperand::createImm(Addr));
-        }
-      }
+    void getRepRegs(BitVector &Regs) const override {
+        Regs |= getAliases(X86::RCX);
+    }
 
-      // jump to next target compare.
-      NextTarget =
-          Ctx->createNamedTempSymbol(); // generate label for the next block
-      NewCall->push_back(CallInst);
+    MCPhysReg getAliasSized(MCPhysReg Reg, uint8_t Size) const override {
+        Reg = getX86SubSuperRegister(Reg, Size * 8);
+        assert((Reg != X86::NoRegister) && "Invalid register");
+        return Reg;
+    }
+
+    bool isUpper8BitReg(MCPhysReg Reg) const override {
+        switch (Reg) {
+        case X86::AH:
+        case X86::BH:
+        case X86::CH:
+        case X86::DH:
+            return true;
+        default:
+            return false;
+        }
+    }
 
-      if (IsJumpTable) {
-        MCInst &Je = NewCall->back();
+    bool cannotUseREX(const MCInst &Inst) const override {
+        switch (Inst.getOpcode()) {
+        case X86::MOV8mr_NOREX:
+        case X86::MOV8rm_NOREX:
+        case X86::MOV8rr_NOREX:
+        case X86::MOVSX32rm8_NOREX:
+        case X86::MOVSX32rr8_NOREX:
+        case X86::MOVZX32rm8_NOREX:
+        case X86::MOVZX32rr8_NOREX:
+        case X86::MOV8mr:
+        case X86::MOV8rm:
+        case X86::MOV8rr:
+        case X86::MOVSX32rm8:
+        case X86::MOVSX32rr8:
+        case X86::MOVZX32rm8:
+        case X86::MOVZX32rr8:
+        case X86::TEST8ri:
+            for (const MCOperand &Operand : MCPlus::primeOperands(Inst)) {
+                if (!Operand.isReg())
+                    continue;
+                if (isUpper8BitReg(Operand.getReg()))
+                    return true;
+            }
+            [[fallthrough]];
+        default:
+            return false;
+        }
+    }
 
-        // Jump to next compare if target addresses don't match.
-        Je.clear();
-        Je.setOpcode(X86::JCC_1);
-        if (Targets[i].first)
-          Je.addOperand(MCOperand::createExpr(MCSymbolRefExpr::create(
-              Targets[i].first, MCSymbolRefExpr::VK_None, *Ctx)));
-        else
-          Je.addOperand(MCOperand::createImm(Targets[i].second));
+    static uint8_t getMemDataSize(const MCInst &Inst, int MemOpNo) {
+        using namespace llvm::X86;
+        int OpType = getOperandType(Inst.getOpcode(), MemOpNo);
+        return getMemOperandSize(OpType) / 8;
+    }
 
-        Je.addOperand(MCOperand::createImm(X86::COND_E));
-        assert(!isInvoke(CallInst));
-      } else {
-        MCInst &Jne = NewCall->back();
+    /// Classifying a stack access as *not* "SIMPLE" here means we don't know
+    /// how to change this instruction memory access. It will disable any
+    /// changes to the stack layout, so we can't do the most aggressive form of
+    /// shrink wrapping. We must do so in a way that keeps the original stack
+    /// layout. Otherwise you need to adjust the offset of all instructions
+    /// accessing the stack: we can't do that anymore because there is one
+    /// instruction that is not simple. There are other implications as well. We
+    /// have heuristics to detect when a register is callee-saved and thus
+    /// eligible for shrink wrapping. If you are restoring a register using a
+    /// non-simple stack access, then it is classified as NOT callee-saved, and
+    /// it disables shrink wrapping for *that* register (but not for others).
+    ///
+    /// Classifying a stack access as "size 0" or detecting an indexed memory
+    /// access (to address a vector, for example) here means we know there is a
+    /// stack access, but we can't quite understand how wide is the access in
+    /// bytes. This is very serious because we can't understand how memory
+    /// accesses alias with each other for this function. This will essentially
+    /// disable not only shrink wrapping but all frame analysis, it will fail it
+    /// as "we don't understand this function and we give up on it".
+    bool isStackAccess(const MCInst &Inst, bool &IsLoad, bool &IsStore,
+                       bool &IsStoreFromReg, MCPhysReg &Reg, int32_t &SrcImm,
+                       uint16_t &StackPtrReg, int64_t &StackOffset,
+                       uint8_t &Size, bool &IsSimple,
+                       bool &IsIndexed) const override {
+        // Detect simple push/pop cases first
+        if (int Sz = getPushSize(Inst)) {
+            IsLoad         = false;
+            IsStore        = true;
+            IsStoreFromReg = true;
+            StackPtrReg    = X86::RSP;
+            StackOffset    = -Sz;
+            Size           = Sz;
+            IsSimple       = true;
+            if (Inst.getOperand(0).isImm())
+                SrcImm = Inst.getOperand(0).getImm();
+            else if (Inst.getOperand(0).isReg())
+                Reg = Inst.getOperand(0).getReg();
+            else
+                IsSimple = false;
+
+            return true;
+        }
+        if (int Sz = getPopSize(Inst)) {
+            IsLoad  = true;
+            IsStore = false;
+            if (Inst.getNumOperands() == 0 || !Inst.getOperand(0).isReg()) {
+                IsSimple = false;
+            } else {
+                Reg      = Inst.getOperand(0).getReg();
+                IsSimple = true;
+            }
+            StackPtrReg = X86::RSP;
+            StackOffset = 0;
+            Size        = Sz;
+            return true;
+        }
 
-        // Jump to next compare if target addresses don't match.
-        Jne.clear();
-        Jne.setOpcode(X86::JCC_1);
-        Jne.addOperand(MCOperand::createExpr(MCSymbolRefExpr::create(
-            NextTarget, MCSymbolRefExpr::VK_None, *Ctx)));
-        Jne.addOperand(MCOperand::createImm(X86::COND_NE));
+        struct InstInfo {
+            // Size in bytes that Inst loads from memory.
+            uint8_t DataSize;
+            bool    IsLoad;
+            bool    IsStore;
+            bool    StoreFromReg;
+            bool    Simple;
+        };
+
+        InstInfo           I;
+        int                MemOpNo = getMemoryOperandNo(Inst);
+        const MCInstrDesc &MCII    = Info->get(Inst.getOpcode());
+        // If it is not dealing with a memory operand, we discard it
+        if (MemOpNo == -1 || MCII.isCall())
+            return false;
 
-        // Call specific target directly.
-        Results.emplace_back(Ctx->createNamedTempSymbol(),
-                             InstructionListType());
-        NewCall = &Results.back().second;
-        NewCall->push_back(CallInst);
-        MCInst &CallOrJmp = NewCall->back();
+        switch (Inst.getOpcode()) {
+        default: {
+            bool IsLoad  = MCII.mayLoad();
+            bool IsStore = MCII.mayStore();
+            // Is it LEA? (deals with memory but is not loading nor storing)
+            if (!IsLoad && !IsStore) {
+                I = {0, IsLoad, IsStore, false, false};
+                break;
+            }
+            uint8_t Sz = getMemDataSize(Inst, MemOpNo);
+            I          = {Sz, IsLoad, IsStore, false, false};
+            break;
+        }
+        // Report simple stack accesses
+        case X86::MOV8rm:
+            I = {1, true, false, false, true};
+            break;
+        case X86::MOV16rm:
+            I = {2, true, false, false, true};
+            break;
+        case X86::MOV32rm:
+            I = {4, true, false, false, true};
+            break;
+        case X86::MOV64rm:
+            I = {8, true, false, false, true};
+            break;
+        case X86::MOV8mr:
+            I = {1, false, true, true, true};
+            break;
+        case X86::MOV16mr:
+            I = {2, false, true, true, true};
+            break;
+        case X86::MOV32mr:
+            I = {4, false, true, true, true};
+            break;
+        case X86::MOV64mr:
+            I = {8, false, true, true, true};
+            break;
+        case X86::MOV8mi:
+            I = {1, false, true, false, true};
+            break;
+        case X86::MOV16mi:
+            I = {2, false, true, false, true};
+            break;
+        case X86::MOV32mi:
+            I = {4, false, true, false, true};
+            break;
+        }   // end switch (Inst.getOpcode())
 
-        CallOrJmp.clear();
+        std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Inst);
+        if (!MO) {
+            LLVM_DEBUG(dbgs() << "Evaluate failed on ");
+            LLVM_DEBUG(Inst.dump());
+            return false;
+        }
 
-        if (MinimizeCodeSize && !LoadElim) {
-          CallOrJmp.setOpcode(IsTailCall ? X86::JMP32r : X86::CALL64r);
-          CallOrJmp.addOperand(MCOperand::createReg(FuncAddrReg));
-        } else {
-          CallOrJmp.setOpcode(IsTailCall ? X86::JMP_4 : X86::CALL64pcrel32);
+        // Make sure it's a stack access
+        if (MO->BaseRegNum != X86::RBP && MO->BaseRegNum != X86::RSP)
+            return false;
 
-          if (Targets[i].first)
-            CallOrJmp.addOperand(MCOperand::createExpr(MCSymbolRefExpr::create(
-                Targets[i].first, MCSymbolRefExpr::VK_None, *Ctx)));
-          else
-            CallOrJmp.addOperand(MCOperand::createImm(Targets[i].second));
+        IsLoad         = I.IsLoad;
+        IsStore        = I.IsStore;
+        IsStoreFromReg = I.StoreFromReg;
+        Size           = I.DataSize;
+        IsSimple       = I.Simple;
+        StackPtrReg    = MO->BaseRegNum;
+        StackOffset    = MO->DispImm;
+        IsIndexed      = MO->IndexRegNum != X86::NoRegister ||
+                    MO->SegRegNum != X86::NoRegister;
+
+        if (!I.Simple)
+            return true;
+
+        // Retrieve related register in simple MOV from/to stack operations.
+        unsigned MemOpOffset = static_cast<unsigned>(MemOpNo);
+        if (I.IsLoad) {
+            MCOperand RegOpnd = Inst.getOperand(0);
+            assert(RegOpnd.isReg() && "unexpected destination operand");
+            Reg = RegOpnd.getReg();
+        } else if (I.IsStore) {
+            MCOperand SrcOpnd =
+                Inst.getOperand(MemOpOffset + X86::AddrSegmentReg + 1);
+            if (I.StoreFromReg) {
+                assert(SrcOpnd.isReg() && "unexpected source operand");
+                Reg = SrcOpnd.getReg();
+            } else {
+                assert(SrcOpnd.isImm() && "unexpected source operand");
+                SrcImm = SrcOpnd.getImm();
+            }
         }
-        if (IsTailCall)
-          setTailCall(CallOrJmp);
-
-        if (CallOrJmp.getOpcode() == X86::CALL64r ||
-            CallOrJmp.getOpcode() == X86::CALL64pcrel32) {
-          if (std::optional<uint32_t> Offset = getOffset(CallInst))
-            // Annotated as duplicated call
-            setOffset(CallOrJmp, *Offset);
-        }
-
-        if (isInvoke(CallInst) && !isInvoke(CallOrJmp)) {
-          // Copy over any EH or GNU args size information from the original
-          // call.
-          std::optional<MCPlus::MCLandingPad> EHInfo = getEHInfo(CallInst);
-          if (EHInfo)
-            addEHInfo(CallOrJmp, *EHInfo);
-          int64_t GnuArgsSize = getGnuArgsSize(CallInst);
-          if (GnuArgsSize >= 0)
-            addGnuArgsSize(CallOrJmp, GnuArgsSize);
-        }
-
-        if (!IsTailCall) {
-          // The fallthrough block for the most common target should be
-          // the merge block.
-          if (i == 0) {
-            // Fallthrough to merge block.
-            MergeBlock = Ctx->createNamedTempSymbol();
-          } else {
-            // Insert jump to the merge block if we are not doing a fallthrough.
-            jumpToMergeBlock(*NewCall);
-          }
-        }
-      }
-    }
-
-    // Cold call block.
-    Results.emplace_back(NextTarget, InstructionListType());
-    InstructionListType &NewCall = Results.back().second;
-    for (const MCInst *Inst : MethodFetchInsns)
-      if (Inst != &CallInst)
-        NewCall.push_back(*Inst);
-    NewCall.push_back(CallInst);
-
-    // Jump to merge block from cold call block
-    if (!IsTailCall && !IsJumpTable) {
-      jumpToMergeBlock(NewCall);
-
-      // Record merge block
-      Results.emplace_back(MergeBlock, InstructionListType());
-    }
-
-    return Results;
-  }
-
-  BlocksVectorTy jumpTablePromotion(
-      const MCInst &IJmpInst,
-      const std::vector<std::pair<MCSymbol *, uint64_t>> &Targets,
-      const std::vector<MCInst *> &TargetFetchInsns,
-      MCContext *Ctx) const override {
-    assert(getJumpTable(IJmpInst) != 0);
-    uint16_t IndexReg = getAnnotationAs<uint16_t>(IJmpInst, "JTIndexReg");
-    if (IndexReg == 0)
-      return BlocksVectorTy();
-
-    BlocksVectorTy Results;
-
-    // Label for the current code block.
-    MCSymbol *NextTarget = nullptr;
-
-    for (unsigned int i = 0; i < Targets.size(); ++i) {
-      Results.emplace_back(NextTarget, InstructionListType());
-      InstructionListType *CurBB = &Results.back().second;
-
-      // Compare current index to a specific index.
-      CurBB->emplace_back(MCInst());
-      MCInst &CompareInst = CurBB->back();
-      CompareInst.setLoc(IJmpInst.getLoc());
-      CompareInst.setOpcode(X86::CMP64ri32);
-      CompareInst.addOperand(MCOperand::createReg(IndexReg));
-
-      const uint64_t CaseIdx = Targets[i].second;
-      // Immediate address is out of sign extended 32 bit range.
-      if (int64_t(CaseIdx) != int64_t(int32_t(CaseIdx)))
-        return BlocksVectorTy();
-
-      CompareInst.addOperand(MCOperand::createImm(CaseIdx));
-      shortenInstruction(CompareInst, *Ctx->getSubtargetInfo());
-
-      // jump to next target compare.
-      NextTarget =
-          Ctx->createNamedTempSymbol(); // generate label for the next block
-      CurBB->push_back(MCInst());
-
-      MCInst &JEInst = CurBB->back();
-      JEInst.setLoc(IJmpInst.getLoc());
-
-      // Jump to target if indices match
-      JEInst.setOpcode(X86::JCC_1);
-      JEInst.addOperand(MCOperand::createExpr(MCSymbolRefExpr::create(
-          Targets[i].first, MCSymbolRefExpr::VK_None, *Ctx)));
-      JEInst.addOperand(MCOperand::createImm(X86::COND_E));
-    }
-
-    // Cold call block.
-    Results.emplace_back(NextTarget, InstructionListType());
-    InstructionListType &CurBB = Results.back().second;
-    for (const MCInst *Inst : TargetFetchInsns)
-      if (Inst != &IJmpInst)
-        CurBB.push_back(*Inst);
-
-    CurBB.push_back(IJmpInst);
-
-    return Results;
-  }
-
-private:
-  bool createMove(MCInst &Inst, const MCSymbol *Src, unsigned Reg,
-                  MCContext *Ctx) const {
-    Inst.setOpcode(X86::MOV64rm);
-    Inst.addOperand(MCOperand::createReg(Reg));
-    Inst.addOperand(MCOperand::createReg(X86::RIP));        // BaseReg
-    Inst.addOperand(MCOperand::createImm(1));               // ScaleAmt
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // IndexReg
-    Inst.addOperand(MCOperand::createExpr(
-        MCSymbolRefExpr::create(Src, MCSymbolRefExpr::VK_None,
-                                *Ctx)));                    // Displacement
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // AddrSegmentReg
-
-    return true;
-  }
-
-  bool createLea(MCInst &Inst, const MCSymbol *Src, unsigned Reg,
-                 MCContext *Ctx) const {
-    Inst.setOpcode(X86::LEA64r);
-    Inst.addOperand(MCOperand::createReg(Reg));
-    Inst.addOperand(MCOperand::createReg(X86::RIP));        // BaseReg
-    Inst.addOperand(MCOperand::createImm(1));               // ScaleAmt
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // IndexReg
-    Inst.addOperand(MCOperand::createExpr(
-        MCSymbolRefExpr::create(Src, MCSymbolRefExpr::VK_None,
-                                *Ctx)));                    // Displacement
-    Inst.addOperand(MCOperand::createReg(X86::NoRegister)); // AddrSegmentReg
-    return true;
-  }
+
+        return true;
+    }
+
+    void changeToPushOrPop(MCInst &Inst) const override {
+        assert(!isPush(Inst) && !isPop(Inst));
+
+        struct InstInfo {
+            // Size in bytes that Inst loads from memory.
+            uint8_t DataSize;
+            bool    IsLoad;
+            bool    StoreFromReg;
+        };
+
+        InstInfo I;
+        switch (Inst.getOpcode()) {
+        default: {
+            llvm_unreachable("Unhandled opcode");
+            return;
+        }
+        case X86::MOV16rm:
+            I = {2, true, false};
+            break;
+        case X86::MOV32rm:
+            I = {4, true, false};
+            break;
+        case X86::MOV64rm:
+            I = {8, true, false};
+            break;
+        case X86::MOV16mr:
+            I = {2, false, true};
+            break;
+        case X86::MOV32mr:
+            I = {4, false, true};
+            break;
+        case X86::MOV64mr:
+            I = {8, false, true};
+            break;
+        case X86::MOV16mi:
+            I = {2, false, false};
+            break;
+        case X86::MOV32mi:
+            I = {4, false, false};
+            break;
+        }   // end switch (Inst.getOpcode())
+
+        std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Inst);
+        if (!MO) {
+            llvm_unreachable("Evaluate failed");
+            return;
+        }
+        // Make sure it's a stack access
+        if (MO->BaseRegNum != X86::RBP && MO->BaseRegNum != X86::RSP) {
+            llvm_unreachable("Not a stack access");
+            return;
+        }
+
+        unsigned MemOpOffset = getMemoryOperandNo(Inst);
+        unsigned NewOpcode   = 0;
+        if (I.IsLoad) {
+            switch (I.DataSize) {
+            case 2:
+                NewOpcode = X86::POP16r;
+                break;
+            case 4:
+                NewOpcode = X86::POP32r;
+                break;
+            case 8:
+                NewOpcode = X86::POP64r;
+                break;
+            default:
+                llvm_unreachable("Unexpected size");
+            }
+            unsigned RegOpndNum = Inst.getOperand(0).getReg();
+            Inst.clear();
+            Inst.setOpcode(NewOpcode);
+            Inst.addOperand(MCOperand::createReg(RegOpndNum));
+        } else {
+            MCOperand SrcOpnd =
+                Inst.getOperand(MemOpOffset + X86::AddrSegmentReg + 1);
+            if (I.StoreFromReg) {
+                switch (I.DataSize) {
+                case 2:
+                    NewOpcode = X86::PUSH16r;
+                    break;
+                case 4:
+                    NewOpcode = X86::PUSH32r;
+                    break;
+                case 8:
+                    NewOpcode = X86::PUSH64r;
+                    break;
+                default:
+                    llvm_unreachable("Unexpected size");
+                }
+                assert(SrcOpnd.isReg() && "Unexpected source operand");
+                unsigned RegOpndNum = SrcOpnd.getReg();
+                Inst.clear();
+                Inst.setOpcode(NewOpcode);
+                Inst.addOperand(MCOperand::createReg(RegOpndNum));
+            } else {
+                switch (I.DataSize) {
+                case 2:
+                    NewOpcode = X86::PUSH16i8;
+                    break;
+                case 4:
+                    NewOpcode = X86::PUSH32i8;
+                    break;
+                case 8:
+                    NewOpcode = X86::PUSH64i32;
+                    break;
+                default:
+                    llvm_unreachable("Unexpected size");
+                }
+                assert(SrcOpnd.isImm() && "Unexpected source operand");
+                int64_t SrcImm = SrcOpnd.getImm();
+                Inst.clear();
+                Inst.setOpcode(NewOpcode);
+                Inst.addOperand(MCOperand::createImm(SrcImm));
+            }
+        }
+    }
+
+    bool isStackAdjustment(const MCInst &Inst) const override {
+        switch (Inst.getOpcode()) {
+        default:
+            return false;
+        case X86::SUB64ri32:
+        case X86::SUB64ri8:
+        case X86::ADD64ri32:
+        case X86::ADD64ri8:
+        case X86::LEA64r:
+            break;
+        }
+
+        const MCInstrDesc &MCII = Info->get(Inst.getOpcode());
+        for (int I = 0, E = MCII.getNumDefs(); I != E; ++I) {
+            const MCOperand &Operand = Inst.getOperand(I);
+            if (Operand.isReg() && Operand.getReg() == X86::RSP)
+                return true;
+        }
+        return false;
+    }
+
+    bool evaluateStackOffsetExpr(
+        const MCInst &Inst, int64_t &Output,
+        std::pair<MCPhysReg, int64_t> Input1,
+        std::pair<MCPhysReg, int64_t> Input2) const override {
+
+        auto getOperandVal = [&](MCPhysReg Reg) -> ErrorOr<int64_t> {
+            if (Reg == Input1.first)
+                return Input1.second;
+            if (Reg == Input2.first)
+                return Input2.second;
+            return make_error_code(errc::result_out_of_range);
+        };
+
+        switch (Inst.getOpcode()) {
+        default:
+            return false;
+
+        case X86::SUB64ri32:
+        case X86::SUB64ri8:
+            if (!Inst.getOperand(2).isImm())
+                return false;
+            if (ErrorOr<int64_t> InputVal =
+                    getOperandVal(Inst.getOperand(1).getReg()))
+                Output = *InputVal - Inst.getOperand(2).getImm();
+            else
+                return false;
+            break;
+        case X86::ADD64ri32:
+        case X86::ADD64ri8:
+            if (!Inst.getOperand(2).isImm())
+                return false;
+            if (ErrorOr<int64_t> InputVal =
+                    getOperandVal(Inst.getOperand(1).getReg()))
+                Output = *InputVal + Inst.getOperand(2).getImm();
+            else
+                return false;
+            break;
+        case X86::ADD64i32:
+            if (!Inst.getOperand(0).isImm())
+                return false;
+            if (ErrorOr<int64_t> InputVal = getOperandVal(X86::RAX))
+                Output = *InputVal + Inst.getOperand(0).getImm();
+            else
+                return false;
+            break;
+
+        case X86::LEA64r: {
+            std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(Inst);
+            if (!MO)
+                return false;
+
+            if (MO->BaseRegNum == X86::NoRegister ||
+                MO->IndexRegNum != X86::NoRegister ||
+                MO->SegRegNum != X86::NoRegister || MO->DispExpr)
+                return false;
+
+            if (ErrorOr<int64_t> InputVal = getOperandVal(MO->BaseRegNum))
+                Output = *InputVal + MO->DispImm;
+            else
+                return false;
+
+            break;
+        }
+        }
+        return true;
+    }
+
+    bool isRegToRegMove(const MCInst &Inst, MCPhysReg &From,
+                        MCPhysReg &To) const override {
+        switch (Inst.getOpcode()) {
+        default:
+            return false;
+        case X86::LEAVE:
+        case X86::LEAVE64:
+            To   = getStackPointer();
+            From = getFramePointer();
+            return true;
+        case X86::MOV64rr:
+            To   = Inst.getOperand(0).getReg();
+            From = Inst.getOperand(1).getReg();
+            return true;
+        }
+    }
+
+    MCPhysReg getStackPointer() const override { return X86::RSP; }
+    MCPhysReg getFramePointer() const override { return X86::RBP; }
+    MCPhysReg getFlagsReg() const override { return X86::EFLAGS; }
+
+    bool escapesVariable(const MCInst &Inst,
+                         bool          HasFramePointer) const override {
+        int                MemOpNo = getMemoryOperandNo(Inst);
+        const MCInstrDesc &MCII    = Info->get(Inst.getOpcode());
+        const unsigned     NumDefs = MCII.getNumDefs();
+        static BitVector   SPBPAliases(BitVector(getAliases(X86::RSP)) |=
+                                       getAliases(X86::RBP));
+        static BitVector   SPAliases(getAliases(X86::RSP));
+
+        // FIXME: PUSH can be technically a leak, but let's ignore this for now
+        // because a lot of harmless prologue code will spill SP to the stack.
+        // Unless push is clearly pushing an object address to the stack as
+        // demonstrated by having a MemOp.
+        bool IsPush = isPush(Inst);
+        if (IsPush && MemOpNo == -1)
+            return false;
+
+        // We use this to detect LEA (has memop but does not access mem)
+        bool AccessMem = MCII.mayLoad() || MCII.mayStore();
+        bool DoesLeak  = false;
+        for (int I = 0, E = MCPlus::getNumPrimeOperands(Inst); I != E; ++I) {
+            // Ignore if SP/BP is used to dereference memory -- that's fine
+            if (MemOpNo != -1 && !IsPush && AccessMem && I >= MemOpNo &&
+                I <= MemOpNo + 5)
+                continue;
+            // Ignore if someone is writing to SP/BP
+            if (I < static_cast<int>(NumDefs))
+                continue;
+
+            const MCOperand &Operand = Inst.getOperand(I);
+            if (HasFramePointer && Operand.isReg() &&
+                SPBPAliases[Operand.getReg()]) {
+                DoesLeak = true;
+                break;
+            }
+            if (!HasFramePointer && Operand.isReg() &&
+                SPAliases[Operand.getReg()]) {
+                DoesLeak = true;
+                break;
+            }
+        }
+
+        // If potential leak, check if it is not just writing to itself/sp/bp
+        if (DoesLeak) {
+            for (int I = 0, E = NumDefs; I != E; ++I) {
+                const MCOperand &Operand = Inst.getOperand(I);
+                if (HasFramePointer && Operand.isReg() &&
+                    SPBPAliases[Operand.getReg()]) {
+                    DoesLeak = false;
+                    break;
+                }
+                if (!HasFramePointer && Operand.isReg() &&
+                    SPAliases[Operand.getReg()]) {
+                    DoesLeak = false;
+                    break;
+                }
+            }
+        }
+        return DoesLeak;
+    }
+
+    bool addToImm(MCInst &Inst, int64_t &Amt, MCContext *Ctx) const override {
+        unsigned ImmOpNo = -1U;
+        int      MemOpNo = getMemoryOperandNo(Inst);
+        if (MemOpNo != -1)
+            ImmOpNo = MemOpNo + X86::AddrDisp;
+        else
+            for (unsigned Index = 0; Index < MCPlus::getNumPrimeOperands(Inst);
+                 ++Index)
+                if (Inst.getOperand(Index).isImm())
+                    ImmOpNo = Index;
+        if (ImmOpNo == -1U)
+            return false;
+
+        MCOperand &Operand = Inst.getOperand(ImmOpNo);
+        Amt += Operand.getImm();
+        Operand.setImm(Amt);
+        // Check for the need for relaxation
+        if (int64_t(Amt) == int64_t(int8_t(Amt)))
+            return true;
+
+        // Relax instruction
+        switch (Inst.getOpcode()) {
+        case X86::SUB64ri8:
+            Inst.setOpcode(X86::SUB64ri32);
+            break;
+        case X86::ADD64ri8:
+            Inst.setOpcode(X86::ADD64ri32);
+            break;
+        default:
+            // No need for relaxation
+            break;
+        }
+        return true;
+    }
+
+    /// TODO: this implementation currently works for the most common opcodes
+    /// that load from memory. It can be extended to work with memory store
+    /// opcodes as well as more memory load opcodes.
+    bool replaceMemOperandWithImm(MCInst &Inst, StringRef ConstantData,
+                                  uint64_t Offset) const override {
+        enum CheckSignExt : uint8_t {
+            NOCHECK = 0,
+            CHECK8,
+            CHECK32,
+        };
+
+        using CheckList = std::vector<std::pair<CheckSignExt, unsigned>>;
+        struct InstInfo {
+            // Size in bytes that Inst loads from memory.
+            uint8_t DataSize;
+
+            // True when the target operand has to be duplicated because the
+            // opcode expects a LHS operand.
+            bool HasLHS;
+
+            // List of checks and corresponding opcodes to be used. We try to
+            // use the smallest possible immediate value when various sizes are
+            // available, hence we may need to check whether a larger constant
+            // fits in a smaller immediate.
+            CheckList Checks;
+        };
+
+        InstInfo I;
+
+        switch (Inst.getOpcode()) {
+        default: {
+            switch (getPopSize(Inst)) {
+            case 2:
+                I = {2, false, {{NOCHECK, X86::MOV16ri}}};
+                break;
+            case 4:
+                I = {4, false, {{NOCHECK, X86::MOV32ri}}};
+                break;
+            case 8:
+                I = {8,
+                     false,
+                     {{CHECK32, X86::MOV64ri32}, {NOCHECK, X86::MOV64rm}}};
+                break;
+            default:
+                return false;
+            }
+            break;
+        }
+
+        // MOV
+        case X86::MOV8rm:
+            I = {1, false, {{NOCHECK, X86::MOV8ri}}};
+            break;
+        case X86::MOV16rm:
+            I = {2, false, {{NOCHECK, X86::MOV16ri}}};
+            break;
+        case X86::MOV32rm:
+            I = {4, false, {{NOCHECK, X86::MOV32ri}}};
+            break;
+        case X86::MOV64rm:
+            I = {
+                8, false, {{CHECK32, X86::MOV64ri32}, {NOCHECK, X86::MOV64rm}}};
+            break;
+
+        // MOVZX
+        case X86::MOVZX16rm8:
+            I = {1, false, {{NOCHECK, X86::MOV16ri}}};
+            break;
+        case X86::MOVZX32rm8:
+            I = {1, false, {{NOCHECK, X86::MOV32ri}}};
+            break;
+        case X86::MOVZX32rm16:
+            I = {2, false, {{NOCHECK, X86::MOV32ri}}};
+            break;
+
+        // CMP
+        case X86::CMP8rm:
+            I = {1, false, {{NOCHECK, X86::CMP8ri}}};
+            break;
+        case X86::CMP16rm:
+            I = {2, false, {{CHECK8, X86::CMP16ri8}, {NOCHECK, X86::CMP16ri}}};
+            break;
+        case X86::CMP32rm:
+            I = {4, false, {{CHECK8, X86::CMP32ri8}, {NOCHECK, X86::CMP32ri}}};
+            break;
+        case X86::CMP64rm:
+            I = {8,
+                 false,
+                 {{CHECK8, X86::CMP64ri8},
+                  {CHECK32, X86::CMP64ri32},
+                  {NOCHECK, X86::CMP64rm}}};
+            break;
+
+        // TEST
+        case X86::TEST8mr:
+            I = {1, false, {{NOCHECK, X86::TEST8ri}}};
+            break;
+        case X86::TEST16mr:
+            I = {2, false, {{NOCHECK, X86::TEST16ri}}};
+            break;
+        case X86::TEST32mr:
+            I = {4, false, {{NOCHECK, X86::TEST32ri}}};
+            break;
+        case X86::TEST64mr:
+            I = {8,
+                 false,
+                 {{CHECK32, X86::TEST64ri32}, {NOCHECK, X86::TEST64mr}}};
+            break;
+
+        // ADD
+        case X86::ADD8rm:
+            I = {1, true, {{NOCHECK, X86::ADD8ri}}};
+            break;
+        case X86::ADD16rm:
+            I = {2, true, {{CHECK8, X86::ADD16ri8}, {NOCHECK, X86::ADD16ri}}};
+            break;
+        case X86::ADD32rm:
+            I = {4, true, {{CHECK8, X86::ADD32ri8}, {NOCHECK, X86::ADD32ri}}};
+            break;
+        case X86::ADD64rm:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::ADD64ri8},
+                  {CHECK32, X86::ADD64ri32},
+                  {NOCHECK, X86::ADD64rm}}};
+            break;
+
+        // SUB
+        case X86::SUB8rm:
+            I = {1, true, {{NOCHECK, X86::SUB8ri}}};
+            break;
+        case X86::SUB16rm:
+            I = {2, true, {{CHECK8, X86::SUB16ri8}, {NOCHECK, X86::SUB16ri}}};
+            break;
+        case X86::SUB32rm:
+            I = {4, true, {{CHECK8, X86::SUB32ri8}, {NOCHECK, X86::SUB32ri}}};
+            break;
+        case X86::SUB64rm:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::SUB64ri8},
+                  {CHECK32, X86::SUB64ri32},
+                  {NOCHECK, X86::SUB64rm}}};
+            break;
+
+        // AND
+        case X86::AND8rm:
+            I = {1, true, {{NOCHECK, X86::AND8ri}}};
+            break;
+        case X86::AND16rm:
+            I = {2, true, {{CHECK8, X86::AND16ri8}, {NOCHECK, X86::AND16ri}}};
+            break;
+        case X86::AND32rm:
+            I = {4, true, {{CHECK8, X86::AND32ri8}, {NOCHECK, X86::AND32ri}}};
+            break;
+        case X86::AND64rm:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::AND64ri8},
+                  {CHECK32, X86::AND64ri32},
+                  {NOCHECK, X86::AND64rm}}};
+            break;
+
+        // OR
+        case X86::OR8rm:
+            I = {1, true, {{NOCHECK, X86::OR8ri}}};
+            break;
+        case X86::OR16rm:
+            I = {2, true, {{CHECK8, X86::OR16ri8}, {NOCHECK, X86::OR16ri}}};
+            break;
+        case X86::OR32rm:
+            I = {4, true, {{CHECK8, X86::OR32ri8}, {NOCHECK, X86::OR32ri}}};
+            break;
+        case X86::OR64rm:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::OR64ri8},
+                  {CHECK32, X86::OR64ri32},
+                  {NOCHECK, X86::OR64rm}}};
+            break;
+
+        // XOR
+        case X86::XOR8rm:
+            I = {1, true, {{NOCHECK, X86::XOR8ri}}};
+            break;
+        case X86::XOR16rm:
+            I = {2, true, {{CHECK8, X86::XOR16ri8}, {NOCHECK, X86::XOR16ri}}};
+            break;
+        case X86::XOR32rm:
+            I = {4, true, {{CHECK8, X86::XOR32ri8}, {NOCHECK, X86::XOR32ri}}};
+            break;
+        case X86::XOR64rm:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::XOR64ri8},
+                  {CHECK32, X86::XOR64ri32},
+                  {NOCHECK, X86::XOR64rm}}};
+            break;
+        }
+
+        // Compute the immediate value.
+        assert(Offset + I.DataSize <= ConstantData.size() &&
+               "invalid offset for given constant data");
+        int64_t ImmVal =
+            DataExtractor(ConstantData, true, 8).getSigned(&Offset, I.DataSize);
+
+        // Compute the new opcode.
+        unsigned NewOpcode = 0;
+        for (const std::pair<CheckSignExt, unsigned> &Check : I.Checks) {
+            NewOpcode = Check.second;
+            if (Check.first == NOCHECK)
+                break;
+            if (Check.first == CHECK8 && isInt<8>(ImmVal))
+                break;
+            if (Check.first == CHECK32 && isInt<32>(ImmVal))
+                break;
+        }
+        if (NewOpcode == Inst.getOpcode())
+            return false;
+
+        // Modify the instruction.
+        MCOperand ImmOp       = MCOperand::createImm(ImmVal);
+        uint32_t  TargetOpNum = 0;
+        // Test instruction does not follow the regular pattern of putting the
+        // memory reference of a load (5 MCOperands) last in the list of
+        // operands. Since it is not modifying the register operand, it is not
+        // treated as a destination operand and it is not the first operand as
+        // it is in the other instructions we treat here.
+        if (NewOpcode == X86::TEST8ri || NewOpcode == X86::TEST16ri ||
+            NewOpcode == X86::TEST32ri || NewOpcode == X86::TEST64ri32)
+            TargetOpNum = getMemoryOperandNo(Inst) + X86::AddrNumOperands;
+
+        MCOperand TargetOp = Inst.getOperand(TargetOpNum);
+        Inst.clear();
+        Inst.setOpcode(NewOpcode);
+        Inst.addOperand(TargetOp);
+        if (I.HasLHS)
+            Inst.addOperand(TargetOp);
+        Inst.addOperand(ImmOp);
+
+        return true;
+    }
+
+    /// TODO: this implementation currently works for the most common opcodes
+    /// that load from memory. It can be extended to work with memory store
+    /// opcodes as well as more memory load opcodes.
+    bool replaceMemOperandWithReg(MCInst   &Inst,
+                                  MCPhysReg RegNum) const override {
+        unsigned NewOpcode;
+
+        switch (Inst.getOpcode()) {
+        default: {
+            switch (getPopSize(Inst)) {
+            case 2:
+                NewOpcode = X86::MOV16rr;
+                break;
+            case 4:
+                NewOpcode = X86::MOV32rr;
+                break;
+            case 8:
+                NewOpcode = X86::MOV64rr;
+                break;
+            default:
+                return false;
+            }
+            break;
+        }
+
+        // MOV
+        case X86::MOV8rm:
+            NewOpcode = X86::MOV8rr;
+            break;
+        case X86::MOV16rm:
+            NewOpcode = X86::MOV16rr;
+            break;
+        case X86::MOV32rm:
+            NewOpcode = X86::MOV32rr;
+            break;
+        case X86::MOV64rm:
+            NewOpcode = X86::MOV64rr;
+            break;
+        }
+
+        // Modify the instruction.
+        MCOperand RegOp    = MCOperand::createReg(RegNum);
+        MCOperand TargetOp = Inst.getOperand(0);
+        Inst.clear();
+        Inst.setOpcode(NewOpcode);
+        Inst.addOperand(TargetOp);
+        Inst.addOperand(RegOp);
+
+        return true;
+    }
+
+    bool isRedundantMove(const MCInst &Inst) const override {
+        switch (Inst.getOpcode()) {
+        default:
+            return false;
+
+        // MOV
+        case X86::MOV8rr:
+        case X86::MOV16rr:
+        case X86::MOV32rr:
+        case X86::MOV64rr:
+            break;
+        }
+
+        assert(Inst.getOperand(0).isReg() && Inst.getOperand(1).isReg());
+        return Inst.getOperand(0).getReg() == Inst.getOperand(1).getReg();
+    }
+
+    bool requiresAlignedAddress(const MCInst &Inst) const override {
+        const MCInstrDesc &Desc = Info->get(Inst.getOpcode());
+        for (unsigned int I = 0; I < Desc.getNumOperands(); ++I) {
+            const MCOperandInfo &Op = Desc.operands()[I];
+            if (Op.OperandType != MCOI::OPERAND_REGISTER)
+                continue;
+            if (Op.RegClass == X86::VR128RegClassID)
+                return true;
+        }
+        return false;
+    }
+
+    bool convertJmpToTailCall(MCInst &Inst) override {
+        if (isTailCall(Inst))
+            return false;
+
+        int NewOpcode;
+        switch (Inst.getOpcode()) {
+        default:
+            return false;
+        case X86::JMP_1:
+        case X86::JMP_2:
+        case X86::JMP_4:
+            NewOpcode = X86::JMP_4;
+            break;
+        case X86::JMP16m:
+        case X86::JMP32m:
+        case X86::JMP64m:
+            NewOpcode = X86::JMP32m;
+            break;
+        case X86::JMP16r:
+        case X86::JMP32r:
+        case X86::JMP64r:
+            NewOpcode = X86::JMP32r;
+            break;
+        }
+
+        Inst.setOpcode(NewOpcode);
+        setTailCall(Inst);
+        return true;
+    }
+
+    bool convertTailCallToJmp(MCInst &Inst) override {
+        int NewOpcode;
+        switch (Inst.getOpcode()) {
+        default:
+            return false;
+        case X86::JMP_4:
+            NewOpcode = X86::JMP_1;
+            break;
+        case X86::JMP32m:
+            NewOpcode = X86::JMP64m;
+            break;
+        case X86::JMP32r:
+            NewOpcode = X86::JMP64r;
+            break;
+        }
+
+        Inst.setOpcode(NewOpcode);
+        removeAnnotation(Inst, MCPlus::MCAnnotation::kTailCall);
+        clearOffset(Inst);
+        return true;
+    }
+
+    bool convertTailCallToCall(MCInst &Inst) override {
+        int NewOpcode;
+        switch (Inst.getOpcode()) {
+        default:
+            return false;
+        case X86::JMP_4:
+            NewOpcode = X86::CALL64pcrel32;
+            break;
+        case X86::JMP32m:
+            NewOpcode = X86::CALL64m;
+            break;
+        case X86::JMP32r:
+            NewOpcode = X86::CALL64r;
+            break;
+        }
+
+        Inst.setOpcode(NewOpcode);
+        removeAnnotation(Inst, MCPlus::MCAnnotation::kTailCall);
+        return true;
+    }
+
+    bool convertCallToIndirectCall(MCInst &Inst, const MCSymbol *TargetLocation,
+                                   MCContext *Ctx) override {
+        assert((Inst.getOpcode() == X86::CALL64pcrel32 ||
+                (Inst.getOpcode() == X86::JMP_4 && isTailCall(Inst))) &&
+               "64-bit direct (tail) call instruction expected");
+        const auto NewOpcode = (Inst.getOpcode() == X86::CALL64pcrel32)
+                                   ? X86::CALL64m
+                                   : X86::JMP32m;
+        Inst.setOpcode(NewOpcode);
+
+        // Replace the first operand and preserve auxiliary operands of
+        // the instruction.
+        Inst.erase(Inst.begin());
+        Inst.insert(Inst.begin(),
+                    MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+        Inst.insert(Inst.begin(),
+                    MCOperand::createExpr(   // Displacement
+                        MCSymbolRefExpr::create(
+                            TargetLocation, MCSymbolRefExpr::VK_None, *Ctx)));
+        Inst.insert(Inst.begin(),
+                    MCOperand::createReg(X86::NoRegister));   // IndexReg
+        Inst.insert(Inst.begin(),
+                    MCOperand::createImm(1));   // ScaleAmt
+        Inst.insert(Inst.begin(),
+                    MCOperand::createReg(X86::RIP));   // BaseReg
+
+        return true;
+    }
+
+    void convertIndirectCallToLoad(MCInst &Inst, MCPhysReg Reg) override {
+        bool IsTailCall = isTailCall(Inst);
+        if (IsTailCall)
+            removeAnnotation(Inst, MCPlus::MCAnnotation::kTailCall);
+        if (Inst.getOpcode() == X86::CALL64m ||
+            (Inst.getOpcode() == X86::JMP32m && IsTailCall)) {
+            Inst.setOpcode(X86::MOV64rm);
+            Inst.insert(Inst.begin(), MCOperand::createReg(Reg));
+            return;
+        }
+        if (Inst.getOpcode() == X86::CALL64r ||
+            (Inst.getOpcode() == X86::JMP32r && IsTailCall)) {
+            Inst.setOpcode(X86::MOV64rr);
+            Inst.insert(Inst.begin(), MCOperand::createReg(Reg));
+            return;
+        }
+        LLVM_DEBUG(Inst.dump());
+        llvm_unreachable("not implemented");
+    }
+
+    bool shortenInstruction(MCInst                &Inst,
+                            const MCSubtargetInfo &STI) const override {
+        unsigned OldOpcode = Inst.getOpcode();
+        unsigned NewOpcode = OldOpcode;
+
+        int MemOpNo = getMemoryOperandNo(Inst);
+
+        // Check and remove redundant Address-Size override prefix.
+        if (opts::X86StripRedundantAddressSize) {
+            uint64_t TSFlags = Info->get(OldOpcode).TSFlags;
+            unsigned Flags   = Inst.getFlags();
+
+            if (!X86_MC::needsAddressSizeOverride(Inst, STI, MemOpNo,
+                                                  TSFlags) &&
+                Flags & X86::IP_HAS_AD_SIZE)
+                Inst.setFlags(Flags ^ X86::IP_HAS_AD_SIZE);
+        }
+
+        // Check and remove EIZ/RIZ. These cases represent ambiguous cases where
+        // SIB byte is present, but no index is used and modrm alone should have
+        // been enough. Converting to NoRegister effectively removes the SIB
+        // byte.
+        if (MemOpNo >= 0) {
+            MCOperand &IndexOp = Inst.getOperand(
+                static_cast<unsigned>(MemOpNo) + X86::AddrIndexReg);
+            if (IndexOp.getReg() == X86::EIZ || IndexOp.getReg() == X86::RIZ)
+                IndexOp = MCOperand::createReg(X86::NoRegister);
+        }
+
+        if (isBranch(Inst)) {
+            NewOpcode = getShortBranchOpcode(OldOpcode);
+        } else if (OldOpcode == X86::MOV64ri) {
+            if (Inst.getOperand(MCPlus::getNumPrimeOperands(Inst) - 1)
+                    .isImm()) {
+                const int64_t Imm =
+                    Inst.getOperand(MCPlus::getNumPrimeOperands(Inst) - 1)
+                        .getImm();
+                if (int64_t(Imm) == int64_t(int32_t(Imm)))
+                    NewOpcode = X86::MOV64ri32;
+            }
+        } else {
+            // If it's arithmetic instruction check if signed operand fits in 1
+            // byte.
+            const unsigned ShortOpcode = getShortArithOpcode(OldOpcode);
+            if (ShortOpcode != OldOpcode &&
+                Inst.getOperand(MCPlus::getNumPrimeOperands(Inst) - 1)
+                    .isImm()) {
+                int64_t Imm =
+                    Inst.getOperand(MCPlus::getNumPrimeOperands(Inst) - 1)
+                        .getImm();
+                if (int64_t(Imm) == int64_t(int8_t(Imm)))
+                    NewOpcode = ShortOpcode;
+            }
+        }
+
+        if (NewOpcode == OldOpcode)
+            return false;
+
+        Inst.setOpcode(NewOpcode);
+        return true;
+    }
+
+    bool
+    convertMoveToConditionalMove(MCInst &Inst, unsigned CC,
+                                 bool AllowStackMemOp,
+                                 bool AllowBasePtrStackMemOp) const override {
+        // - Register-register moves are OK
+        // - Stores are filtered out by opcode (no store CMOV)
+        // - Non-stack loads are prohibited (generally unsafe)
+        // - Stack loads are OK if AllowStackMemOp is true
+        // - Stack loads with RBP are OK if AllowBasePtrStackMemOp is true
+        if (isLoad(Inst)) {
+            // If stack memory operands are not allowed, no loads are allowed
+            if (!AllowStackMemOp)
+                return false;
+
+            // If stack memory operands are allowed, check if it's a load from
+            // stack
+            bool      IsLoad, IsStore, IsStoreFromReg, IsSimple, IsIndexed;
+            MCPhysReg Reg;
+            int32_t   SrcImm;
+            uint16_t  StackPtrReg;
+            int64_t   StackOffset;
+            uint8_t   Size;
+            bool      IsStackAccess = isStackAccess(
+                Inst, IsLoad, IsStore, IsStoreFromReg, Reg, SrcImm, StackPtrReg,
+                StackOffset, Size, IsSimple, IsIndexed);
+            // Prohibit non-stack-based loads
+            if (!IsStackAccess)
+                return false;
+            // If stack memory operands are allowed, check if it's RBP-based
+            if (!AllowBasePtrStackMemOp &&
+                RegInfo->isSubRegisterEq(X86::RBP, StackPtrReg))
+                return false;
+        }
+
+        unsigned NewOpcode = 0;
+        switch (Inst.getOpcode()) {
+        case X86::MOV16rr:
+            NewOpcode = X86::CMOV16rr;
+            break;
+        case X86::MOV16rm:
+            NewOpcode = X86::CMOV16rm;
+            break;
+        case X86::MOV32rr:
+            NewOpcode = X86::CMOV32rr;
+            break;
+        case X86::MOV32rm:
+            NewOpcode = X86::CMOV32rm;
+            break;
+        case X86::MOV64rr:
+            NewOpcode = X86::CMOV64rr;
+            break;
+        case X86::MOV64rm:
+            NewOpcode = X86::CMOV64rm;
+            break;
+        default:
+            return false;
+        }
+        Inst.setOpcode(NewOpcode);
+        // Insert CC at the end of prime operands, before annotations
+        Inst.insert(Inst.begin() + MCPlus::getNumPrimeOperands(Inst),
+                    MCOperand::createImm(CC));
+        // CMOV is a 3-operand MCInst, so duplicate the destination as src1
+        Inst.insert(Inst.begin(), Inst.getOperand(0));
+        return true;
+    }
+
+    bool lowerTailCall(MCInst &Inst) override {
+        if (Inst.getOpcode() == X86::JMP_4 && isTailCall(Inst)) {
+            Inst.setOpcode(X86::JMP_1);
+            removeAnnotation(Inst, MCPlus::MCAnnotation::kTailCall);
+            return true;
+        }
+        return false;
+    }
+
+    const MCSymbol *getTargetSymbol(const MCInst &Inst,
+                                    unsigned      OpNum = 0) const override {
+        if (OpNum >= MCPlus::getNumPrimeOperands(Inst))
+            return nullptr;
+
+        const MCOperand &Op = Inst.getOperand(OpNum);
+        if (!Op.isExpr())
+            return nullptr;
+
+        auto *SymExpr = dyn_cast<MCSymbolRefExpr>(Op.getExpr());
+        if (!SymExpr || SymExpr->getKind() != MCSymbolRefExpr::VK_None)
+            return nullptr;
+
+        return &SymExpr->getSymbol();
+    }
+
+    // This is the same as the base class, but since we are overriding one of
+    // getTargetSymbol's signatures above, we need to override all of them.
+    const MCSymbol *getTargetSymbol(const MCExpr *Expr) const override {
+        return &cast<const MCSymbolRefExpr>(Expr)->getSymbol();
+    }
+
+    bool analyzeBranch(InstructionIterator Begin, InstructionIterator End,
+                       const MCSymbol *&TBB, const MCSymbol *&FBB,
+                       MCInst *&CondBranch,
+                       MCInst *&UncondBranch) const override {
+        auto I = End;
+
+        // Bottom-up analysis
+        while (I != Begin) {
+            --I;
+
+            // Ignore nops and CFIs
+            if (isPseudo(*I))
+                continue;
+
+            // Stop when we find the first non-terminator
+            if (!isTerminator(*I))
+                break;
+
+            if (!isBranch(*I))
+                break;
+
+            // Handle unconditional branches.
+            if ((I->getOpcode() == X86::JMP_1 || I->getOpcode() == X86::JMP_2 ||
+                 I->getOpcode() == X86::JMP_4) &&
+                !isTailCall(*I)) {
+                // If any code was seen after this unconditional branch, we've
+                // seen unreachable code. Ignore them.
+                CondBranch          = nullptr;
+                UncondBranch        = &*I;
+                const MCSymbol *Sym = getTargetSymbol(*I);
+                assert(Sym != nullptr &&
+                       "Couldn't extract BB symbol from jump operand");
+                TBB = Sym;
+                continue;
+            }
+
+            // Handle conditional branches and ignore indirect branches
+            if (!isUnsupportedBranch(I->getOpcode()) &&
+                getCondCode(*I) == X86::COND_INVALID) {
+                // Indirect branch
+                return false;
+            }
+
+            if (CondBranch == nullptr) {
+                const MCSymbol *TargetBB = getTargetSymbol(*I);
+                if (TargetBB == nullptr) {
+                    // Unrecognized branch target
+                    return false;
+                }
+                FBB        = TBB;
+                TBB        = TargetBB;
+                CondBranch = &*I;
+                continue;
+            }
+
+            llvm_unreachable("multiple conditional branches in one BB");
+        }
+        return true;
+    }
+
+    template <typename Itr>
+    std::pair<IndirectBranchType, MCInst *>
+    analyzePICJumpTable(Itr II, Itr IE, MCPhysReg R1, MCPhysReg R2) const {
+        // Analyze PIC-style jump table code template:
+        //
+        //    lea PIC_JUMP_TABLE(%rip), {%r1|%r2}     <- MemLocInstr
+        //    mov ({%r1|%r2}, %index, 4), {%r2|%r1}
+        //    add %r2, %r1
+        //    jmp *%r1
+        //
+        // (with any irrelevant instructions in-between)
+        //
+        // When we call this helper we've already determined %r1 and %r2, and
+        // reverse instruction iterator \p II is pointing to the ADD
+        // instruction.
+        //
+        // PIC jump table looks like following:
+        //
+        //   JT:  ----------
+        //    E1:| L1 - JT  |
+        //       |----------|
+        //    E2:| L2 - JT  |
+        //       |----------|
+        //       |          |
+        //          ......
+        //    En:| Ln - JT  |
+        //        ----------
+        //
+        // Where L1, L2, ..., Ln represent labels in the function.
+        //
+        // The actual relocations in the table will be of the form:
+        //
+        //   Ln - JT
+        //    = (Ln - En) + (En - JT)
+        //    = R_X86_64_PC32(Ln) + En - JT
+        //    = R_X86_64_PC32(Ln + offsetof(En))
+        //
+        LLVM_DEBUG(dbgs() << "Checking for PIC jump table\n");
+        MCInst       *MemLocInstr = nullptr;
+        const MCInst *MovInstr    = nullptr;
+        while (++II != IE) {
+            MCInst            &Instr     = *II;
+            const MCInstrDesc &InstrDesc = Info->get(Instr.getOpcode());
+            if (!InstrDesc.hasDefOfPhysReg(Instr, R1, *RegInfo) &&
+                !InstrDesc.hasDefOfPhysReg(Instr, R2, *RegInfo)) {
+                // Ignore instructions that don't affect R1, R2 registers.
+                continue;
+            }
+            if (!MovInstr) {
+                // Expect to see MOV instruction.
+                if (!isMOVSX64rm32(Instr)) {
+                    LLVM_DEBUG(dbgs() << "MOV instruction expected.\n");
+                    break;
+                }
+
+                // Check if it's setting %r1 or %r2. In canonical form it sets
+                // %r2. If it sets %r1 - rename the registers so we have to only
+                // check a single form.
+                unsigned MovDestReg = Instr.getOperand(0).getReg();
+                if (MovDestReg != R2)
+                    std::swap(R1, R2);
+                if (MovDestReg != R2) {
+                    LLVM_DEBUG(dbgs()
+                               << "MOV instruction expected to set %r2\n");
+                    break;
+                }
+
+                // Verify operands for MOV.
+                std::optional<X86MemOperand> MO =
+                    evaluateX86MemoryOperand(Instr);
+                if (!MO)
+                    break;
+                if (MO->BaseRegNum != R1 || MO->ScaleImm != 4 ||
+                    MO->IndexRegNum == X86::NoRegister || MO->DispImm != 0 ||
+                    MO->SegRegNum != X86::NoRegister)
+                    break;
+                MovInstr = &Instr;
+            } else {
+                if (!InstrDesc.hasDefOfPhysReg(Instr, R1, *RegInfo))
+                    continue;
+                if (!isLEA64r(Instr)) {
+                    LLVM_DEBUG(dbgs() << "LEA instruction expected\n");
+                    break;
+                }
+                if (Instr.getOperand(0).getReg() != R1) {
+                    LLVM_DEBUG(dbgs()
+                               << "LEA instruction expected to set %r1\n");
+                    break;
+                }
+
+                // Verify operands for LEA.
+                std::optional<X86MemOperand> MO =
+                    evaluateX86MemoryOperand(Instr);
+                if (!MO)
+                    break;
+                if (MO->BaseRegNum != RegInfo->getProgramCounter() ||
+                    MO->IndexRegNum != X86::NoRegister ||
+                    MO->SegRegNum != X86::NoRegister || MO->DispExpr == nullptr)
+                    break;
+                MemLocInstr = &Instr;
+                break;
+            }
+        }
+
+        if (!MemLocInstr)
+            return std::make_pair(IndirectBranchType::UNKNOWN, nullptr);
+
+        LLVM_DEBUG(dbgs() << "checking potential PIC jump table\n");
+        return std::make_pair(IndirectBranchType::POSSIBLE_PIC_JUMP_TABLE,
+                              MemLocInstr);
+    }
+
+    IndirectBranchType
+    analyzeIndirectBranch(MCInst &Instruction, InstructionIterator Begin,
+                          InstructionIterator End, const unsigned PtrSize,
+                          MCInst *&MemLocInstrOut, unsigned &BaseRegNumOut,
+                          unsigned &IndexRegNumOut, int64_t &DispValueOut,
+                          const MCExpr *&DispExprOut,
+                          MCInst       *&PCRelBaseOut) const override {
+        // Try to find a (base) memory location from where the address for
+        // the indirect branch is loaded. For X86-64 the memory will be
+        // specified in the following format:
+        //
+        //   {%rip}/{%basereg} + Imm + IndexReg * Scale
+        //
+        // We are interested in the cases where Scale == sizeof(uintptr_t) and
+        // the contents of the memory are presumably an array of pointers to
+        // code.
+        //
+        // Normal jump table:
+        //
+        //    jmp *(JUMP_TABLE, %index, Scale)        <- MemLocInstr
+        //
+        //    or
+        //
+        //    mov (JUMP_TABLE, %index, Scale), %r1    <- MemLocInstr
+        //    ...
+        //    jmp %r1
+        //
+        // We handle PIC-style jump tables separately.
+        //
+        MemLocInstrOut = nullptr;
+        BaseRegNumOut  = X86::NoRegister;
+        IndexRegNumOut = X86::NoRegister;
+        DispValueOut   = 0;
+        DispExprOut    = nullptr;
+
+        std::reverse_iterator<InstructionIterator> II(End);
+        std::reverse_iterator<InstructionIterator> IE(Begin);
+
+        IndirectBranchType Type = IndirectBranchType::UNKNOWN;
+
+        // An instruction referencing memory used by jump instruction (directly
+        // or via register). This location could be an array of function
+        // pointers in case of indirect tail call, or a jump table.
+        MCInst *MemLocInstr = nullptr;
+
+        if (MCPlus::getNumPrimeOperands(Instruction) == 1) {
+            // If the indirect jump is on register - try to detect if the
+            // register value is loaded from a memory location.
+            assert(Instruction.getOperand(0).isReg() &&
+                   "register operand expected");
+            const unsigned R1 = Instruction.getOperand(0).getReg();
+            // Check if one of the previous instructions defines the jump-on
+            // register.
+            for (auto PrevII = II; PrevII != IE; ++PrevII) {
+                MCInst            &PrevInstr = *PrevII;
+                const MCInstrDesc &PrevInstrDesc =
+                    Info->get(PrevInstr.getOpcode());
+
+                if (!PrevInstrDesc.hasDefOfPhysReg(PrevInstr, R1, *RegInfo))
+                    continue;
+
+                if (isMoveMem2Reg(PrevInstr)) {
+                    MemLocInstr = &PrevInstr;
+                    break;
+                }
+                if (isADD64rr(PrevInstr)) {
+                    unsigned R2 = PrevInstr.getOperand(2).getReg();
+                    if (R1 == R2)
+                        return IndirectBranchType::UNKNOWN;
+                    std::tie(Type, MemLocInstr) =
+                        analyzePICJumpTable(PrevII, IE, R1, R2);
+                    break;
+                }
+                return IndirectBranchType::UNKNOWN;
+            }
+            if (!MemLocInstr) {
+                // No definition seen for the register in this function so far.
+                // Could be an input parameter - which means it is an external
+                // code reference. It also could be that the definition happens
+                // to be in the code that we haven't processed yet. Since we
+                // have to be conservative, return as UNKNOWN case.
+                return IndirectBranchType::UNKNOWN;
+            }
+        } else {
+            MemLocInstr = &Instruction;
+        }
+
+        const MCRegister RIPRegister = RegInfo->getProgramCounter();
+
+        // Analyze the memory location.
+        std::optional<X86MemOperand> MO =
+            evaluateX86MemoryOperand(*MemLocInstr);
+        if (!MO)
+            return IndirectBranchType::UNKNOWN;
+
+        BaseRegNumOut  = MO->BaseRegNum;
+        IndexRegNumOut = MO->IndexRegNum;
+        DispValueOut   = MO->DispImm;
+        DispExprOut    = MO->DispExpr;
+
+        if ((MO->BaseRegNum != X86::NoRegister &&
+             MO->BaseRegNum != RIPRegister) ||
+            MO->SegRegNum != X86::NoRegister)
+            return IndirectBranchType::UNKNOWN;
+
+        if (MemLocInstr == &Instruction &&
+            (!MO->ScaleImm || MO->IndexRegNum == X86::NoRegister)) {
+            MemLocInstrOut = MemLocInstr;
+            return IndirectBranchType::POSSIBLE_FIXED_BRANCH;
+        }
+
+        if (Type == IndirectBranchType::POSSIBLE_PIC_JUMP_TABLE &&
+            (MO->ScaleImm != 1 || MO->BaseRegNum != RIPRegister))
+            return IndirectBranchType::UNKNOWN;
+
+        if (Type != IndirectBranchType::POSSIBLE_PIC_JUMP_TABLE &&
+            MO->ScaleImm != PtrSize)
+            return IndirectBranchType::UNKNOWN;
+
+        MemLocInstrOut = MemLocInstr;
+
+        return Type;
+    }
+
+    /// Analyze a callsite to see if it could be a virtual method call.  This
+    /// only checks to see if the overall pattern is satisfied, it does not
+    /// guarantee that the callsite is a true virtual method call. The format of
+    /// virtual method calls that are recognized is one of the following:
+    ///
+    ///  Form 1: (found in debug code)
+    ///    add METHOD_OFFSET, %VtableReg
+    ///    mov (%VtableReg), %MethodReg
+    ///    ...
+    ///    call or jmp *%MethodReg
+    ///
+    ///  Form 2:
+    ///    mov METHOD_OFFSET(%VtableReg), %MethodReg
+    ///    ...
+    ///    call or jmp *%MethodReg
+    ///
+    ///  Form 3:
+    ///    ...
+    ///    call or jmp *METHOD_OFFSET(%VtableReg)
+    ///
+    bool analyzeVirtualMethodCall(InstructionIterator    ForwardBegin,
+                                  InstructionIterator    ForwardEnd,
+                                  std::vector<MCInst *> &MethodFetchInsns,
+                                  unsigned              &VtableRegNum,
+                                  unsigned              &MethodRegNum,
+                                  uint64_t &MethodOffset) const override {
+        VtableRegNum = X86::NoRegister;
+        MethodRegNum = X86::NoRegister;
+        MethodOffset = 0;
+
+        std::reverse_iterator<InstructionIterator> Itr(ForwardEnd);
+        std::reverse_iterator<InstructionIterator> End(ForwardBegin);
+
+        MCInst &CallInst = *Itr++;
+        assert(isIndirectBranch(CallInst) || isCall(CallInst));
+
+        // The call can just be jmp offset(reg)
+        if (std::optional<X86MemOperand> MO =
+                evaluateX86MemoryOperand(CallInst)) {
+            if (!MO->DispExpr && MO->BaseRegNum != X86::RIP &&
+                MO->BaseRegNum != X86::RBP &&
+                MO->BaseRegNum != X86::NoRegister) {
+                MethodRegNum = MO->BaseRegNum;
+                if (MO->ScaleImm == 1 && MO->IndexRegNum == X86::NoRegister &&
+                    MO->SegRegNum == X86::NoRegister) {
+                    VtableRegNum = MethodRegNum;
+                    MethodOffset = MO->DispImm;
+                    MethodFetchInsns.push_back(&CallInst);
+                    return true;
+                }
+            }
+            return false;
+        }
+        if (CallInst.getOperand(0).isReg())
+            MethodRegNum = CallInst.getOperand(0).getReg();
+        else
+            return false;
+
+        if (MethodRegNum == X86::RIP || MethodRegNum == X86::RBP) {
+            VtableRegNum = X86::NoRegister;
+            MethodRegNum = X86::NoRegister;
+            return false;
+        }
+
+        // find load from vtable, this may or may not include the method offset
+        while (Itr != End) {
+            MCInst            &CurInst = *Itr++;
+            const MCInstrDesc &Desc    = Info->get(CurInst.getOpcode());
+            if (Desc.hasDefOfPhysReg(CurInst, MethodRegNum, *RegInfo)) {
+                if (!isLoad(CurInst))
+                    return false;
+                if (std::optional<X86MemOperand> MO =
+                        evaluateX86MemoryOperand(CurInst)) {
+                    if (!MO->DispExpr && MO->ScaleImm == 1 &&
+                        MO->BaseRegNum != X86::RIP &&
+                        MO->BaseRegNum != X86::RBP &&
+                        MO->BaseRegNum != X86::NoRegister &&
+                        MO->IndexRegNum == X86::NoRegister &&
+                        MO->SegRegNum == X86::NoRegister &&
+                        MO->BaseRegNum != X86::RIP) {
+                        VtableRegNum = MO->BaseRegNum;
+                        MethodOffset = MO->DispImm;
+                        MethodFetchInsns.push_back(&CurInst);
+                        if (MethodOffset != 0)
+                            return true;
+                        break;
+                    }
+                }
+                return false;
+            }
+        }
+
+        if (!VtableRegNum)
+            return false;
+
+        // look for any adds affecting the method register.
+        while (Itr != End) {
+            MCInst            &CurInst = *Itr++;
+            const MCInstrDesc &Desc    = Info->get(CurInst.getOpcode());
+            if (Desc.hasDefOfPhysReg(CurInst, VtableRegNum, *RegInfo)) {
+                if (isADDri(CurInst)) {
+                    assert(!MethodOffset);
+                    MethodOffset = CurInst.getOperand(2).getImm();
+                    MethodFetchInsns.insert(MethodFetchInsns.begin(), &CurInst);
+                    break;
+                }
+            }
+        }
+
+        return true;
+    }
+
+    bool createStackPointerIncrement(MCInst &Inst, int Size,
+                                     bool NoFlagsClobber) const override {
+        if (NoFlagsClobber) {
+            Inst.setOpcode(X86::LEA64r);
+            Inst.clear();
+            Inst.addOperand(MCOperand::createReg(X86::RSP));
+            Inst.addOperand(MCOperand::createReg(X86::RSP));   // BaseReg
+            Inst.addOperand(MCOperand::createImm(1));          // ScaleAmt
+            Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));     // IndexReg
+            Inst.addOperand(MCOperand::createImm(-Size));   // Displacement
+            Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+            return true;
+        }
+        Inst.setOpcode(X86::SUB64ri8);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createReg(X86::RSP));
+        Inst.addOperand(MCOperand::createReg(X86::RSP));
+        Inst.addOperand(MCOperand::createImm(Size));
+        return true;
+    }
+
+    bool createStackPointerDecrement(MCInst &Inst, int Size,
+                                     bool NoFlagsClobber) const override {
+        if (NoFlagsClobber) {
+            Inst.setOpcode(X86::LEA64r);
+            Inst.clear();
+            Inst.addOperand(MCOperand::createReg(X86::RSP));
+            Inst.addOperand(MCOperand::createReg(X86::RSP));   // BaseReg
+            Inst.addOperand(MCOperand::createImm(1));          // ScaleAmt
+            Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));    // IndexReg
+            Inst.addOperand(MCOperand::createImm(Size));   // Displacement
+            Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+            return true;
+        }
+        Inst.setOpcode(X86::ADD64ri8);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createReg(X86::RSP));
+        Inst.addOperand(MCOperand::createReg(X86::RSP));
+        Inst.addOperand(MCOperand::createImm(Size));
+        return true;
+    }
+
+    bool createSaveToStack(MCInst &Inst, const MCPhysReg &StackReg, int Offset,
+                           const MCPhysReg &SrcReg, int Size) const override {
+        unsigned NewOpcode;
+        switch (Size) {
+        default:
+            return false;
+        case 2:
+            NewOpcode = X86::MOV16mr;
+            break;
+        case 4:
+            NewOpcode = X86::MOV32mr;
+            break;
+        case 8:
+            NewOpcode = X86::MOV64mr;
+            break;
+        }
+        Inst.setOpcode(NewOpcode);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createReg(StackReg));          // BaseReg
+        Inst.addOperand(MCOperand::createImm(1));                 // ScaleAmt
+        Inst.addOperand(MCOperand::createReg(X86::NoRegister));   // IndexReg
+        Inst.addOperand(MCOperand::createImm(Offset));   // Displacement
+        Inst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+        Inst.addOperand(MCOperand::createReg(SrcReg));
+        return true;
+    }
+
+    bool createRestoreFromStack(MCInst &Inst, const MCPhysReg &StackReg,
+                                int Offset, const MCPhysReg &DstReg,
+                                int Size) const override {
+        return createLoad(Inst, StackReg, /*Scale=*/1,
+                          /*IndexReg=*/X86::NoRegister, Offset, nullptr,
+                          /*AddrSegmentReg=*/X86::NoRegister, DstReg, Size);
+    }
+
+    bool createLoad(MCInst &Inst, const MCPhysReg &BaseReg, int64_t Scale,
+                    const MCPhysReg &IndexReg, int64_t Offset,
+                    const MCExpr *OffsetExpr, const MCPhysReg &AddrSegmentReg,
+                    const MCPhysReg &DstReg, int Size) const override {
+        unsigned NewOpcode;
+        switch (Size) {
+        default:
+            return false;
+        case 2:
+            NewOpcode = X86::MOV16rm;
+            break;
+        case 4:
+            NewOpcode = X86::MOV32rm;
+            break;
+        case 8:
+            NewOpcode = X86::MOV64rm;
+            break;
+        }
+        Inst.setOpcode(NewOpcode);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createReg(DstReg));
+        Inst.addOperand(MCOperand::createReg(BaseReg));
+        Inst.addOperand(MCOperand::createImm(Scale));
+        Inst.addOperand(MCOperand::createReg(IndexReg));
+        if (OffsetExpr)
+            Inst.addOperand(
+                MCOperand::createExpr(OffsetExpr));   // Displacement
+        else
+            Inst.addOperand(MCOperand::createImm(Offset));   // Displacement
+        Inst.addOperand(
+            MCOperand::createReg(AddrSegmentReg));   // AddrSegmentReg
+        return true;
+    }
+
+    void createLoadImmediate(MCInst &Inst, const MCPhysReg Dest,
+                             uint32_t Imm) const override {
+        Inst.setOpcode(X86::MOV64ri32);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createReg(Dest));
+        Inst.addOperand(MCOperand::createImm(Imm));
+    }
+
+    bool createIncMemory(MCInst &Inst, const MCSymbol *Target,
+                         MCContext *Ctx) const override {
+
+        Inst.setOpcode(X86::LOCK_INC64m);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createReg(X86::RIP));          // BaseReg
+        Inst.addOperand(MCOperand::createImm(1));                 // ScaleAmt
+        Inst.addOperand(MCOperand::createReg(X86::NoRegister));   // IndexReg
+
+        Inst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(Target, MCSymbolRefExpr::VK_None,
+                                    *Ctx)));   // Displacement
+        Inst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+        return true;
+    }
+
+    bool createIJmp32Frag(SmallVectorImpl<MCInst> &Insts,
+                          const MCOperand &BaseReg, const MCOperand &Scale,
+                          const MCOperand &IndexReg, const MCOperand &Offset,
+                          const MCOperand &TmpReg) const override {
+        // The code fragment we emit here is:
+        //
+        //  mov32 (%base, %index, scale), %tmpreg
+        //  ijmp *(%tmpreg)
+        //
+        MCInst IJmp;
+        IJmp.setOpcode(X86::JMP64r);
+        IJmp.addOperand(TmpReg);
+
+        MCInst Load;
+        Load.setOpcode(X86::MOV32rm);
+        Load.addOperand(TmpReg);
+        Load.addOperand(BaseReg);
+        Load.addOperand(Scale);
+        Load.addOperand(IndexReg);
+        Load.addOperand(Offset);
+        Load.addOperand(MCOperand::createReg(X86::NoRegister));
+
+        Insts.push_back(Load);
+        Insts.push_back(IJmp);
+        return true;
+    }
+
+    bool createNoop(MCInst &Inst) const override {
+        Inst.setOpcode(X86::NOOP);
+        return true;
+    }
+
+    bool getCrtCtxReg(unsigned &Reg, int size) const override {
+        switch (size) {
+        case 2:
+            Reg = X86::DI;
+            break;
+        case 4:
+            Reg = X86::EDI;
+            break;
+        case 8:
+            Reg = X86::RDI;
+            break;
+        default:
+            return false;
+        }
+
+        return true;
+    }
+
+    bool getTmpReg1(unsigned &Reg) const {
+        Reg = X86::R12;
+        return true;
+    }
+    int offsetInRegSet(unsigned Reg, unsigned RegSetIdx) const {
+        // int RegsBegin = 16;
+        // int Begin     = RegsBegin + RegSetIdx * 160;
+        int Begin = RegSetIdx * SizeOfX86Regs;
+
+        switch (Reg) {
+        case X86::RAX:
+            return Begin + 0;
+        case X86::RBX:
+            return Begin + 8;
+        case X86::RCX:
+            return Begin + 16;
+        case X86::RDX:
+            return Begin + 24;
+        case X86::RSI:
+            return Begin + 32;
+        case X86::RDI:
+            return Begin + 40;
+        case X86::RBP:
+            return Begin + 48;
+        case X86::RSP:
+            return Begin + 56;
+        case X86::R8:
+            return Begin + 64;
+        case X86::R9:
+            return Begin + 72;
+        case X86::R10:
+            return Begin + 80;
+        case X86::R11:
+            return Begin + 88;
+        case X86::R12:
+            return Begin + 96;
+        case X86::R13:
+            return Begin + 104;
+        case X86::R14:
+            return Begin + 112;
+        case X86::R15:
+            return Begin + 120;
+        case X86::XMM0:
+            return Begin + 128;
+        case X86::XMM1:
+            return Begin + 144;
+        case X86::XMM2:
+            return Begin + 160;
+        case X86::XMM3:
+            return Begin + 176;
+        default:
+            return -1;
+        }
+    }
+
+    bool createPushToRegSet(InstructionListType &Instrs, const BitVector &Regs,
+                            const MCSymbol *CrtPos, MCContext *Ctx,
+                            unsigned RegSetIdx, bool UseStack,
+                            bool SkipCtxManage) const override {
+        if (UseStack) {   // shouldn't be used..
+            for (int i = 0; i < Regs.size(); i++) {
+                if (Regs.test(i)) {
+                    // push
+                    MCInst PushInst;
+                    PushInst.setOpcode(X86::PUSH64r);
+                    PushInst.clear();
+                    PushInst.addOperand(MCOperand::createReg(i));
+                    Instrs.push_back(PushInst);
+                }
+            }
+        } else {
+            // Look at insertResiduePushLEA64rPop
+            // for which registers are safe to use in this code
+            // I chose R11 to save the current context
+            // Note that we don't need to unset R11 in Regs
+            // because R11 can never be in Alive\Used set
+            unsigned CCReg;
+            getCrtCtxReg(CCReg, 8);
+            unsigned CCRegdw;
+            getCrtCtxReg(CCRegdw, 4);
+
+            if (!SkipCtxManage) {
+                MCInst PushCrtCtxRegInst;
+                PushCrtCtxRegInst.setOpcode(X86::PUSH64r);
+                PushCrtCtxRegInst.addOperand(MCOperand::createReg(CCReg));
+                Instrs.push_back(PushCrtCtxRegInst);
+            }
+            // gs:yield_ctx_size + crt_ctx * idx + regsetoffset
+            //  constant = yield_ctx_size + regsetoffset
+            //  runtime computation = crt_ctx * idx
+
+            if (CrtPos) {
+                MCInst MovIdxInst;
+                MovIdxInst.setOpcode(X86::MOV32rm);
+                MovIdxInst.addOperand(MCOperand::createReg(CCRegdw));
+                MovIdxInst.addOperand(
+                    MCOperand::createReg(X86::RIP));              // BaseReg
+                MovIdxInst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+                MovIdxInst.addOperand(
+                    MCOperand::createReg(X86::NoRegister));   // IndexReg
+                MovIdxInst.addOperand(MCOperand::createExpr(
+                    MCSymbolRefExpr::create(CrtPos, MCSymbolRefExpr::VK_None,
+                                            *Ctx)));   // Displacement
+                MovIdxInst.addOperand(
+                    MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+
+                MCInst ImulCtxSzInst;
+                // upper 32-bit of RDI is zero-extended
+                ImulCtxSzInst.setOpcode(X86::IMUL64rri32);
+                ImulCtxSzInst.addOperand(MCOperand::createReg(CCReg));
+                ImulCtxSzInst.addOperand(MCOperand::createReg(CCRegdw));
+                ImulCtxSzInst.addOperand(
+                    MCOperand::createImm(SizeOfRegset / SizeOfYieldCtx));
+
+                // MCInst MovRegSetLoc;
+                // MovRegSetLoc.setOpcode(X86::MOV64rm);
+                // MovRegSetLoc.addOperand(MCOperand::createReg(CCReg));
+                // MovRegSetLoc.addOperand(MCOperand::createReg(CCReg));
+                // MovIdxInst.addOperand(MCOperand::createImm(1)); // ScaleAmt
+                // MovIdxInst.addOperand(MCOperand::createReg(X86::NoRegister));
+                // // IndexReg
+                // MovIdxInst.addOperand(MCOperand::createImm(OffsetToRegSetPointer));
+                // // Displacement MovIdxInst.addOperand(
+                //     MCOperand::createReg(X86::GS));
+
+                Instrs.push_back(MovIdxInst);
+                Instrs.push_back(ImulCtxSzInst);
+                // Instrs.push_back(MovRegSetLoc);
+            } else {
+                MCInst MovIdxInst;
+                MovIdxInst.setOpcode(X86::XOR64rr);
+                MovIdxInst.addOperand(MCOperand::createReg(CCReg));
+                MovIdxInst.addOperand(MCOperand::createReg(CCReg));
+                MovIdxInst.addOperand(MCOperand::createReg(CCReg));
+                Instrs.push_back(MovIdxInst);
+            }
+
+            // CCReg points to the beginning of the first regset
+            for (int i = 0; i < Regs.size(); i++) {
+                if (Regs.test(i)) {
+                    int RegOffset = offsetInRegSet(i, RegSetIdx);
+                    if (RegOffset == -1) {
+                        errs() << "Error: unsupported register in "
+                                  "offsetToRegSet1\n";
+                        exit(1);
+                    }
+                    MCInst PushToRegsetInst;
+                    if (i == X86::XMM0 || i == X86::XMM1 || i == X86::XMM2 || i == X86::XMM3)
+                        PushToRegsetInst.setOpcode(X86::MOVUPSmr);
+                    else
+                        PushToRegsetInst.setOpcode(X86::MOV64mr);
+                    PushToRegsetInst.addOperand(
+                        MCOperand::createReg(CCReg));   // BaseReg
+                    PushToRegsetInst.addOperand(
+                        MCOperand::createImm(1));   // ScaleAmt
+                    PushToRegsetInst.addOperand(
+                        MCOperand::createReg(X86::NoRegister));   // IndexReg
+                    PushToRegsetInst.addOperand(MCOperand::createImm(
+                        RegOffset + OffsetToRegSet));   // Displacement
+                    PushToRegsetInst.addOperand(
+                        MCOperand::createReg(X86::GS));   // AddrSegmentReg
+                    PushToRegsetInst.addOperand(
+                        MCOperand::createReg(i));   // src reg
+
+                    Instrs.push_back(PushToRegsetInst);
+                }
+            }
+            if (!SkipCtxManage) {
+                MCInst PopCrtCtxRegInst;
+                PopCrtCtxRegInst.setOpcode(X86::POP64r);
+                PopCrtCtxRegInst.addOperand(MCOperand::createReg(CCReg));
+                Instrs.push_back(PopCrtCtxRegInst);
+            }
+        }
+        return true;
+    }
+
+    bool createPopFromRegSet(InstructionListType &Instrs, const BitVector &Regs,
+                             const MCSymbol *CrtPos, MCContext *Ctx,
+                             unsigned RegSetIdx, bool UseStack,
+                             bool SkipCtxManage) const override {
+        if (UseStack) {   // shouldn't be used...
+            for (int i = Regs.size() - 1; i >= 0; i--) {
+                if (Regs.test(i)) {
+                    MCInst PopInst;
+                    PopInst.setOpcode(X86::POP64r);
+                    PopInst.clear();
+                    PopInst.addOperand(MCOperand::createReg(i));
+                    Instrs.push_back(PopInst);
+                }
+            }
+        } else {
+            // Look at insertResiduePushPop
+            // for which registers are safe to use in this code
+            unsigned CCReg;
+            getCrtCtxReg(CCReg, 8);
+            unsigned CCRegdw;
+            getCrtCtxReg(CCRegdw, 4);
+
+            if (!SkipCtxManage) {
+                MCInst PushCrtCtxRegInst;
+                PushCrtCtxRegInst.setOpcode(X86::PUSH64r);
+                PushCrtCtxRegInst.addOperand(MCOperand::createReg(CCReg));
+                Instrs.push_back(PushCrtCtxRegInst);
+            }
+            // gs:yield_ctx_size + crt_ctx * idx + regsetoffset
+            //  constant = yield_ctx_size + regsetoffset
+            //  runtime computation = crt_ctx * idx
+
+            if (CrtPos) {
+                MCInst MovIdxInst;
+                MovIdxInst.setOpcode(X86::MOV32rm);
+                MovIdxInst.addOperand(MCOperand::createReg(CCRegdw));
+                MovIdxInst.addOperand(
+                    MCOperand::createReg(X86::RIP));              // BaseReg
+                MovIdxInst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+                MovIdxInst.addOperand(
+                    MCOperand::createReg(X86::NoRegister));   // IndexReg
+                MovIdxInst.addOperand(MCOperand::createExpr(
+                    MCSymbolRefExpr::create(CrtPos, MCSymbolRefExpr::VK_None,
+                                            *Ctx)));   // Displacement
+                MovIdxInst.addOperand(
+                    MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+
+                MCInst ImulCtxSzInst;
+                // upper 32-bit of RDI is zero-extended
+                ImulCtxSzInst.setOpcode(X86::IMUL64rri32);
+                ImulCtxSzInst.addOperand(MCOperand::createReg(CCReg));
+                ImulCtxSzInst.addOperand(MCOperand::createReg(CCRegdw));
+                ImulCtxSzInst.addOperand(
+                    MCOperand::createImm(SizeOfRegset / SizeOfYieldCtx));
+
+                //               MCInst MovRegSetLoc;
+                //               MovRegSetLoc.setOpcode(X86::MOV64rr);
+                //               MovRegSetLoc.addOperand(MCOperand::createReg(CCReg));
+                //               MovRegSetLoc.addOperand(MCOperand::createReg(CCRegdw));
+                //               MovIdxInst.addOperand(MCOperand::createImm(1));
+                //               // ScaleAmt
+                //               MovIdxInst.addOperand(MCOperand::createReg(X86::NoRegister));
+                //               // IndexReg
+                //               MovIdxInst.addOperand(MCOperand::createImm(OffsetToRegSetPointer));
+                //               // Displacement MovIdxInst.addOperand(
+                //                   MCOperand::createReg(X86::GS));
+
+                Instrs.push_back(MovIdxInst);
+                // Instrs.push_back(MovRegSetLoc);
+                Instrs.push_back(ImulCtxSzInst);
+            } else {
+                MCInst MovIdxInst;
+                MovIdxInst.setOpcode(X86::XOR64rr);
+                MovIdxInst.addOperand(MCOperand::createReg(CCReg));
+                MovIdxInst.addOperand(MCOperand::createReg(CCReg));
+                MovIdxInst.addOperand(MCOperand::createReg(CCReg));
+                Instrs.push_back(MovIdxInst);
+            }
+
+            for (int i = 0; i < Regs.size(); i++) {
+                if (Regs.test(i)) {
+                    int RegOffset = offsetInRegSet(i, RegSetIdx);
+                    if (RegOffset == -1) {
+                        errs() << "Error: unsupported register in "
+                                  "offsetToRegSet1\n";
+                        exit(1);
+                    }
+                    MCInst PopFromRegsetInst;
+                    if (i == X86::XMM0 
+                         || i == X86::XMM1
+                         || i == X86::XMM2
+                         || i == X86::XMM3)
+                        PopFromRegsetInst.setOpcode(X86::MOVUPSrm);
+                    else
+                        PopFromRegsetInst.setOpcode(X86::MOV64rm);
+                    PopFromRegsetInst.addOperand(
+                        MCOperand::createReg(i));   // dst reg
+                    PopFromRegsetInst.addOperand(
+                        MCOperand::createReg(CCReg));   // BaseReg
+                    PopFromRegsetInst.addOperand(
+                        MCOperand::createImm(1));   // ScaleAmt
+                    PopFromRegsetInst.addOperand(
+                        MCOperand::createReg(X86::NoRegister));   // IndexReg
+                    PopFromRegsetInst.addOperand(MCOperand::createImm(
+                        RegOffset + OffsetToRegSet));   // Displacement
+                    PopFromRegsetInst.addOperand(
+                        MCOperand::createReg(X86::GS));   // AddrSegmentReg
+
+                    Instrs.push_back(PopFromRegsetInst);
+                }
+            }
+            if (!SkipCtxManage) {
+                MCInst PopCrtCtxRegInst;
+                PopCrtCtxRegInst.setOpcode(X86::POP64r);
+                PopCrtCtxRegInst.addOperand(MCOperand::createReg(CCReg));
+                Instrs.push_back(PopCrtCtxRegInst);
+            }
+        }
+        return true;
+    }
+
+    // prefetch instruction follows the standard register addressing rule:
+    // (BaseReg + SCALE(Imm) * Index(Reg) + Displacement(Imm)) +
+    // AddrSegment(Reg)) MOVXXrm: load, dst = register, source = memory location
+    // pointed by register addresing rule
+    // --> for MOVXXrm, we need to set the memory location pointed by the source
+    // to be the prefetch target.
+    // --> MOVXXrm: [dst reg] [src base reg] [SCALE imm] [Index Reg]
+    // [Displacement Imm] [Addr Segment Reg]
+    // --> prefetcht: [base reg] [SCALE imm] [Index Reg] [Displacement Imm]
+    // [Addr Segment Reg]
+    bool createPrefetchInstrs(InstructionListType &Instrs,
+                              const MCInst        &CMInst) const override {
+        MCInst  PrefetchInst, PrefetchInst2;
+        int64_t disp = 0;
+        PrefetchInst.setOpcode(X86::PREFETCHT0);
+        bool IsWide = false;
+        if (!isPrefetchable(CMInst, IsWide)) {
+            return false;
+        }
+        /*
+        if (IsWide) {                                  // 128-bit
+            PrefetchInst.addOperand(CMInst.getOperand(1));   // src base reg
+            PrefetchInst.addOperand(CMInst.getOperand(2));   // scale imm
+            PrefetchInst.addOperand(CMInst.getOperand(3));   // index reg
+            PrefetchInst.addOperand(CMInst.getOperand(4));   // displacement imm
+            PrefetchInst.addOperand(CMInst.getOperand(5));   // addr segment reg
+            Instrs.push_back(PrefetchInst);
+
+            // prefetch next cache line as well
+            PrefetchInst2.setOpcode(X86::PREFETCHT0);
+            PrefetchInst2.addOperand(CMInst.getOperand(1));   // src base reg
+            PrefetchInst2.addOperand(CMInst.getOperand(2));   // scale imm
+            PrefetchInst2.addOperand(CMInst.getOperand(3));   // index reg
+            disp = CMInst.getOperand(4).getImm();
+            PrefetchInst2.addOperand(
+                MCOperand::createImm(disp + 64));   // displacement imm
+            PrefetchInst2.addOperand(
+                CMInst.getOperand(5));   // addr segment reg
+            Instrs.push_back(PrefetchInst2);
+        } else {*/
+        std::optional<X86MemOperand> MO = evaluateX86MemoryOperand(CMInst);
+        if (!MO)
+            return false;
+        PrefetchInst.addOperand(
+            MCOperand::createReg(MO->BaseRegNum));   // src base reg
+        PrefetchInst.addOperand(
+            MCOperand::createImm(MO->ScaleImm));   // scale imm
+        PrefetchInst.addOperand(
+            MCOperand::createReg(MO->IndexRegNum));   // index reg
+        if (MO->DispExpr) {
+            PrefetchInst.addOperand(
+                MCOperand::createExpr(MO->DispExpr));   // displacement expr
+        } else {
+            PrefetchInst.addOperand(
+                MCOperand::createImm(MO->DispImm));   // displacement imm
+        }
+        PrefetchInst.addOperand(
+            MCOperand::createReg(MO->SegRegNum));   // addr segment reg
+        Instrs.push_back(PrefetchInst);
+        //}
+
+        return true;
+    }
+
+    bool filterValidRegsInPnY(BitVector &Regs) const override {
+        BitVector ValidRegs = BitVector(RegInfo->getNumRegs(), false);
+        getGPRegs(ValidRegs, false);
+        ValidRegs.set(X86::XMM0);
+        ValidRegs.set(X86::XMM1);
+        ValidRegs.set(X86::XMM2);
+        ValidRegs.set(X86::XMM3);
+
+        Regs &= ValidRegs;
+
+        return true;
+    }
+
+    bool createPrefetchContext(InstructionListType &Instrs,
+                               MCContext           *Ctx) const override {
+        MCInst PrefetchCurCtxInst, PrefetchNextCtxInst;
+        PrefetchCurCtxInst.setOpcode(X86::PREFETCHT0);
+        PrefetchCurCtxInst.addOperand(
+            MCOperand::createReg(X86::NoRegister));               // BaseReg
+        PrefetchCurCtxInst.addOperand(MCOperand::createImm(0));   // ScaleAmt
+        PrefetchCurCtxInst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // IndexReg
+        PrefetchCurCtxInst.addOperand(
+            MCOperand::createImm(0));   // Displacement
+        PrefetchCurCtxInst.addOperand(
+            MCOperand::createReg(X86::GS));   // AddrSegmentReg
+
+        Instrs.push_back(PrefetchCurCtxInst);
+        /*
+        PrefetchCurCtxInst.setOpcode(X86::PREFETCHT0);
+        PrefetchCurCtxInst.addOperand(
+            MCOperand::createReg(X86::RIP));                      // BaseReg
+        PrefetchCurCtxInst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+        PrefetchCurCtxInst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // IndexReg
+        PrefetchCurCtxInst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(CurCrtCtx, MCSymbolRefExpr::VK_None,
+                                    *Ctx)));   // Displacement
+        PrefetchCurCtxInst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+
+        PrefetchNextCtxInst.setOpcode(X86::PREFETCHT0);
+        PrefetchNextCtxInst.addOperand(
+            MCOperand::createReg(X86::RIP));                       // BaseReg
+        PrefetchNextCtxInst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+        PrefetchNextCtxInst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // IndexReg
+        PrefetchNextCtxInst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(NextCrtCtx, MCSymbolRefExpr::VK_None,
+                                    *Ctx)));   // Displacement
+        PrefetchNextCtxInst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+
+        Instrs.push_back(PrefetchCurCtxInst);
+        Instrs.push_back(PrefetchNextCtxInst);
+        */
+        return true;
+    }
+
+    unsigned getUnusedGPReg(const BitVector &AliveRegs) {
+        BitVector ValidRegs = BitVector(RegInfo->getNumRegs(), false);
+        getGPRegs(ValidRegs, false);
+        
+        for (int i = 0; i < ValidRegs.size(); i++) {
+            if (ValidRegs.test(i) && AliveRegs.test(i)) {
+                ValidRegs.reset(i);
+            }
+        } 
+        
+        for (int i = 1; i < ValidRegs.size(); i++) {
+            if (ValidRegs.test(i)) {
+                return i;
+            }
+        }
+        return 0;
+    }
+
+    bool setRegToZero(MCInst &Inst, unsigned Reg) override {
+        Inst.setOpcode(X86::XOR64rr);
+        Inst.addOperand(MCOperand::createReg(Reg));
+        Inst.addOperand(MCOperand::createReg(Reg));
+        Inst.addOperand(MCOperand::createReg(Reg));
+
+        return true;
+    }
+
+    bool isCmpXchg(const MCInst &Inst) override {
+        return Inst.getOpcode() == X86::CMPXCHG16rm ||
+               Inst.getOpcode() == X86::CMPXCHG32rm ||
+               Inst.getOpcode() == X86::CMPXCHG64rm ||
+                Inst.getOpcode() == X86::CMPXCHG8rm ||
+                Inst.getOpcode() == X86::CMPXCHG8rr ||
+                Inst.getOpcode() == X86::CMPXCHG16rr ||
+                Inst.getOpcode() == X86::CMPXCHG32rr ||
+                Inst.getOpcode() == X86::CMPXCHG64rr;
+    }
+
+
+    bool isAddByConstant(const MCInst &Inst, unsigned &Reg, int &Const) override {
+        if (Inst.getOpcode() == X86::ADD64ri32 ||
+            Inst.getOpcode() == X86::ADD64ri8 ||
+            Inst.getOpcode() == X86::ADD32ri ||
+            Inst.getOpcode() == X86::ADD32ri8 ||
+            Inst.getOpcode() == X86::ADD16ri ||
+            Inst.getOpcode() == X86::ADD16ri8 ||
+            Inst.getOpcode() == X86::ADD8ri ||
+            Inst.getOpcode() == X86::ADD8ri8) {
+            Reg = 0;
+            Const = 0;
+            if (Inst.getOperand(0).isReg()) {
+                Reg = Inst.getOperand(0).getReg();
+                Const = Inst.getOperand(2).getImm();
+                return true;
+            }
+        }
+
+        return false;
+    }
+
+    bool copyRegToReg(MCInst &Inst, unsigned From, unsigned To) override {
+        Inst.setOpcode(X86::MOV64rr);
+        Inst.addOperand(MCOperand::createReg(To));
+        Inst.addOperand(MCOperand::createReg(From));
+        return true;
+    } 
+
+    bool isCmp(const MCInst &Inst) override {
+        return Inst.getOpcode() == X86::CMP64rr ||
+               Inst.getOpcode() == X86::CMP64rm ||
+               Inst.getOpcode() == X86::CMP32rr ||
+               Inst.getOpcode() == X86::CMP32rm ||
+               Inst.getOpcode() == X86::CMP16rr ||
+               Inst.getOpcode() == X86::CMP16rm ||
+               Inst.getOpcode() == X86::CMP8rr ||
+               Inst.getOpcode() == X86::CMP8rm ||
+                Inst.getOpcode() == X86::CMP64ri32 ||
+                Inst.getOpcode() == X86::CMP64ri8 ||
+                Inst.getOpcode() == X86::CMP32ri ||
+                Inst.getOpcode() == X86::CMP32ri8 ||
+                Inst.getOpcode() == X86::CMP16ri ||
+                Inst.getOpcode() == X86::CMP16ri8 ||
+                Inst.getOpcode() == X86::CMP8ri ||
+                Inst.getOpcode() == X86::CMP8ri8; 
+    }
+
+    bool isTest(const MCInst &Inst) override {
+        return Inst.getOpcode() == X86::TEST16i16 ||
+                Inst.getOpcode() == X86::TEST16mr ||
+                Inst.getOpcode() == X86::TEST16ri ||
+                Inst.getOpcode() == X86::TEST16rr ||
+                Inst.getOpcode() == X86::TEST32i32 ||
+                Inst.getOpcode() == X86::TEST32mr ||
+                Inst.getOpcode() == X86::TEST32ri ||
+                Inst.getOpcode() == X86::TEST32rr ||
+                Inst.getOpcode() == X86::TEST64i32 ||
+                Inst.getOpcode() == X86::TEST64mr ||
+                Inst.getOpcode() == X86::TEST64ri32 ||
+                Inst.getOpcode() == X86::TEST64rr ||
+                Inst.getOpcode() == X86::TEST8i8 ||
+                Inst.getOpcode() == X86::TEST8mr ||
+                Inst.getOpcode() == X86::TEST8ri ||
+                Inst.getOpcode() == X86::TEST8rr;
+    }
+
+
+    bool createConditionalBranchForYield(InstructionListType &Instrs,
+                                        MCContext *Ctx,
+                                        int IterTh, const MCSymbol *Target, const MCSymbol *LoopCounterLoc,
+                                        unsigned Reg, int cur_loop_counter_idx, int InductionReg, int InductionConst) {
+        if (cur_loop_counter_idx != -1) {
+            MCInst LoadLoopCounterLocToReg;
+            LoadLoopCounterLocToReg.setOpcode(X86::MOV64rm);
+            LoadLoopCounterLocToReg.addOperand(MCOperand::createReg(Reg));
+            LoadLoopCounterLocToReg.addOperand(MCOperand::createReg(X86::RIP));   // BaseReg
+            LoadLoopCounterLocToReg.addOperand(MCOperand::createImm(1));          // ScaleAmt
+            LoadLoopCounterLocToReg.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // IndexReg
+            LoadLoopCounterLocToReg.addOperand(MCOperand::createExpr(
+                MCSymbolRefExpr::create(LoopCounterLoc, MCSymbolRefExpr::VK_None,
+                                        *Ctx)));   // Displacement
+            LoadLoopCounterLocToReg.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+
+            MCInst AddMemByOne;
+            AddMemByOne.setOpcode(X86::ADD16mi);
+            // Address = gs:(Reg + 2*cur_loop_counter_idx)
+            AddMemByOne.addOperand(MCOperand::createReg(Reg));
+            AddMemByOne.addOperand(MCOperand::createImm(1));   // ScaleAmt
+            AddMemByOne.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // IndexReg 
+            AddMemByOne.addOperand(MCOperand::createImm(2 * cur_loop_counter_idx));   // Displacement
+            AddMemByOne.addOperand(
+                MCOperand::createReg(X86::GS));   // AddrSegmentReg
+            AddMemByOne.addOperand(MCOperand::createImm(1));
+
+            MCInst CmpMem;
+            CmpMem.setOpcode(X86::CMP16mi);
+            CmpMem.addOperand(MCOperand::createReg(Reg));
+            CmpMem.addOperand(MCOperand::createImm(1));   // ScaleAmt
+            CmpMem.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // IndexReg
+            CmpMem.addOperand(MCOperand::createImm(2 * cur_loop_counter_idx));   // Displacement
+            CmpMem.addOperand(
+                MCOperand::createReg(X86::GS));   // AddrSegmentReg
+            CmpMem.addOperand(MCOperand::createImm(IterTh));
+
+            Instrs.push_back(LoadLoopCounterLocToReg);
+            Instrs.push_back(AddMemByOne);
+            Instrs.push_back(CmpMem);
+        } else if (InductionReg == 0) {
+            MCInst AddRegByOne;
+            AddRegByOne.setOpcode(X86::ADD64ri32);
+            AddRegByOne.addOperand(MCOperand::createReg(Reg));
+            AddRegByOne.addOperand(MCOperand::createReg(Reg));
+            AddRegByOne.addOperand(MCOperand::createImm(1));
+
+            MCInst CmpRegToIterTh;
+            CmpRegToIterTh.setOpcode(X86::CMP64ri32);
+            CmpRegToIterTh.addOperand(MCOperand::createReg(Reg));
+            CmpRegToIterTh.addOperand(MCOperand::createImm(IterTh));
+            Instrs.push_back(AddRegByOne);
+            Instrs.push_back(CmpRegToIterTh);
+        } else {
+            MCInst CmpIRegToReg;
+            CmpIRegToReg.setOpcode(X86::CMP64rr);
+            CmpIRegToReg.addOperand(MCOperand::createReg(InductionReg));
+            CmpIRegToReg.addOperand(MCOperand::createReg(Reg));
+            if (!opts::TestNoCmp)
+                Instrs.push_back(CmpIRegToReg);
+        }
+
+        MCInst JumpToNextCtx;
+        JumpToNextCtx.setOpcode(X86::JCC_4);
+        JumpToNextCtx.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(Target, MCSymbolRefExpr::VK_None, *Ctx)));
+        JumpToNextCtx.addOperand(MCOperand::createImm(X86::COND_LE));
+        
+        // [SAM] for test
+        if (!opts::TestNoJump)
+            Instrs.push_back(JumpToNextCtx);
+
+        if (InductionReg) {
+            MCInst AddRegByIterTh;
+            AddRegByIterTh.setOpcode(X86::ADD64ri32);
+            AddRegByIterTh.addOperand(MCOperand::createReg(Reg));
+            AddRegByIterTh.addOperand(MCOperand::createReg(Reg));
+            AddRegByIterTh.addOperand(MCOperand::createImm(IterTh * InductionConst));
+            // [SAM] for test
+            if (!opts::TestNoJump)
+                Instrs.push_back(AddRegByIterTh);
+        } else {
+            if (cur_loop_counter_idx != -1) {
+                MCInst SetMemToZero;
+                SetMemToZero.setOpcode(X86::MOV16mi);
+                // same address
+                SetMemToZero.addOperand(MCOperand::createReg(Reg));
+                SetMemToZero.addOperand(MCOperand::createImm(1));   // ScaleAmt
+                SetMemToZero.addOperand(
+                    MCOperand::createReg(X86::NoRegister));   // IndexReg
+                SetMemToZero.addOperand(MCOperand::createImm(2 * cur_loop_counter_idx));   // Displacement
+                SetMemToZero.addOperand(
+                    MCOperand::createReg(X86::GS));   // AddrSegmentReg
+                SetMemToZero.addOperand(MCOperand::createImm(0));
+                Instrs.push_back(SetMemToZero);
+            } else {
+                MCInst SetRegToZero;
+                SetRegToZero.setOpcode(X86::XOR64rr);
+                SetRegToZero.addOperand(MCOperand::createReg(Reg));
+                SetRegToZero.addOperand(MCOperand::createReg(Reg));
+                SetRegToZero.addOperand(MCOperand::createReg(Reg));
+                Instrs.push_back(SetRegToZero);
+            }
+        }
+
+        return true;
+    }
+
+    bool createYieldInstrs(InstructionListType &Instrs, const MCInst &CMInst,
+                           const MCSymbol *CrtPos, MCContext *Ctx,
+                           const BitVector &AliveRegs, bool RetFloat, bool IsNormalYield, int cur_loop_counter_idx) override {
+        BitVector RegsToSave = AliveRegs;
+
+        unsigned CCR;
+        getCrtCtxReg(CCR, 8);
+        unsigned CCRdw;
+        getCrtCtxReg(CCRdw, 4);
+        unsigned CCRw;
+        getCrtCtxReg(CCRw, 2);
+        RegsToSave.set(CCR);
+        RegsToSave.set(X86::RCX);
+
+        bool isCB = isConditionalBranch(CMInst);
+
+        // invariant: aliveregs include only gprs or xmm0, xmm1
+        // push
+
+        if (isCB) {
+            MCInst PushEFLAGSInst;
+            createPushRegister(PushEFLAGSInst, X86::EFLAGS, 8);
+            Instrs.push_back(PushEFLAGSInst);
+        }
+
+        bool XMMAliveVec[4] = {false, false, false, false};
+        XMMAliveVec[0] = RegsToSave.test(X86::XMM0) && RetFloat;
+        XMMAliveVec[1] = RegsToSave.test(X86::XMM1) && RetFloat;
+        XMMAliveVec[2] = RegsToSave.test(X86::XMM2) && RetFloat;
+        XMMAliveVec[3] = RegsToSave.test(X86::XMM3) && RetFloat;
+
+        for (int i = 0; i < 4; i++) {
+            if (!XMMAliveVec[i])
+                continue;
+            unsigned XMMReg = X86::XMM0 + i;
+            MCInst SubStack16Inst, PushXMMInst;
+            SubStack16Inst.setOpcode(X86::SUB64ri8);
+            SubStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            SubStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            SubStack16Inst.addOperand(MCOperand::createImm(16));
+            Instrs.push_back(SubStack16Inst);
+
+            PushXMMInst.setOpcode(X86::MOVUPSmr);
+            PushXMMInst.addOperand(
+                MCOperand::createReg(X86::RSP));                // BaseReg
+            PushXMMInst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+            PushXMMInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));         // IndexReg
+            PushXMMInst.addOperand(MCOperand::createImm(0));   // Displacement
+            PushXMMInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+            PushXMMInst.addOperand(
+                MCOperand::createReg(XMMReg));   // src reg
+            Instrs.push_back(PushXMMInst);
+        }
+
+        /*
+        if (XMM0Alive) {
+            MCInst SubStack16Inst, PushXMM0Inst;
+            SubStack16Inst.setOpcode(X86::SUB64ri8);
+            SubStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            SubStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            SubStack16Inst.addOperand(MCOperand::createImm(16));
+            Instrs.push_back(SubStack16Inst);
+
+            PushXMM0Inst.setOpcode(X86::MOVUPSmr);
+            PushXMM0Inst.addOperand(
+                MCOperand::createReg(X86::RSP));                // BaseReg
+            PushXMM0Inst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+            PushXMM0Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));         // IndexReg
+            PushXMM0Inst.addOperand(MCOperand::createImm(0));   // Displacement
+            PushXMM0Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+            PushXMM0Inst.addOperand(
+                MCOperand::createReg(X86::XMM0));   // src reg
+            Instrs.push_back(PushXMM0Inst);
+        }
+
+        if (XMM1Alive) {
+            MCInst SubStack16Inst, PushXMM1Inst;
+            SubStack16Inst.setOpcode(X86::SUB64ri8);
+            SubStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            SubStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            SubStack16Inst.addOperand(MCOperand::createImm(16));
+            Instrs.push_back(SubStack16Inst);
+
+            PushXMM1Inst.setOpcode(X86::MOVUPSmr);
+            PushXMM1Inst.addOperand(
+                MCOperand::createReg(X86::RSP));                // BaseReg
+            PushXMM1Inst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+            PushXMM1Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));         // IndexReg
+            PushXMM1Inst.addOperand(MCOperand::createImm(0));   // Displacement
+            PushXMM1Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+            PushXMM1Inst.addOperand(
+                MCOperand::createReg(X86::XMM1));   // src reg
+            Instrs.push_back(PushXMM1Inst);
+        }*/
+
+        for (MCPhysReg I = 1; I < RegInfo->getNumRegs(); ++I) {
+            if (!RegsToSave.test(I) || I == X86::XMM0 || I == X86::XMM1 || I == X86::XMM2 || I == X86::XMM3) {
+                continue;
+            } else {
+                outs() << "reg " << I << " is alive\n";
+            }
+            MCInst PushInst;
+            createPushRegister(PushInst, I, 8);
+            Instrs.push_back(PushInst);
+        }
+
+        // yield
+        {
+            // expected code (operand order follows intel syntax)
+            // (1) set idx at rdi
+            // (2) mov gs:[0 + rdi * SizeOfCrtCtx + OffsetToSp], %rsp
+            // (3) lea %rcx, 0x12(%rip)
+            // (4) mov gs:[0 + rdi * SizeOfCrtCtx + OffsetToIp], %rcx
+            // (5) set next_idx at rdi
+            // (6) mov %rsp, gs:[0 + rdi * SizeOfCrtCtx + OffsetToSp]
+            // (7) (indirect jump) jmp gs:[0 + rdi * SizeOfCrtCtx + OffsetToIp]
+
+            // crt_ctx = gs:[crt_idx * sizeof coroutine_ctx]
+            // (1) set idx at rdi
+
+            // RDI = SizeOfYieldCtx * EDI
+
+            // MCInst ImulCtxSzInst;
+            // ImulCtxSzInst.setOpcode(X86::IMUL64rri32);
+            // ImulCtxSzInst.addOperand(MCOperand::createReg(X86::RDI)); //
+            // upper 32-bit of RDI is zero-extended
+            // ImulCtxSzInst.addOperand(MCOperand::createReg(X86::EDI));
+            // ImulCtxSzInst.addOperand(MCOperand::createImm(SizeOfYieldCtx));
+
+            // optimized version. Assumed SizeOfYieldCtx = 20
+            MCInst Mul1Inst;
+            MCInst Mul2Inst;
+            MCInst Mul3Inst;
+            MCInst Mul4Inst;
+
+            Mul1Inst.setOpcode(X86::MOV64rr);
+            Mul1Inst.addOperand(MCOperand::createReg(X86::RDX));
+            Mul1Inst.addOperand(MCOperand::createReg(X86::RDI));
+
+            Mul2Inst.setOpcode(X86::SHL64ri);
+            Mul2Inst.addOperand(MCOperand::createReg(X86::RDX));
+            Mul2Inst.addOperand(MCOperand::createReg(X86::RDX));
+            Mul2Inst.addOperand(MCOperand::createImm(2));
+
+            Mul3Inst.setOpcode(X86::ADD64rr);
+            Mul3Inst.addOperand(MCOperand::createReg(X86::RDI));
+            Mul3Inst.addOperand(MCOperand::createReg(X86::RDI));
+            Mul3Inst.addOperand(MCOperand::createReg(X86::RDX));
+
+            Mul4Inst.setOpcode(X86::SHL64ri);
+            Mul4Inst.addOperand(MCOperand::createReg(X86::RDI));
+            Mul4Inst.addOperand(MCOperand::createReg(X86::RDI));
+            Mul4Inst.addOperand(MCOperand::createImm(2));
+            if (CrtPos) {   // scav
+                // set idx reg = value in CrtPos
+                // let's use rdi for idx reg
+                MCInst MovIdxInst;
+                MovIdxInst.setOpcode(X86::MOV32rm);
+                MovIdxInst.addOperand(MCOperand::createReg(CCRdw));
+                MovIdxInst.addOperand(
+                    MCOperand::createReg(X86::RIP));              // BaseReg
+                MovIdxInst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+                MovIdxInst.addOperand(
+                    MCOperand::createReg(X86::NoRegister));   // IndexReg
+                MovIdxInst.addOperand(MCOperand::createExpr(
+                    MCSymbolRefExpr::create(CrtPos, MCSymbolRefExpr::VK_None,
+                                            *Ctx)));   // Displacement
+                MovIdxInst.addOperand(
+                    MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+
+                addAnnotation(MovIdxInst, "FirstCtxAccess", 1);
+                Instrs.push_back(MovIdxInst);
+                // Instrs.push_back(Mul1Inst);
+                // Instrs.push_back(Mul2Inst);
+                // Instrs.push_back(Mul3Inst);
+                // Instrs.push_back(Mul4Inst);
+                // Instrs.push_back(ImulCtxSzInst);
+            } else {   // primary
+                // set idx reg = 0
+                // mov %rdi 0
+                MCInst MovIdxInst;
+                MovIdxInst.setOpcode(X86::XOR64rr);
+                MovIdxInst.addOperand(MCOperand::createReg(CCR));
+                MovIdxInst.addOperand(MCOperand::createReg(CCR));
+                MovIdxInst.addOperand(MCOperand::createReg(CCR));
+                /*
+                MovIdxInst.setOpcode(X86::MOV64ri);
+                MovIdxInst.addOperand(MCOperand::createReg(X86::RDI));
+                MovIdxInst.addOperand(MCOperand::createImm(0));
+                */
+                addAnnotation(MovIdxInst, "FirstCtxAccess", 1);
+                Instrs.push_back(MovIdxInst);
+            }
+
+            // (2) mov gs:[0 + rdi * SizeOfCrtCtx + OffsetToSp], %rsp
+            // store RSP at gs:[0 + rdi * SizeOfCrtCtx + OffsetToSp]
+            MCInst StoreSpInst;
+            StoreSpInst.setOpcode(X86::MOV64mr);
+
+            StoreSpInst.addOperand(MCOperand::createReg(CCR));   // BaseReg
+            StoreSpInst.addOperand(MCOperand::createImm(0));     // ScaleAmt
+            StoreSpInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // IndexReg
+            StoreSpInst.addOperand(
+                MCOperand::createImm(OffsetToSp));   // Displacement
+            StoreSpInst.addOperand(
+                MCOperand::createReg(X86::GS));   // AddrSegmentReg
+            StoreSpInst.addOperand(MCOperand::createReg(X86::RSP));   // src reg
+
+            // (3) lea %rcx, 0x11(%rip)
+            // (4) mov gs:[0 + rdi * SizeOfCrtCtx + OffsetToIp], %rcx
+            // compute load return addr, Constant = offset to where control is
+            // back lea 0xoo(%rip), %rcx
+            MCInst LoadReturnIpInst;
+            LoadReturnIpInst.setOpcode(X86::LEA64r);
+            LoadReturnIpInst.addOperand(MCOperand::createReg(X86::RCX));   // dst
+            LoadReturnIpInst.addOperand(
+                MCOperand::createReg(X86::RIP));                    // BaseReg
+            LoadReturnIpInst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+            LoadReturnIpInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // IndexReg
+            LoadReturnIpInst.addOperand(MCOperand::createImm(
+                18));   // Displacement: should be manually computed
+            LoadReturnIpInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+
+            MCInst StoreIpInst;
+            StoreIpInst.setOpcode(X86::MOV64mr);
+            StoreIpInst.addOperand(MCOperand::createReg(CCR));   // BaseReg
+            StoreIpInst.addOperand(MCOperand::createImm(0));     // ScaleAmt
+            StoreIpInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // IndexReg
+            StoreIpInst.addOperand(
+                MCOperand::createImm(OffsetToIp));   // Displacement
+            StoreIpInst.addOperand(
+                MCOperand::createReg(X86::GS));   // AddrSegmentReg
+            StoreIpInst.addOperand(MCOperand::createReg(X86::RCX));   // src reg
+
+            // (5) set next_idx at rdi
+            // mov %rdi, gs:[0 + rdi * SizeOfCrtCtx + OffsetToNext]
+            MCInst MovNextIdxInst;
+            MovNextIdxInst.setOpcode(X86::MOV16rm);
+            MovNextIdxInst.addOperand(MCOperand::createReg(CCRw));   // dst reg
+            MovNextIdxInst.addOperand(MCOperand::createReg(CCR));    // BaseReg
+            MovNextIdxInst.addOperand(MCOperand::createImm(0));      // ScaleAmt
+            MovNextIdxInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // IndexReg
+            MovNextIdxInst.addOperand(MCOperand::createImm(
+                IsNormalYield ? OffsetToNext
+                            : OffsetToSpecialNext));   // Displacement
+            MovNextIdxInst.addOperand(
+                MCOperand::createReg(X86::GS));   // AddrSegmentReg
+
+            // (6) mov %rsp, gs:[0 + rdi * SizeOfCrtCtx + OffsetToSp]
+            // (7) (indirect jump) jmp gs:[0 + rdi * SizeOfCrtCtx + OffsetToIp]
+            // mov %rsp, gs:[0 + rdi * SizeOfCrtCtx + OffsetToSp]
+            MCInst LoadSpInst;
+            LoadSpInst.setOpcode(X86::MOV64rm);
+            LoadSpInst.addOperand(MCOperand::createReg(X86::RSP));   // dst reg
+            LoadSpInst.addOperand(MCOperand::createReg(CCR));        // BaseReg
+            LoadSpInst.addOperand(MCOperand::createImm(0));          // ScaleAmt
+            LoadSpInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // IndexReg
+            LoadSpInst.addOperand(
+                MCOperand::createImm(OffsetToSp));   // Displacement
+            LoadSpInst.addOperand(
+                MCOperand::createReg(X86::GS));   // AddrSegmentReg
+
+            MCInst JmpNextCrtInst;
+            JmpNextCrtInst.setOpcode(X86::JMP64m);
+            JmpNextCrtInst.addOperand(MCOperand::createReg(CCR));   // BaseReg
+            JmpNextCrtInst.addOperand(MCOperand::createImm(0));     // ScaleAmt
+            JmpNextCrtInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // IndexReg
+            JmpNextCrtInst.addOperand(
+                MCOperand::createImm(OffsetToIp));   // Displacement
+            JmpNextCrtInst.addOperand(
+                MCOperand::createReg(X86::GS));   // AddrSegmentReg
+
+            Instrs.push_back(StoreSpInst);
+            Instrs.push_back(LoadReturnIpInst);
+            Instrs.push_back(StoreIpInst);
+            Instrs.push_back(MovNextIdxInst);
+            // Instrs.push_back(ImulCtxSzInst); // repeat for next crt
+            // Instrs.push_back(Mul1Inst);
+            // Instrs.push_back(Mul2Inst);
+            // Instrs.push_back(Mul3Inst);
+            // Instrs.push_back(Mul4Inst);
+
+            Instrs.push_back(LoadSpInst);
+            Instrs.push_back(JmpNextCrtInst);
+        }
+
+        // pop
+        bool IsFirstPop = true;
+        for (MCPhysReg I = RegInfo->getNumRegs() - 1; I > 0; --I) {
+            if (!RegsToSave.test(I) || I == X86::XMM0 || I == X86::XMM1 || I == X86::XMM2|| I == X86::XMM3)
+                continue;
+
+            MCInst PopInst;
+            createPopRegister(PopInst, I, 8);
+            if (IsFirstPop) {
+                addAnnotation(PopInst, "FirstPop", 1);
+                IsFirstPop = false;
+            }
+
+            Instrs.push_back(PopInst);
+        }
+
+        for (int i = 3; i >= 0; i--) {
+            if (!XMMAliveVec[i])
+                continue;
+            unsigned XMMReg = X86::XMM0 + i;
+            MCInst PopXMMInst, AddStack16Inst;
+            PopXMMInst.setOpcode(X86::MOVUPSrm);
+            PopXMMInst.addOperand(MCOperand::createReg(XMMReg));
+            PopXMMInst.addOperand(MCOperand::createReg(X86::RSP));   // BaseReg
+            PopXMMInst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+            PopXMMInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));        // IndexReg
+            PopXMMInst.addOperand(MCOperand::createImm(0));   // Displacement
+            PopXMMInst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+            Instrs.push_back(PopXMMInst);
+
+            AddStack16Inst.setOpcode(X86::ADD64ri8);
+            AddStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            AddStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            AddStack16Inst.addOperand(MCOperand::createImm(16));
+            Instrs.push_back(AddStack16Inst);
+        }
+        /*
+        if (XMM1Alive) {
+            MCInst PopXMM1Inst, AddStack16Inst;
+            PopXMM1Inst.setOpcode(X86::MOVUPSrm);
+            PopXMM1Inst.addOperand(MCOperand::createReg(X86::XMM1));
+            PopXMM1Inst.addOperand(MCOperand::createReg(X86::RSP));   // BaseReg
+            PopXMM1Inst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+            PopXMM1Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));        // IndexReg
+            PopXMM1Inst.addOperand(MCOperand::createImm(0));   // Displacement
+            PopXMM1Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+            Instrs.push_back(PopXMM1Inst);
+
+            AddStack16Inst.setOpcode(X86::ADD64ri8);
+            AddStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            AddStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            AddStack16Inst.addOperand(MCOperand::createImm(16));
+            Instrs.push_back(AddStack16Inst);
+        }
+
+        if (XMM0Alive) {
+            MCInst PopXMM0Inst, AddStack16Inst;
+            PopXMM0Inst.setOpcode(X86::MOVUPSrm);
+            PopXMM0Inst.addOperand(MCOperand::createReg(X86::XMM0));
+            PopXMM0Inst.addOperand(MCOperand::createReg(X86::RSP));   // BaseReg
+            PopXMM0Inst.addOperand(MCOperand::createImm(1));   // ScaleAmt
+            PopXMM0Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));        // IndexReg
+            PopXMM0Inst.addOperand(MCOperand::createImm(0));   // Displacement
+            PopXMM0Inst.addOperand(
+                MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+            Instrs.push_back(PopXMM0Inst);
+
+            AddStack16Inst.setOpcode(X86::ADD64ri8);
+            AddStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            AddStack16Inst.addOperand(MCOperand::createReg(X86::RSP));
+            AddStack16Inst.addOperand(MCOperand::createImm(16));
+            Instrs.push_back(AddStack16Inst);
+        }*/
+ 
+
+        if (isCB) {
+            MCInst PopEFLAGSInst;
+            createPopRegister(PopEFLAGSInst, X86::EFLAGS, 8);
+            Instrs.push_back(PopEFLAGSInst);
+        }
+
+        return true;
+    }
+
+    bool createReturn(MCInst &Inst) const override {
+        Inst.setOpcode(X86::RET64);
+        return true;
+    }
+
+    InstructionListType createInlineMemcpy(bool ReturnEnd) const override {
+        InstructionListType Code;
+        if (ReturnEnd)
+            Code.emplace_back(MCInstBuilder(X86::LEA64r)
+                                  .addReg(X86::RAX)
+                                  .addReg(X86::RDI)
+                                  .addImm(1)
+                                  .addReg(X86::RDX)
+                                  .addImm(0)
+                                  .addReg(X86::NoRegister));
+        else
+            Code.emplace_back(
+                MCInstBuilder(X86::MOV64rr).addReg(X86::RAX).addReg(X86::RDI));
+
+        Code.emplace_back(
+            MCInstBuilder(X86::MOV32rr).addReg(X86::ECX).addReg(X86::EDX));
+        Code.emplace_back(MCInstBuilder(X86::REP_MOVSB_64));
+
+        return Code;
+    }
+
+    InstructionListType createOneByteMemcpy() const override {
+        InstructionListType Code;
+        Code.emplace_back(MCInstBuilder(X86::MOV8rm)
+                              .addReg(X86::CL)
+                              .addReg(X86::RSI)
+                              .addImm(0)
+                              .addReg(X86::NoRegister)
+                              .addImm(0)
+                              .addReg(X86::NoRegister));
+        Code.emplace_back(MCInstBuilder(X86::MOV8mr)
+                              .addReg(X86::RDI)
+                              .addImm(0)
+                              .addReg(X86::NoRegister)
+                              .addImm(0)
+                              .addReg(X86::NoRegister)
+                              .addReg(X86::CL));
+        Code.emplace_back(
+            MCInstBuilder(X86::MOV64rr).addReg(X86::RAX).addReg(X86::RDI));
+        return Code;
+    }
+
+    InstructionListType createCmpJE(MCPhysReg RegNo, int64_t Imm,
+                                    const MCSymbol *Target,
+                                    MCContext      *Ctx) const override {
+        InstructionListType Code;
+        Code.emplace_back(
+            MCInstBuilder(X86::CMP64ri8).addReg(RegNo).addImm(Imm));
+        Code.emplace_back(MCInstBuilder(X86::JCC_1)
+                              .addExpr(MCSymbolRefExpr::create(
+                                  Target, MCSymbolRefExpr::VK_None, *Ctx))
+                              .addImm(X86::COND_E));
+        return Code;
+    }
+
+    std::optional<Relocation>
+    createRelocation(const MCFixup      &Fixup,
+                     const MCAsmBackend &MAB) const override {
+        const MCFixupKindInfo &FKI = MAB.getFixupKindInfo(Fixup.getKind());
+
+        assert(FKI.TargetOffset == 0 && "0-bit relocation offset expected");
+        const uint64_t RelOffset = Fixup.getOffset();
+
+        uint64_t RelType;
+        if (FKI.Flags & MCFixupKindInfo::FKF_IsPCRel) {
+            switch (FKI.TargetSize) {
+            default:
+                return std::nullopt;
+            case 8:
+                RelType = ELF::R_X86_64_PC8;
+                break;
+            case 16:
+                RelType = ELF::R_X86_64_PC16;
+                break;
+            case 32:
+                RelType = ELF::R_X86_64_PC32;
+                break;
+            case 64:
+                RelType = ELF::R_X86_64_PC64;
+                break;
+            }
+        } else {
+            switch (FKI.TargetSize) {
+            default:
+                return std::nullopt;
+            case 8:
+                RelType = ELF::R_X86_64_8;
+                break;
+            case 16:
+                RelType = ELF::R_X86_64_16;
+                break;
+            case 32:
+                RelType = ELF::R_X86_64_32;
+                break;
+            case 64:
+                RelType = ELF::R_X86_64_64;
+                break;
+            }
+        }
+
+        // Extract a symbol and an addend out of the fixup value expression.
+        //
+        // Only the following limited expression types are supported:
+        //   Symbol + Addend
+        //   Symbol
+        uint64_t      Addend    = 0;
+        MCSymbol     *Symbol    = nullptr;
+        const MCExpr *ValueExpr = Fixup.getValue();
+        if (ValueExpr->getKind() == MCExpr::Binary) {
+            const auto *BinaryExpr = cast<MCBinaryExpr>(ValueExpr);
+            assert(BinaryExpr->getOpcode() == MCBinaryExpr::Add &&
+                   "unexpected binary expression");
+            const MCExpr *LHS = BinaryExpr->getLHS();
+            assert(LHS->getKind() == MCExpr::SymbolRef && "unexpected LHS");
+            Symbol = const_cast<MCSymbol *>(this->getTargetSymbol(LHS));
+            const MCExpr *RHS = BinaryExpr->getRHS();
+            assert(RHS->getKind() == MCExpr::Constant && "unexpected RHS");
+            Addend = cast<MCConstantExpr>(RHS)->getValue();
+        } else {
+            assert(ValueExpr->getKind() == MCExpr::SymbolRef &&
+                   "unexpected value");
+            Symbol = const_cast<MCSymbol *>(this->getTargetSymbol(ValueExpr));
+        }
+
+        return Relocation({RelOffset, Symbol, RelType, Addend, 0});
+    }
+
+    bool replaceImmWithSymbolRef(MCInst &Inst, const MCSymbol *Symbol,
+                                 int64_t Addend, MCContext *Ctx, int64_t &Value,
+                                 uint64_t RelType) const override {
+        unsigned ImmOpNo = -1U;
+
+        for (unsigned Index = 0; Index < MCPlus::getNumPrimeOperands(Inst);
+             ++Index) {
+            if (Inst.getOperand(Index).isImm()) {
+                ImmOpNo = Index;
+                // TODO: this is a bit hacky.  It finds the correct operand by
+                // searching for a specific immediate value.  If no value is
+                // provided it defaults to the last immediate operand found.
+                // This could lead to unexpected results if the instruction
+                // has more than one immediate with the same value.
+                if (Inst.getOperand(ImmOpNo).getImm() == Value)
+                    break;
+            }
+        }
+
+        if (ImmOpNo == -1U)
+            return false;
+
+        Value = Inst.getOperand(ImmOpNo).getImm();
+
+        setOperandToSymbolRef(Inst, ImmOpNo, Symbol, Addend, Ctx, RelType);
+
+        return true;
+    }
+
+    bool replaceRegWithImm(MCInst &Inst, unsigned Register,
+                           int64_t Imm) const override {
+
+        enum CheckSignExt : uint8_t {
+            NOCHECK = 0,
+            CHECK8,
+            CHECK32,
+        };
+
+        using CheckList = std::vector<std::pair<CheckSignExt, unsigned>>;
+        struct InstInfo {
+            // Size in bytes that Inst loads from memory.
+            uint8_t DataSize;
+
+            // True when the target operand has to be duplicated because the
+            // opcode expects a LHS operand.
+            bool HasLHS;
+
+            // List of checks and corresponding opcodes to be used. We try to
+            // use the smallest possible immediate value when various sizes are
+            // available, hence we may need to check whether a larger constant
+            // fits in a smaller immediate.
+            CheckList Checks;
+        };
+
+        InstInfo I;
+
+        switch (Inst.getOpcode()) {
+        default: {
+            switch (getPushSize(Inst)) {
+
+            case 2:
+                I = {2,
+                     false,
+                     {{CHECK8, X86::PUSH16i8}, {NOCHECK, X86::PUSHi16}}};
+                break;
+            case 4:
+                I = {4,
+                     false,
+                     {{CHECK8, X86::PUSH32i8}, {NOCHECK, X86::PUSHi32}}};
+                break;
+            case 8:
+                I = {8,
+                     false,
+                     {{CHECK8, X86::PUSH64i8},
+                      {CHECK32, X86::PUSH64i32},
+                      {NOCHECK, Inst.getOpcode()}}};
+                break;
+            default:
+                return false;
+            }
+            break;
+        }
+
+        // MOV
+        case X86::MOV8rr:
+            I = {1, false, {{NOCHECK, X86::MOV8ri}}};
+            break;
+        case X86::MOV16rr:
+            I = {2, false, {{NOCHECK, X86::MOV16ri}}};
+            break;
+        case X86::MOV32rr:
+            I = {4, false, {{NOCHECK, X86::MOV32ri}}};
+            break;
+        case X86::MOV64rr:
+            I = {
+                8, false, {{CHECK32, X86::MOV64ri32}, {NOCHECK, X86::MOV64ri}}};
+            break;
+
+        case X86::MOV8mr:
+            I = {1, false, {{NOCHECK, X86::MOV8mi}}};
+            break;
+        case X86::MOV16mr:
+            I = {2, false, {{NOCHECK, X86::MOV16mi}}};
+            break;
+        case X86::MOV32mr:
+            I = {4, false, {{NOCHECK, X86::MOV32mi}}};
+            break;
+        case X86::MOV64mr:
+            I = {
+                8, false, {{CHECK32, X86::MOV64mi32}, {NOCHECK, X86::MOV64mr}}};
+            break;
+
+        // MOVZX
+        case X86::MOVZX16rr8:
+            I = {1, false, {{NOCHECK, X86::MOV16ri}}};
+            break;
+        case X86::MOVZX32rr8:
+            I = {1, false, {{NOCHECK, X86::MOV32ri}}};
+            break;
+        case X86::MOVZX32rr16:
+            I = {2, false, {{NOCHECK, X86::MOV32ri}}};
+            break;
+
+        // CMP
+        case X86::CMP8rr:
+            I = {1, false, {{NOCHECK, X86::CMP8ri}}};
+            break;
+        case X86::CMP16rr:
+            I = {2, false, {{CHECK8, X86::CMP16ri8}, {NOCHECK, X86::CMP16ri}}};
+            break;
+        case X86::CMP32rr:
+            I = {4, false, {{CHECK8, X86::CMP32ri8}, {NOCHECK, X86::CMP32ri}}};
+            break;
+        case X86::CMP64rr:
+            I = {8,
+                 false,
+                 {{CHECK8, X86::CMP64ri8},
+                  {CHECK32, X86::CMP64ri32},
+                  {NOCHECK, X86::CMP64rr}}};
+            break;
+
+        // TEST
+        case X86::TEST8rr:
+            I = {1, false, {{NOCHECK, X86::TEST8ri}}};
+            break;
+        case X86::TEST16rr:
+            I = {2, false, {{NOCHECK, X86::TEST16ri}}};
+            break;
+        case X86::TEST32rr:
+            I = {4, false, {{NOCHECK, X86::TEST32ri}}};
+            break;
+        case X86::TEST64rr:
+            I = {8,
+                 false,
+                 {{CHECK32, X86::TEST64ri32}, {NOCHECK, X86::TEST64rr}}};
+            break;
+
+        // ADD
+        case X86::ADD8rr:
+            I = {1, true, {{NOCHECK, X86::ADD8ri}}};
+            break;
+        case X86::ADD16rr:
+            I = {2, true, {{CHECK8, X86::ADD16ri8}, {NOCHECK, X86::ADD16ri}}};
+            break;
+        case X86::ADD32rr:
+            I = {4, true, {{CHECK8, X86::ADD32ri8}, {NOCHECK, X86::ADD32ri}}};
+            break;
+        case X86::ADD64rr:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::ADD64ri8},
+                  {CHECK32, X86::ADD64ri32},
+                  {NOCHECK, X86::ADD64rr}}};
+            break;
+
+        // SUB
+        case X86::SUB8rr:
+            I = {1, true, {{NOCHECK, X86::SUB8ri}}};
+            break;
+        case X86::SUB16rr:
+            I = {2, true, {{CHECK8, X86::SUB16ri8}, {NOCHECK, X86::SUB16ri}}};
+            break;
+        case X86::SUB32rr:
+            I = {4, true, {{CHECK8, X86::SUB32ri8}, {NOCHECK, X86::SUB32ri}}};
+            break;
+        case X86::SUB64rr:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::SUB64ri8},
+                  {CHECK32, X86::SUB64ri32},
+                  {NOCHECK, X86::SUB64rr}}};
+            break;
+
+        // AND
+        case X86::AND8rr:
+            I = {1, true, {{NOCHECK, X86::AND8ri}}};
+            break;
+        case X86::AND16rr:
+            I = {2, true, {{CHECK8, X86::AND16ri8}, {NOCHECK, X86::AND16ri}}};
+            break;
+        case X86::AND32rr:
+            I = {4, true, {{CHECK8, X86::AND32ri8}, {NOCHECK, X86::AND32ri}}};
+            break;
+        case X86::AND64rr:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::AND64ri8},
+                  {CHECK32, X86::AND64ri32},
+                  {NOCHECK, X86::AND64rr}}};
+            break;
+
+        // OR
+        case X86::OR8rr:
+            I = {1, true, {{NOCHECK, X86::OR8ri}}};
+            break;
+        case X86::OR16rr:
+            I = {2, true, {{CHECK8, X86::OR16ri8}, {NOCHECK, X86::OR16ri}}};
+            break;
+        case X86::OR32rr:
+            I = {4, true, {{CHECK8, X86::OR32ri8}, {NOCHECK, X86::OR32ri}}};
+            break;
+        case X86::OR64rr:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::OR64ri8},
+                  {CHECK32, X86::OR64ri32},
+                  {NOCHECK, X86::OR64rr}}};
+            break;
+
+        // XOR
+        case X86::XOR8rr:
+            I = {1, true, {{NOCHECK, X86::XOR8ri}}};
+            break;
+        case X86::XOR16rr:
+            I = {2, true, {{CHECK8, X86::XOR16ri8}, {NOCHECK, X86::XOR16ri}}};
+            break;
+        case X86::XOR32rr:
+            I = {4, true, {{CHECK8, X86::XOR32ri8}, {NOCHECK, X86::XOR32ri}}};
+            break;
+        case X86::XOR64rr:
+            I = {8,
+                 true,
+                 {{CHECK8, X86::XOR64ri8},
+                  {CHECK32, X86::XOR64ri32},
+                  {NOCHECK, X86::XOR64rr}}};
+            break;
+        }
+
+        // Compute the new opcode.
+        unsigned NewOpcode = 0;
+        for (const std::pair<CheckSignExt, unsigned> &Check : I.Checks) {
+            NewOpcode = Check.second;
+            if (Check.first == NOCHECK)
+                break;
+            if (Check.first == CHECK8 && isInt<8>(Imm))
+                break;
+            if (Check.first == CHECK32 && isInt<32>(Imm))
+                break;
+        }
+        if (NewOpcode == Inst.getOpcode())
+            return false;
+
+        const MCInstrDesc &InstDesc = Info->get(Inst.getOpcode());
+
+        unsigned NumFound = 0;
+        for (unsigned Index = InstDesc.getNumDefs() + (I.HasLHS ? 1 : 0),
+                      E     = InstDesc.getNumOperands();
+             Index != E; ++Index)
+            if (Inst.getOperand(Index).isReg() &&
+                Inst.getOperand(Index).getReg() == Register)
+                NumFound++;
+
+        if (NumFound != 1)
+            return false;
+
+        MCOperand TargetOp = Inst.getOperand(0);
+        Inst.clear();
+        Inst.setOpcode(NewOpcode);
+        Inst.addOperand(TargetOp);
+        if (I.HasLHS)
+            Inst.addOperand(TargetOp);
+        Inst.addOperand(MCOperand::createImm(Imm));
+
+        return true;
+    }
+
+    bool replaceRegWithReg(MCInst &Inst, unsigned ToReplace,
+                           unsigned ReplaceWith) const override {
+
+        // Get the HasLHS value so that iteration can be done
+        bool HasLHS;
+        if (X86::isAND(Inst.getOpcode()) || X86::isADD(Inst.getOpcode()) ||
+            X86::isSUB(Inst.getOpcode())) {
+            HasLHS = true;
+        } else if (isPop(Inst) || isPush(Inst) ||
+                   X86::isCMP(Inst.getOpcode()) ||
+                   X86::isTEST(Inst.getOpcode())) {
+            HasLHS = false;
+        } else {
+            switch (Inst.getOpcode()) {
+            case X86::MOV8rr:
+            case X86::MOV8rm:
+            case X86::MOV8mr:
+            case X86::MOV8ri:
+            case X86::MOV16rr:
+            case X86::MOV16rm:
+            case X86::MOV16mr:
+            case X86::MOV16ri:
+            case X86::MOV32rr:
+            case X86::MOV32rm:
+            case X86::MOV32mr:
+            case X86::MOV32ri:
+            case X86::MOV64rr:
+            case X86::MOV64rm:
+            case X86::MOV64mr:
+            case X86::MOV64ri:
+            case X86::MOVZX16rr8:
+            case X86::MOVZX32rr8:
+            case X86::MOVZX32rr16:
+            case X86::MOVSX32rm8:
+            case X86::MOVSX32rr8:
+            case X86::MOVSX64rm32:
+            case X86::LEA64r:
+                HasLHS = false;
+                break;
+            default:
+                return false;
+            }
+        }
+
+        const MCInstrDesc &InstDesc = Info->get(Inst.getOpcode());
+
+        bool FoundOne = false;
+
+        // Iterate only through src operands that arent also dest operands
+        for (unsigned Index = InstDesc.getNumDefs() + (HasLHS ? 1 : 0),
+                      E     = InstDesc.getNumOperands();
+             Index != E; ++Index) {
+            BitVector RegAliases = getAliases(ToReplace, true);
+            if (!Inst.getOperand(Index).isReg() ||
+                !RegAliases.test(Inst.getOperand(Index).getReg()))
+                continue;
+            // Resize register if needed
+            unsigned SizedReplaceWith = getAliasSized(
+                ReplaceWith, getRegSize(Inst.getOperand(Index).getReg()));
+            MCOperand NewOperand   = MCOperand::createReg(SizedReplaceWith);
+            Inst.getOperand(Index) = NewOperand;
+            FoundOne               = true;
+        }
+
+        // Return true if at least one operand was replaced
+        return FoundOne;
+    }
+
+    bool createUncondBranch(MCInst &Inst, const MCSymbol *TBB,
+                            MCContext *Ctx) const override {
+        Inst.setOpcode(X86::JMP_1);
+        Inst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(TBB, MCSymbolRefExpr::VK_None, *Ctx)));
+        return true;
+    }
+
+    bool createCall(MCInst &Inst, const MCSymbol *Target,
+                    MCContext *Ctx) override {
+        Inst.setOpcode(X86::CALL64pcrel32);
+        Inst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(Target, MCSymbolRefExpr::VK_None, *Ctx)));
+        return true;
+    }
+
+    bool createTailCall(MCInst &Inst, const MCSymbol *Target,
+                        MCContext *Ctx) override {
+        return createDirectCall(Inst, Target, Ctx, /*IsTailCall*/ true);
+    }
+
+    void createLongTailCall(InstructionListType &Seq, const MCSymbol *Target,
+                            MCContext *Ctx) override {
+        Seq.clear();
+        Seq.emplace_back();
+        createDirectCall(Seq.back(), Target, Ctx, /*IsTailCall*/ true);
+    }
+
+    bool createTrap(MCInst &Inst) const override {
+        Inst.clear();
+        Inst.setOpcode(X86::TRAP);
+        return true;
+    }
+
+    bool reverseBranchCondition(MCInst &Inst, const MCSymbol *TBB,
+                                MCContext *Ctx) const override {
+        unsigned InvCC = getInvertedCondCode(getCondCode(Inst));
+        assert(InvCC != X86::COND_INVALID && "invalid branch instruction");
+        Inst.getOperand(Info->get(Inst.getOpcode()).NumOperands - 1)
+            .setImm(InvCC);
+        Inst.getOperand(0) = MCOperand::createExpr(
+            MCSymbolRefExpr::create(TBB, MCSymbolRefExpr::VK_None, *Ctx));
+        return true;
+    }
+
+    bool replaceBranchCondition(MCInst &Inst, const MCSymbol *TBB,
+                                MCContext *Ctx, unsigned CC) const override {
+        if (CC == X86::COND_INVALID)
+            return false;
+        Inst.getOperand(Info->get(Inst.getOpcode()).NumOperands - 1).setImm(CC);
+        Inst.getOperand(0) = MCOperand::createExpr(
+            MCSymbolRefExpr::create(TBB, MCSymbolRefExpr::VK_None, *Ctx));
+        return true;
+    }
+
+    unsigned getCanonicalBranchCondCode(unsigned CC) const override {
+        switch (CC) {
+        default:
+            return X86::COND_INVALID;
+
+        case X86::COND_E:
+            return X86::COND_E;
+        case X86::COND_NE:
+            return X86::COND_E;
+
+        case X86::COND_L:
+            return X86::COND_L;
+        case X86::COND_GE:
+            return X86::COND_L;
+
+        case X86::COND_LE:
+            return X86::COND_G;
+        case X86::COND_G:
+            return X86::COND_G;
+
+        case X86::COND_B:
+            return X86::COND_B;
+        case X86::COND_AE:
+            return X86::COND_B;
+
+        case X86::COND_BE:
+            return X86::COND_A;
+        case X86::COND_A:
+            return X86::COND_A;
+
+        case X86::COND_S:
+            return X86::COND_S;
+        case X86::COND_NS:
+            return X86::COND_S;
+
+        case X86::COND_P:
+            return X86::COND_P;
+        case X86::COND_NP:
+            return X86::COND_P;
+
+        case X86::COND_O:
+            return X86::COND_O;
+        case X86::COND_NO:
+            return X86::COND_O;
+        }
+    }
+
+    bool replaceBranchTarget(MCInst &Inst, const MCSymbol *TBB,
+                             MCContext *Ctx) const override {
+        assert((isCall(Inst) || isBranch(Inst)) && !isIndirectBranch(Inst) &&
+               "Invalid instruction");
+        Inst.getOperand(0) = MCOperand::createExpr(
+            MCSymbolRefExpr::create(TBB, MCSymbolRefExpr::VK_None, *Ctx));
+        return true;
+    }
+
+    MCPhysReg getX86R11() const override { return X86::R11; }
+
+    MCPhysReg getIntArgRegister(unsigned ArgNo) const override {
+        // FIXME: this should depend on the calling convention.
+        switch (ArgNo) {
+        case 0:
+            return X86::RDI;
+        case 1:
+            return X86::RSI;
+        case 2:
+            return X86::RDX;
+        case 3:
+            return X86::RCX;
+        case 4:
+            return X86::R8;
+        case 5:
+            return X86::R9;
+        default:
+            return getNoRegister();
+        }
+    }
+
+    void createPause(MCInst &Inst) const override {
+        Inst.clear();
+        Inst.setOpcode(X86::PAUSE);
+    }
+
+    void createLfence(MCInst &Inst) const override {
+        Inst.clear();
+        Inst.setOpcode(X86::LFENCE);
+    }
+
+    bool createDirectCall(MCInst &Inst, const MCSymbol *Target, MCContext *Ctx,
+                          bool IsTailCall) override {
+        Inst.clear();
+        Inst.setOpcode(IsTailCall ? X86::JMP_4 : X86::CALL64pcrel32);
+        Inst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(Target, MCSymbolRefExpr::VK_None, *Ctx)));
+        if (IsTailCall)
+            setTailCall(Inst);
+        return true;
+    }
+
+    void createShortJmp(InstructionListType &Seq, const MCSymbol *Target,
+                        MCContext *Ctx, bool IsTailCall) override {
+        Seq.clear();
+        MCInst Inst;
+        Inst.setOpcode(X86::JMP_1);
+        Inst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(Target, MCSymbolRefExpr::VK_None, *Ctx)));
+        if (IsTailCall)
+            setTailCall(Inst);
+        Seq.emplace_back(Inst);
+    }
+
+    bool isConditionalMove(const MCInst &Inst) const override {
+        unsigned OpCode = Inst.getOpcode();
+        return (OpCode == X86::CMOV16rr || OpCode == X86::CMOV32rr ||
+                OpCode == X86::CMOV64rr);
+    }
+
+    bool isBranchOnMem(const MCInst &Inst) const override {
+        unsigned OpCode = Inst.getOpcode();
+        if (OpCode == X86::CALL64m ||
+            (OpCode == X86::JMP32m && isTailCall(Inst)) ||
+            OpCode == X86::JMP64m)
+            return true;
+
+        return false;
+    }
+
+    bool isBranchOnReg(const MCInst &Inst) const override {
+        unsigned OpCode = Inst.getOpcode();
+        if (OpCode == X86::CALL64r ||
+            (OpCode == X86::JMP32r && isTailCall(Inst)) ||
+            OpCode == X86::JMP64r)
+            return true;
+
+        return false;
+    }
+
+    void createPushRegister(MCInst &Inst, MCPhysReg Reg,
+                            unsigned Size) const override {
+        Inst.clear();
+        unsigned NewOpcode = 0;
+        if (Reg == X86::EFLAGS) {
+            switch (Size) {
+            case 2:
+                NewOpcode = X86::PUSHF16;
+                break;
+            case 4:
+                NewOpcode = X86::PUSHF32;
+                break;
+            case 8:
+                NewOpcode = X86::PUSHF64;
+                break;
+            default:
+                llvm_unreachable("Unexpected size");
+            }
+            Inst.setOpcode(NewOpcode);
+            return;
+        }
+        switch (Size) {
+        case 2:
+            NewOpcode = X86::PUSH16r;
+            break;
+        case 4:
+            NewOpcode = X86::PUSH32r;
+            break;
+        case 8:
+            NewOpcode = X86::PUSH64r;
+            break;
+        default:
+            llvm_unreachable("Unexpected size");
+        }
+        Inst.setOpcode(NewOpcode);
+        Inst.addOperand(MCOperand::createReg(Reg));
+    }
+
+    void createPopRegister(MCInst &Inst, MCPhysReg Reg,
+                           unsigned Size) const override {
+        Inst.clear();
+        unsigned NewOpcode = 0;
+        if (Reg == X86::EFLAGS) {
+            switch (Size) {
+            case 2:
+                NewOpcode = X86::POPF16;
+                break;
+            case 4:
+                NewOpcode = X86::POPF32;
+                break;
+            case 8:
+                NewOpcode = X86::POPF64;
+                break;
+            default:
+                llvm_unreachable("Unexpected size");
+            }
+            Inst.setOpcode(NewOpcode);
+            return;
+        }
+        switch (Size) {
+        case 2:
+            NewOpcode = X86::POP16r;
+            break;
+        case 4:
+            NewOpcode = X86::POP32r;
+            break;
+        case 8:
+            NewOpcode = X86::POP64r;
+            break;
+        default:
+            llvm_unreachable("Unexpected size");
+        }
+        Inst.setOpcode(NewOpcode);
+        Inst.addOperand(MCOperand::createReg(Reg));
+    }
+
+    void createPushFlags(MCInst &Inst, unsigned Size) const override {
+        return createPushRegister(Inst, X86::EFLAGS, Size);
+    }
+
+    void createPopFlags(MCInst &Inst, unsigned Size) const override {
+        return createPopRegister(Inst, X86::EFLAGS, Size);
+    }
+
+    void createAddRegImm(MCInst &Inst, MCPhysReg Reg, int64_t Value,
+                         unsigned Size) const override {
+        unsigned int Opcode;
+        switch (Size) {
+        case 1:
+            Opcode = X86::ADD8ri;
+            break;
+        case 2:
+            Opcode = X86::ADD16ri;
+            break;
+        case 4:
+            Opcode = X86::ADD32ri;
+            break;
+        case 8:
+            Opcode = X86::ADD64ri32;
+            break;
+        default:
+            llvm_unreachable("Unexpected size");
+        }
+        Inst.setOpcode(Opcode);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createReg(Reg));
+        Inst.addOperand(MCOperand::createReg(Reg));
+        Inst.addOperand(MCOperand::createImm(Value));
+    }
+
+    void createClearRegWithNoEFlagsUpdate(MCInst &Inst, MCPhysReg Reg,
+                                          unsigned Size) const {
+        unsigned int Opcode;
+        switch (Size) {
+        case 1:
+            Opcode = X86::MOV8ri;
+            break;
+        case 2:
+            Opcode = X86::MOV16ri;
+            break;
+        case 4:
+            Opcode = X86::MOV32ri;
+            break;
+        // Writing to a 32-bit register always zeros the upper 32 bits of the
+        // full-width register
+        case 8:
+            Opcode = X86::MOV32ri;
+            Reg    = getAliasSized(Reg, 4);
+            break;
+        default:
+            llvm_unreachable("Unexpected size");
+        }
+        Inst.setOpcode(Opcode);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createReg(Reg));
+        Inst.addOperand(MCOperand::createImm(0));
+    }
+
+    void createX86SaveOVFlagToRegister(MCInst &Inst, MCPhysReg Reg) const {
+        Inst.setOpcode(X86::SETCCr);
+        Inst.clear();
+        Inst.addOperand(MCOperand::createReg(Reg));
+        Inst.addOperand(MCOperand::createImm(X86::COND_O));
+    }
+
+    void createX86Lahf(MCInst &Inst) const {
+        Inst.setOpcode(X86::LAHF);
+        Inst.clear();
+    }
+
+    void createX86Sahf(MCInst &Inst) const {
+        Inst.setOpcode(X86::SAHF);
+        Inst.clear();
+    }
+
+    InstructionListType createInstrIncMemory(const MCSymbol *Target,
+                                             MCContext      *Ctx,
+                                             bool IsLeaf) const override {
+        InstructionListType Instrs(IsLeaf ? 13 : 11);
+        unsigned int        I = 0;
+
+        // Don't clobber application red zone (ABI dependent)
+        if (IsLeaf)
+            createStackPointerIncrement(Instrs[I++], 128,
+                                        /*NoFlagsClobber=*/true);
+
+        // Performance improvements based on the optimization discussed at
+        // https://reviews.llvm.org/D6629
+        // LAHF/SAHF are used instead of PUSHF/POPF
+        // PUSHF
+        createPushRegister(Instrs[I++], X86::RAX, 8);
+        createClearRegWithNoEFlagsUpdate(Instrs[I++], X86::RAX, 8);
+        createX86Lahf(Instrs[I++]);
+        createPushRegister(Instrs[I++], X86::RAX, 8);
+        createClearRegWithNoEFlagsUpdate(Instrs[I++], X86::RAX, 8);
+        createX86SaveOVFlagToRegister(Instrs[I++], X86::AL);
+        // LOCK INC
+        createIncMemory(Instrs[I++], Target, Ctx);
+        // POPF
+        createAddRegImm(Instrs[I++], X86::AL, 127, 1);
+        createPopRegister(Instrs[I++], X86::RAX, 8);
+        createX86Sahf(Instrs[I++]);
+        createPopRegister(Instrs[I++], X86::RAX, 8);
+
+        if (IsLeaf)
+            createStackPointerDecrement(Instrs[I], 128,
+                                        /*NoFlagsClobber=*/true);
+        return Instrs;
+    }
+
+    void createSwap(MCInst &Inst, MCPhysReg Source, MCPhysReg MemBaseReg,
+                    int64_t Disp) const {
+        Inst.setOpcode(X86::XCHG64rm);
+        Inst.addOperand(MCOperand::createReg(Source));
+        Inst.addOperand(MCOperand::createReg(Source));
+        Inst.addOperand(MCOperand::createReg(MemBaseReg));        // BaseReg
+        Inst.addOperand(MCOperand::createImm(1));                 // ScaleAmt
+        Inst.addOperand(MCOperand::createReg(X86::NoRegister));   // IndexReg
+        Inst.addOperand(MCOperand::createImm(Disp));   // Displacement
+        Inst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+    }
+
+    void createIndirectBranch(MCInst &Inst, MCPhysReg MemBaseReg,
+                              int64_t Disp) const {
+        Inst.setOpcode(X86::JMP64m);
+        Inst.addOperand(MCOperand::createReg(MemBaseReg));        // BaseReg
+        Inst.addOperand(MCOperand::createImm(1));                 // ScaleAmt
+        Inst.addOperand(MCOperand::createReg(X86::NoRegister));   // IndexReg
+        Inst.addOperand(MCOperand::createImm(Disp));   // Displacement
+        Inst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+    }
+
+    InstructionListType
+    createInstrumentedIndirectCall(MCInst &&CallInst, MCSymbol *HandlerFuncAddr,
+                                   int CallSiteID, MCContext *Ctx) override {
+        // Check if the target address expression used in the original indirect
+        // call uses the stack pointer, which we are going to clobber.
+        static BitVector SPAliases(getAliases(X86::RSP));
+        bool             UsesSP = false;
+        // Skip defs.
+        for (unsigned I = Info->get(CallInst.getOpcode()).getNumDefs(),
+                      E = MCPlus::getNumPrimeOperands(CallInst);
+             I != E; ++I) {
+            const MCOperand &Operand = CallInst.getOperand(I);
+            if (Operand.isReg() && SPAliases[Operand.getReg()]) {
+                UsesSP = true;
+                break;
+            }
+        }
+
+        InstructionListType Insts;
+        MCPhysReg           TempReg = getIntArgRegister(0);
+        // Code sequence used to enter indirect call instrumentation helper:
+        //   push %rdi
+        //   add $8, %rsp       ;; $rsp may be used in target, so fix it to prev
+        //   val movq target, %rdi  ;; via convertIndirectCallTargetToLoad sub
+        //   $8, %rsp       ;; restore correct stack value push %rdi movq
+        //   $CallSiteID, %rdi push %rdi callq/jmp HandlerFuncAddr
+        Insts.emplace_back();
+        createPushRegister(Insts.back(), TempReg, 8);
+        if (UsesSP) {   // Only adjust SP if we really need to
+            Insts.emplace_back();
+            createStackPointerDecrement(Insts.back(), 8,
+                                        /*NoFlagsClobber=*/false);
+        }
+        Insts.emplace_back(CallInst);
+        // Insts.back() and CallInst now share the same annotation instruction.
+        // Strip it from Insts.back(), only preserving tail call annotation.
+        stripAnnotations(Insts.back(), /*KeepTC=*/true);
+        convertIndirectCallToLoad(Insts.back(), TempReg);
+        if (UsesSP) {
+            Insts.emplace_back();
+            createStackPointerIncrement(Insts.back(), 8,
+                                        /*NoFlagsClobber=*/false);
+        }
+        Insts.emplace_back();
+        createPushRegister(Insts.back(), TempReg, 8);
+        Insts.emplace_back();
+        createLoadImmediate(Insts.back(), TempReg, CallSiteID);
+        Insts.emplace_back();
+        createPushRegister(Insts.back(), TempReg, 8);
+
+        MCInst &NewCallInst = Insts.emplace_back();
+        createDirectCall(NewCallInst, HandlerFuncAddr, Ctx,
+                         isTailCall(CallInst));
+
+        // Carry over metadata including tail call marker if present.
+        stripAnnotations(NewCallInst);
+        moveAnnotations(std::move(CallInst), NewCallInst);
+
+        return Insts;
+    }
+
+    InstructionListType
+    createInstrumentedIndCallHandlerExitBB() const override {
+        const MCPhysReg TempReg = getIntArgRegister(0);
+        // We just need to undo the sequence created for every ind call in
+        // instrumentIndirectTarget(), which can be accomplished minimally with:
+        //   popfq
+        //   pop %rdi
+        //   add $16, %rsp
+        //   xchg (%rsp), %rdi
+        //   jmp *-8(%rsp)
+        InstructionListType Insts(5);
+        createPopFlags(Insts[0], 8);
+        createPopRegister(Insts[1], TempReg, 8);
+        createStackPointerDecrement(Insts[2], 16, /*NoFlagsClobber=*/false);
+        createSwap(Insts[3], TempReg, X86::RSP, 0);
+        createIndirectBranch(Insts[4], X86::RSP, -8);
+        return Insts;
+    }
+
+    InstructionListType
+    createInstrumentedIndTailCallHandlerExitBB() const override {
+        const MCPhysReg TempReg = getIntArgRegister(0);
+        // Same thing as above, but for tail calls
+        //   popfq
+        //   add $16, %rsp
+        //   pop %rdi
+        //   jmp *-16(%rsp)
+        InstructionListType Insts(4);
+        createPopFlags(Insts[0], 8);
+        createStackPointerDecrement(Insts[1], 16, /*NoFlagsClobber=*/false);
+        createPopRegister(Insts[2], TempReg, 8);
+        createIndirectBranch(Insts[3], X86::RSP, -16);
+        return Insts;
+    }
+
+    InstructionListType
+    createInstrumentedIndCallHandlerEntryBB(const MCSymbol *InstrTrampoline,
+                                            const MCSymbol *IndCallHandler,
+                                            MCContext      *Ctx) override {
+        const MCPhysReg TempReg = getIntArgRegister(0);
+        // Code sequence used to check whether InstrTampoline was initialized
+        // and call it if so, returns via IndCallHandler.
+        //   pushfq
+        //   mov    InstrTrampoline,%rdi
+        //   cmp    $0x0,%rdi
+        //   je     IndCallHandler
+        //   callq  *%rdi
+        //   jmpq   IndCallHandler
+        InstructionListType Insts;
+        Insts.emplace_back();
+        createPushFlags(Insts.back(), 8);
+        Insts.emplace_back();
+        createMove(Insts.back(), InstrTrampoline, TempReg, Ctx);
+        InstructionListType cmpJmp =
+            createCmpJE(TempReg, 0, IndCallHandler, Ctx);
+        Insts.insert(Insts.end(), cmpJmp.begin(), cmpJmp.end());
+        Insts.emplace_back();
+        Insts.back().setOpcode(X86::CALL64r);
+        Insts.back().addOperand(MCOperand::createReg(TempReg));
+        Insts.emplace_back();
+        createDirectCall(Insts.back(), IndCallHandler, Ctx,
+                         /*IsTailCall*/ true);
+        return Insts;
+    }
+
+    InstructionListType createNumCountersGetter(MCContext *Ctx) const override {
+        InstructionListType Insts(2);
+        MCSymbol *NumLocs = Ctx->getOrCreateSymbol("__bolt_num_counters");
+        createMove(Insts[0], NumLocs, X86::EAX, Ctx);
+        createReturn(Insts[1]);
+        return Insts;
+    }
+
+    InstructionListType
+    createInstrLocationsGetter(MCContext *Ctx) const override {
+        InstructionListType Insts(2);
+        MCSymbol *Locs = Ctx->getOrCreateSymbol("__bolt_instr_locations");
+        createLea(Insts[0], Locs, X86::EAX, Ctx);
+        createReturn(Insts[1]);
+        return Insts;
+    }
+
+    InstructionListType createInstrTablesGetter(MCContext *Ctx) const override {
+        InstructionListType Insts(2);
+        MCSymbol *Locs = Ctx->getOrCreateSymbol("__bolt_instr_tables");
+        createLea(Insts[0], Locs, X86::EAX, Ctx);
+        createReturn(Insts[1]);
+        return Insts;
+    }
+
+    InstructionListType
+    createInstrNumFuncsGetter(MCContext *Ctx) const override {
+        InstructionListType Insts(2);
+        MCSymbol *NumFuncs = Ctx->getOrCreateSymbol("__bolt_instr_num_funcs");
+        createMove(Insts[0], NumFuncs, X86::EAX, Ctx);
+        createReturn(Insts[1]);
+        return Insts;
+    }
+
+    InstructionListType createSymbolTrampoline(const MCSymbol *TgtSym,
+                                               MCContext *Ctx) const override {
+        InstructionListType Insts(1);
+        createUncondBranch(Insts[0], TgtSym, Ctx);
+        return Insts;
+    }
+
+    InstructionListType
+    createDummyReturnFunction(MCContext *Ctx) const override {
+        InstructionListType Insts(1);
+        createReturn(Insts[0]);
+        return Insts;
+    }
+
+    BlocksVectorTy indirectCallPromotion(
+        const MCInst                                       &CallInst,
+        const std::vector<std::pair<MCSymbol *, uint64_t>> &Targets,
+        const std::vector<std::pair<MCSymbol *, uint64_t>> &VtableSyms,
+        const std::vector<MCInst *>                        &MethodFetchInsns,
+        const bool MinimizeCodeSize, MCContext *Ctx) override {
+        const bool     IsTailCall  = isTailCall(CallInst);
+        const bool     IsJumpTable = getJumpTable(CallInst) != 0;
+        BlocksVectorTy Results;
+
+        // Label for the current code block.
+        MCSymbol *NextTarget = nullptr;
+
+        // The join block which contains all the instructions following
+        // CallInst. MergeBlock remains null if CallInst is a tail call.
+        MCSymbol *MergeBlock = nullptr;
+
+        unsigned FuncAddrReg = X86::R10;
+
+        const bool LoadElim = !VtableSyms.empty();
+        assert((!LoadElim || VtableSyms.size() == Targets.size()) &&
+               "There must be a vtable entry for every method "
+               "in the targets vector.");
+
+        if (MinimizeCodeSize && !LoadElim) {
+            std::set<unsigned> UsedRegs;
+
+            for (unsigned int I = 0; I < MCPlus::getNumPrimeOperands(CallInst);
+                 ++I) {
+                const MCOperand &Op = CallInst.getOperand(I);
+                if (Op.isReg())
+                    UsedRegs.insert(Op.getReg());
+            }
+
+            if (UsedRegs.count(X86::R10) == 0)
+                FuncAddrReg = X86::R10;
+            else if (UsedRegs.count(X86::R11) == 0)
+                FuncAddrReg = X86::R11;
+            else
+                return Results;
+        }
+
+        const auto jumpToMergeBlock = [&](InstructionListType &NewCall) {
+            assert(MergeBlock);
+            NewCall.push_back(CallInst);
+            MCInst &Merge = NewCall.back();
+            Merge.clear();
+            createUncondBranch(Merge, MergeBlock, Ctx);
+        };
+
+        for (unsigned int i = 0; i < Targets.size(); ++i) {
+            Results.emplace_back(NextTarget, InstructionListType());
+            InstructionListType *NewCall = &Results.back().second;
+
+            if (MinimizeCodeSize && !LoadElim) {
+                // Load the call target into FuncAddrReg.
+                NewCall->push_back(
+                    CallInst);   // Copy CallInst in order to get SMLoc
+                MCInst &Target = NewCall->back();
+                Target.clear();
+                Target.setOpcode(X86::MOV64ri32);
+                Target.addOperand(MCOperand::createReg(FuncAddrReg));
+                if (Targets[i].first) {
+                    // Is this OK?
+                    Target.addOperand(
+                        MCOperand::createExpr(MCSymbolRefExpr::create(
+                            Targets[i].first, MCSymbolRefExpr::VK_None, *Ctx)));
+                } else {
+                    const uint64_t Addr = Targets[i].second;
+                    // Immediate address is out of sign extended 32 bit range.
+                    if (int64_t(Addr) != int64_t(int32_t(Addr)))
+                        return BlocksVectorTy();
+
+                    Target.addOperand(MCOperand::createImm(Addr));
+                }
+
+                // Compare current call target to a specific address.
+                NewCall->push_back(CallInst);
+                MCInst &Compare = NewCall->back();
+                Compare.clear();
+                if (isBranchOnReg(CallInst))
+                    Compare.setOpcode(X86::CMP64rr);
+                else if (CallInst.getOpcode() == X86::CALL64pcrel32)
+                    Compare.setOpcode(X86::CMP64ri32);
+                else
+                    Compare.setOpcode(X86::CMP64rm);
+
+                Compare.addOperand(MCOperand::createReg(FuncAddrReg));
+
+                // TODO: Would be preferable to only load this value once.
+                for (unsigned i = 0;
+                     i < Info->get(CallInst.getOpcode()).getNumOperands(); ++i)
+                    if (!CallInst.getOperand(i).isInst())
+                        Compare.addOperand(CallInst.getOperand(i));
+            } else {
+                // Compare current call target to a specific address.
+                NewCall->push_back(CallInst);
+                MCInst &Compare = NewCall->back();
+                Compare.clear();
+                if (isBranchOnReg(CallInst))
+                    Compare.setOpcode(X86::CMP64ri32);
+                else
+                    Compare.setOpcode(X86::CMP64mi32);
+
+                // Original call address.
+                for (unsigned i = 0;
+                     i < Info->get(CallInst.getOpcode()).getNumOperands(); ++i)
+                    if (!CallInst.getOperand(i).isInst())
+                        Compare.addOperand(CallInst.getOperand(i));
+
+                // Target address.
+                if (Targets[i].first || LoadElim) {
+                    const MCSymbol *Sym =
+                        LoadElim ? VtableSyms[i].first : Targets[i].first;
+                    const uint64_t Addend = LoadElim ? VtableSyms[i].second : 0;
+                    const MCExpr  *Expr   = MCSymbolRefExpr::create(Sym, *Ctx);
+                    if (Addend)
+                        Expr = MCBinaryExpr::createAdd(
+                            Expr, MCConstantExpr::create(Addend, *Ctx), *Ctx);
+                    Compare.addOperand(MCOperand::createExpr(Expr));
+                } else {
+                    const uint64_t Addr = Targets[i].second;
+                    // Immediate address is out of sign extended 32 bit range.
+                    if (int64_t(Addr) != int64_t(int32_t(Addr)))
+                        return BlocksVectorTy();
+
+                    Compare.addOperand(MCOperand::createImm(Addr));
+                }
+            }
+
+            // jump to next target compare.
+            NextTarget = Ctx->createNamedTempSymbol();   // generate label for
+                                                         // the next block
+            NewCall->push_back(CallInst);
+
+            if (IsJumpTable) {
+                MCInst &Je = NewCall->back();
+
+                // Jump to next compare if target addresses don't match.
+                Je.clear();
+                Je.setOpcode(X86::JCC_1);
+                if (Targets[i].first)
+                    Je.addOperand(MCOperand::createExpr(MCSymbolRefExpr::create(
+                        Targets[i].first, MCSymbolRefExpr::VK_None, *Ctx)));
+                else
+                    Je.addOperand(MCOperand::createImm(Targets[i].second));
+
+                Je.addOperand(MCOperand::createImm(X86::COND_E));
+                assert(!isInvoke(CallInst));
+            } else {
+                MCInst &Jne = NewCall->back();
+
+                // Jump to next compare if target addresses don't match.
+                Jne.clear();
+                Jne.setOpcode(X86::JCC_1);
+                Jne.addOperand(MCOperand::createExpr(MCSymbolRefExpr::create(
+                    NextTarget, MCSymbolRefExpr::VK_None, *Ctx)));
+                Jne.addOperand(MCOperand::createImm(X86::COND_NE));
+
+                // Call specific target directly.
+                Results.emplace_back(Ctx->createNamedTempSymbol(),
+                                     InstructionListType());
+                NewCall = &Results.back().second;
+                NewCall->push_back(CallInst);
+                MCInst &CallOrJmp = NewCall->back();
+
+                CallOrJmp.clear();
+
+                if (MinimizeCodeSize && !LoadElim) {
+                    CallOrJmp.setOpcode(IsTailCall ? X86::JMP32r
+                                                   : X86::CALL64r);
+                    CallOrJmp.addOperand(MCOperand::createReg(FuncAddrReg));
+                } else {
+                    CallOrJmp.setOpcode(IsTailCall ? X86::JMP_4
+                                                   : X86::CALL64pcrel32);
+
+                    if (Targets[i].first)
+                        CallOrJmp.addOperand(MCOperand::createExpr(
+                            MCSymbolRefExpr::create(Targets[i].first,
+                                                    MCSymbolRefExpr::VK_None,
+                                                    *Ctx)));
+                    else
+                        CallOrJmp.addOperand(
+                            MCOperand::createImm(Targets[i].second));
+                }
+                if (IsTailCall)
+                    setTailCall(CallOrJmp);
+
+                if (CallOrJmp.getOpcode() == X86::CALL64r ||
+                    CallOrJmp.getOpcode() == X86::CALL64pcrel32) {
+                    if (std::optional<uint32_t> Offset = getOffset(CallInst))
+                        // Annotated as duplicated call
+                        setOffset(CallOrJmp, *Offset);
+                }
+
+                if (isInvoke(CallInst) && !isInvoke(CallOrJmp)) {
+                    // Copy over any EH or GNU args size information from the
+                    // original call.
+                    std::optional<MCPlus::MCLandingPad> EHInfo =
+                        getEHInfo(CallInst);
+                    if (EHInfo)
+                        addEHInfo(CallOrJmp, *EHInfo);
+                    int64_t GnuArgsSize = getGnuArgsSize(CallInst);
+                    if (GnuArgsSize >= 0)
+                        addGnuArgsSize(CallOrJmp, GnuArgsSize);
+                }
+
+                if (!IsTailCall) {
+                    // The fallthrough block for the most common target should
+                    // be the merge block.
+                    if (i == 0) {
+                        // Fallthrough to merge block.
+                        MergeBlock = Ctx->createNamedTempSymbol();
+                    } else {
+                        // Insert jump to the merge block if we are not doing a
+                        // fallthrough.
+                        jumpToMergeBlock(*NewCall);
+                    }
+                }
+            }
+        }
+
+        // Cold call block.
+        Results.emplace_back(NextTarget, InstructionListType());
+        InstructionListType &NewCall = Results.back().second;
+        for (const MCInst *Inst : MethodFetchInsns)
+            if (Inst != &CallInst)
+                NewCall.push_back(*Inst);
+        NewCall.push_back(CallInst);
+
+        // Jump to merge block from cold call block
+        if (!IsTailCall && !IsJumpTable) {
+            jumpToMergeBlock(NewCall);
+
+            // Record merge block
+            Results.emplace_back(MergeBlock, InstructionListType());
+        }
+
+        return Results;
+    }
+
+    BlocksVectorTy jumpTablePromotion(
+        const MCInst                                       &IJmpInst,
+        const std::vector<std::pair<MCSymbol *, uint64_t>> &Targets,
+        const std::vector<MCInst *>                        &TargetFetchInsns,
+        MCContext *Ctx) const override {
+        assert(getJumpTable(IJmpInst) != 0);
+        uint16_t IndexReg = getAnnotationAs<uint16_t>(IJmpInst, "JTIndexReg");
+        if (IndexReg == 0)
+            return BlocksVectorTy();
+
+        BlocksVectorTy Results;
+
+        // Label for the current code block.
+        MCSymbol *NextTarget = nullptr;
+
+        for (unsigned int i = 0; i < Targets.size(); ++i) {
+            Results.emplace_back(NextTarget, InstructionListType());
+            InstructionListType *CurBB = &Results.back().second;
+
+            // Compare current index to a specific index.
+            CurBB->emplace_back(MCInst());
+            MCInst &CompareInst = CurBB->back();
+            CompareInst.setLoc(IJmpInst.getLoc());
+            CompareInst.setOpcode(X86::CMP64ri32);
+            CompareInst.addOperand(MCOperand::createReg(IndexReg));
+
+            const uint64_t CaseIdx = Targets[i].second;
+            // Immediate address is out of sign extended 32 bit range.
+            if (int64_t(CaseIdx) != int64_t(int32_t(CaseIdx)))
+                return BlocksVectorTy();
+
+            CompareInst.addOperand(MCOperand::createImm(CaseIdx));
+            shortenInstruction(CompareInst, *Ctx->getSubtargetInfo());
+
+            // jump to next target compare.
+            NextTarget = Ctx->createNamedTempSymbol();   // generate label for
+                                                         // the next block
+            CurBB->push_back(MCInst());
+
+            MCInst &JEInst = CurBB->back();
+            JEInst.setLoc(IJmpInst.getLoc());
+
+            // Jump to target if indices match
+            JEInst.setOpcode(X86::JCC_1);
+            JEInst.addOperand(MCOperand::createExpr(MCSymbolRefExpr::create(
+                Targets[i].first, MCSymbolRefExpr::VK_None, *Ctx)));
+            JEInst.addOperand(MCOperand::createImm(X86::COND_E));
+        }
+
+        // Cold call block.
+        Results.emplace_back(NextTarget, InstructionListType());
+        InstructionListType &CurBB = Results.back().second;
+        for (const MCInst *Inst : TargetFetchInsns)
+            if (Inst != &IJmpInst)
+                CurBB.push_back(*Inst);
+
+        CurBB.push_back(IJmpInst);
+
+        return Results;
+    }
+
+  private:
+    bool createMove(MCInst &Inst, const MCSymbol *Src, unsigned Reg,
+                    MCContext *Ctx) const {
+        Inst.setOpcode(X86::MOV64rm);
+        Inst.addOperand(MCOperand::createReg(Reg));   // dst register?
+
+        // [SAM] seems like this function compute the relative address (w.r.t
+        // RIP) of the src [SAM] Addresing mode: Base + (Index * Scale) +
+        // Displacement
+        Inst.addOperand(MCOperand::createReg(X86::RIP));          // BaseReg
+        Inst.addOperand(MCOperand::createImm(1));                 // ScaleAmt
+        Inst.addOperand(MCOperand::createReg(X86::NoRegister));   // IndexReg
+        Inst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(Src, MCSymbolRefExpr::VK_None,
+                                    *Ctx)));   // Displacement
+        Inst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+
+        return true;
+    }
+
+    bool createLea(MCInst &Inst, const MCSymbol *Src, unsigned Reg,
+                   MCContext *Ctx) const {
+        Inst.setOpcode(X86::LEA64r);
+        Inst.addOperand(MCOperand::createReg(Reg));
+
+        Inst.addOperand(MCOperand::createReg(X86::RIP));          // BaseReg
+        Inst.addOperand(MCOperand::createImm(1));                 // ScaleAmt
+        Inst.addOperand(MCOperand::createReg(X86::NoRegister));   // IndexReg
+        Inst.addOperand(MCOperand::createExpr(
+            MCSymbolRefExpr::create(Src, MCSymbolRefExpr::VK_None,
+                                    *Ctx)));   // Displacement
+        Inst.addOperand(
+            MCOperand::createReg(X86::NoRegister));   // AddrSegmentReg
+        return true;
+    }
 };
 
-} // namespace
+}   // namespace
 
 namespace llvm {
 namespace bolt {
 
-MCPlusBuilder *createX86MCPlusBuilder(const MCInstrAnalysis *Analysis,
-                                      const MCInstrInfo *Info,
-                                      const MCRegisterInfo *RegInfo) {
-  return new X86MCPlusBuilder(Analysis, Info, RegInfo);
+MCPlusBuilder *
+createX86MCPlusBuilder(const MCInstrAnalysis *Analysis, const MCInstrInfo *Info,
+                       const MCRegisterInfo *RegInfo) {
+    return new X86MCPlusBuilder(Analysis, Info, RegInfo);
 }
 
-} // namespace bolt
-} // namespace llvm
+}   // namespace bolt
+}   // namespace llvm
diff --git a/bolt/tools/driver/llvm-bolt.cpp b/bolt/tools/driver/llvm-bolt.cpp
index 5a3af6a44b52..863b96d8c4da 100644
--- a/bolt/tools/driver/llvm-bolt.cpp
+++ b/bolt/tools/driver/llvm-bolt.cpp
@@ -27,6 +27,7 @@
 #include "llvm/Support/Signals.h"
 #include "llvm/Support/TargetSelect.h"
 
+
 #define DEBUG_TYPE "bolt"
 
 using namespace llvm;
@@ -51,6 +52,87 @@ static cl::opt<std::string> InputFilename(cl::Positional,
                                           cl::Required, cl::cat(BoltCategory),
                                           cl::sub(cl::SubCommand::getAll()));
 
+cl::opt<std::string>
+CMPCListFilename("clh-cmpc-list",
+  cl::desc("file containing list of PCs of deliquent loads"),
+  cl::Optional,
+  cl::cat(BoltCategory));
+cl::opt<std::string>
+LatProfFilename("lat-prof",
+  cl::desc("file containing latencies of instructions"),
+  cl::Optional,
+  cl::cat(BoltCategory));
+cl::opt<std::string>
+PredProfFilename("pred-prof",
+  cl::desc("file containing branch prediction information"),
+  cl::Optional,
+  cl::cat(BoltCategory));
+cl::opt<bool>
+OptimizedYield("clh-opt-yield",
+  cl::desc("Optimize yield instruemntation"),
+  cl::Optional,
+  cl::cat(BoltCategory));
+cl::opt<bool>
+DisableAsymPushPop("disable-asympp",
+  cl::desc("Optimize yield instruemntation"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltCategory));
+cl::opt<bool>
+DisableLO("disable-lo",
+  cl::desc("Optimize yield instruemntation"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltCategory));
+cl::opt<bool>
+DisableFUR("disable-fur",
+  cl::desc("Optimize yield instruemntation"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltCategory));
+cl::opt<bool>
+DisableCtxPrefetch("disable-ctx-prefetch",
+  cl::desc("Optimize yield instruemntation"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltCategory));
+cl::opt<bool>
+DisablePseudoInline("disable-pseudo-inline",
+  cl::desc("Optimize yield instruemntation"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltCategory));
+cl::opt<bool>
+InstScavenger("inst-scav",
+  cl::desc("Optimize yield instruemntation"),
+  cl::Optional,
+  cl::init(false),
+  cl::cat(BoltCategory));
+cl::opt<int>
+BoundYieldDistance("bound-yield-distance",
+  cl::desc("Optimize yield instruemntation"),
+  cl::Optional,
+  cl::init(0),
+  cl::cat(BoltCategory));
+cl::opt<int>
+IterTh("iter-th",
+  cl::desc("Threshold"),
+  cl::Optional,
+  cl::init(99999999),
+  cl::cat(BoltCategory));
+cl::opt<int>
+RelOhTh("clh-rel-oh-th",
+  cl::desc("Threshold to put prefetch-yield"),
+  cl::Optional,
+  cl::init(0),
+  cl::cat(BoltCategory));
+cl::opt<int>
+ProbTh("clh-prob-th",
+  cl::desc("Threshold to put prefetch-yield"),
+  cl::Optional,
+  cl::init(0),
+  cl::cat(BoltCategory));
+
 static cl::opt<std::string>
 InputDataFilename("data",
   cl::desc("<data file>"),
@@ -204,6 +286,13 @@ int main(int argc, char **argv) {
   else
     boltMode(argc, argv);
 
+  if (opts::CMPCListFilename.empty()) {
+    errs() << "BOLT-INFO: no PC list file specified. \n";
+  } else {
+    if (!sys::fs::exists(opts::CMPCListFilename))
+      report_error(opts::CMPCListFilename, errc::no_such_file_or_directory);
+  }
+
   if (!sys::fs::exists(opts::InputFilename))
     report_error(opts::InputFilename, errc::no_such_file_or_directory);
 
@@ -215,6 +304,7 @@ int main(int argc, char **argv) {
       report_error(opts::InputFilename, std::move(E));
     Binary &Binary = *BinaryOrErr.get().getBinary();
 
+    //[SAM] pointer to ELF file manipluation
     if (auto *e = dyn_cast<ELFObjectFileBase>(&Binary)) {
       auto RIOrErr = RewriteInstance::create(e, argc, argv, ToolPath);
       if (Error E = RIOrErr.takeError())
@@ -231,6 +321,18 @@ int main(int argc, char **argv) {
         if (Error E = RI.setProfile(opts::PerfData))
           report_error(opts::PerfData, std::move(E));
       }
+
+      if (!opts::CMPCListFilename.empty()) {
+        if (Error E = RI.setCMPC(opts::CMPCListFilename))
+          report_error(opts::CMPCListFilename, std::move(E));  
+      }
+      
+      if (!opts::LatProfFilename.empty() && !opts::PredProfFilename.empty()) {
+        if (Error E = RI.setScavProf(opts::LatProfFilename, opts::PredProfFilename))
+          report_error(opts::LatProfFilename, std::move(E));  
+      }
+
+
       if (!opts::InputDataFilename.empty()) {
         if (Error E = RI.setProfile(opts::InputDataFilename))
           report_error(opts::InputDataFilename, std::move(E));
diff --git a/clang/lib/CodeGen/CGBuiltin.cpp b/clang/lib/CodeGen/CGBuiltin.cpp
index 5952ac4ff5ac..b862d0d6e692 100644
--- a/clang/lib/CodeGen/CGBuiltin.cpp
+++ b/clang/lib/CodeGen/CGBuiltin.cpp
@@ -5217,7 +5217,7 @@ RValue CodeGenFunction::EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,
   }
   case Builtin::BI__builtin_load_half: {
     Address Address = EmitPointerWithAlignment(E->getArg(0));
-    Value *HalfVal = Builder.CreateLoad(Address);
+    Value *HalfVal = Builder.Address);
     return RValue::get(Builder.CreateFPExt(HalfVal, Builder.getDoubleTy()));
   }
   case Builtin::BI__builtin_load_halff: {
diff --git a/llvm/include/llvm/MC/MCInst.h b/llvm/include/llvm/MC/MCInst.h
index 2bc310852fe5..029f37d2cb2c 100644
--- a/llvm/include/llvm/MC/MCInst.h
+++ b/llvm/include/llvm/MC/MCInst.h
@@ -192,6 +192,8 @@ class MCInst {
   SmallVector<MCOperand, 10> Operands;
 
 public:
+  uint64_t OriginalAddress = 0;
+
   MCInst() = default;
 
   void setOpcode(unsigned Op) { Opcode = Op; }
diff --git a/llvm/lib/Target/X86/Disassembler/X86Disassembler.cpp b/llvm/lib/Target/X86/Disassembler/X86Disassembler.cpp
index 49651da63ecf..ab5acf38f6cf 100644
--- a/llvm/lib/Target/X86/Disassembler/X86Disassembler.cpp
+++ b/llvm/lib/Target/X86/Disassembler/X86Disassembler.cpp
@@ -88,6 +88,9 @@
 #include "llvm/Support/Format.h"
 #include "llvm/Support/raw_ostream.h"
 
+// [SAM]
+#include <iostream>
+
 using namespace llvm;
 using namespace llvm::X86Disassembler;
 
@@ -1776,6 +1779,8 @@ MCDisassembler::DecodeStatus X86GenericDisassembler::getInstruction(
     }
     Instr.setFlags(Flags);
   }
+
+
   return (!Ret) ? Success : Fail;
 }
 
